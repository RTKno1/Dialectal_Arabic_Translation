{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BQflUKhjIKNp"
   },
   "source": [
    "### Notes \n",
    "\n",
    "T5 Paper: https://arxiv.org/pdf/1910.10683.pdf\n",
    "\n",
    "T5 Tokenizer: https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_t5.py\n",
    "\n",
    "Important Tasks: https://docs.google.com/document/d/1weIZM6QTlnitpPQmpg-WeV2RW70TnYmDuogBQPr5mB0/edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "XLOmiOta6MJp",
    "outputId": "ff45d283-9268-43e2-8f22-6f20312d5c4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
      "\r",
      "\u001b[K     |▋                               | 10kB 17.2MB/s eta 0:00:01\r",
      "\u001b[K     |█▏                              | 20kB 1.7MB/s eta 0:00:01\r",
      "\u001b[K     |█▊                              | 30kB 2.3MB/s eta 0:00:01\r",
      "\u001b[K     |██▎                             | 40kB 1.7MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 51kB 1.9MB/s eta 0:00:01\r",
      "\u001b[K     |███▌                            | 61kB 2.2MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 71kB 2.4MB/s eta 0:00:01\r",
      "\u001b[K     |████▋                           | 81kB 2.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████▎                          | 92kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████▉                          | 102kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████▍                         | 112kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 122kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 133kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████▏                       | 143kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████▊                       | 153kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▎                      | 163kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▉                      | 174kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▌                     | 184kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 194kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▋                    | 204kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▏                   | 215kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▉                   | 225kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▍                  | 235kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 245kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▌                 | 256kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▏                | 266kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▊                | 276kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▎               | 286kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▉               | 296kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▍              | 307kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 317kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 327kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▏            | 337kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▊            | 348kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▍           | 358kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 368kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▌          | 378kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 389kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▊         | 399kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▎        | 409kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▉        | 419kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▍       | 430kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 440kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▋      | 450kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 460kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▊     | 471kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▎    | 481kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 491kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▌   | 501kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 512kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▋  | 522kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▎ | 532kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▉ | 542kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▍| 552kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 563kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 573kB 2.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.40)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\r",
      "\u001b[K     |▎                               | 10kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |▋                               | 20kB 29.3MB/s eta 0:00:01\r",
      "\u001b[K     |█                               | 30kB 35.0MB/s eta 0:00:01\r",
      "\u001b[K     |█▎                              | 40kB 18.8MB/s eta 0:00:01\r",
      "\u001b[K     |█▋                              | 51kB 13.3MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 61kB 15.3MB/s eta 0:00:01\r",
      "\u001b[K     |██▏                             | 71kB 15.0MB/s eta 0:00:01\r",
      "\u001b[K     |██▌                             | 81kB 12.3MB/s eta 0:00:01\r",
      "\u001b[K     |██▉                             | 92kB 13.5MB/s eta 0:00:01\r",
      "\u001b[K     |███▏                            | 102kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███▌                            | 112kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███▉                            | 122kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 133kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████▍                           | 143kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████▊                           | 153kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 163kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████▍                          | 174kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████▊                          | 184kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 194kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████▎                         | 204kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████▋                         | 215kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 225kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 235kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 245kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 256kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████▏                       | 266kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████▌                       | 276kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████▉                       | 286kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▏                      | 296kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▌                      | 307kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▊                      | 317kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 327kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▍                     | 337kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▊                     | 348kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 358kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▍                    | 368kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▋                    | 378kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 389kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▎                   | 399kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▋                   | 409kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 419kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▎                  | 430kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▌                  | 440kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▉                  | 450kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▏                 | 460kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▌                 | 471kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▉                 | 481kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▏                | 491kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▍                | 501kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▊                | 512kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 522kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▍               | 532kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▊               | 542kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 552kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▎              | 563kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▋              | 573kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 583kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▎             | 593kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 604kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 614kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▏            | 624kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▌            | 634kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▉            | 645kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▏           | 655kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▌           | 665kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▉           | 675kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▏          | 686kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▍          | 696kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▊          | 706kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 716kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▍         | 727kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▊         | 737kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 747kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▎        | 757kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▋        | 768kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 778kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▎       | 788kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▋       | 798kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 808kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▏      | 819kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▌      | 829kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▉      | 839kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 849kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 860kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▉     | 870kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 880kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▍    | 890kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▊    | 901kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 911kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▍   | 921kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▊   | 931kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 942kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▎  | 952kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▋  | 962kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 972kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▎ | 983kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▋ | 993kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▉ | 1.0MB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 1.0MB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▌| 1.0MB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 1.0MB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 1.0MB 12.2MB/s \n",
      "\u001b[?25hCollecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/50/93509f906a40bffd7d175f97fd75ea328ad9bd91f48f59c4bd084c94a25e/sacremoses-0.0.41.tar.gz (883kB)\n",
      "\r",
      "\u001b[K     |▍                               | 10kB 22.2MB/s eta 0:00:01\r",
      "\u001b[K     |▊                               | 20kB 27.8MB/s eta 0:00:01\r",
      "\u001b[K     |█▏                              | 30kB 33.9MB/s eta 0:00:01\r",
      "\u001b[K     |█▌                              | 40kB 34.9MB/s eta 0:00:01\r",
      "\u001b[K     |█▉                              | 51kB 23.6MB/s eta 0:00:01\r",
      "\u001b[K     |██▎                             | 61kB 26.2MB/s eta 0:00:01\r",
      "\u001b[K     |██▋                             | 71kB 21.9MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 81kB 19.3MB/s eta 0:00:01\r",
      "\u001b[K     |███▍                            | 92kB 20.9MB/s eta 0:00:01\r",
      "\u001b[K     |███▊                            | 102kB 19.5MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 112kB 19.5MB/s eta 0:00:01\r",
      "\u001b[K     |████▌                           | 122kB 19.5MB/s eta 0:00:01\r",
      "\u001b[K     |████▉                           | 133kB 19.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████▏                          | 143kB 19.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████▋                          | 153kB 19.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 163kB 19.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████▎                         | 174kB 30kB/s eta 0:00:24\r",
      "\u001b[K     |██████▊                         | 184kB 30kB/s eta 0:00:23\r",
      "\u001b[K     |███████                         | 194kB 30kB/s eta 0:00:23\r",
      "\u001b[K     |███████▍                        | 204kB 30kB/s eta 0:00:23\r",
      "\u001b[K     |███████▉                        | 215kB 30kB/s eta 0:00:22\r",
      "\u001b[K     |████████▏                       | 225kB 30kB/s eta 0:00:22\r",
      "\u001b[K     |████████▌                       | 235kB 30kB/s eta 0:00:22\r",
      "\u001b[K     |█████████                       | 245kB 30kB/s eta 0:00:21\r",
      "\u001b[K     |█████████▎                      | 256kB 30kB/s eta 0:00:21\r",
      "\u001b[K     |█████████▋                      | 266kB 30kB/s eta 0:00:21\r",
      "\u001b[K     |██████████                      | 276kB 30kB/s eta 0:00:20\r",
      "\u001b[K     |██████████▍                     | 286kB 30kB/s eta 0:00:20\r",
      "\u001b[K     |██████████▊                     | 296kB 30kB/s eta 0:00:20\r",
      "\u001b[K     |███████████▏                    | 307kB 30kB/s eta 0:00:19\r",
      "\u001b[K     |███████████▌                    | 317kB 30kB/s eta 0:00:19\r",
      "\u001b[K     |███████████▉                    | 327kB 30kB/s eta 0:00:19\r",
      "\u001b[K     |████████████▎                   | 337kB 30kB/s eta 0:00:18\r",
      "\u001b[K     |████████████▋                   | 348kB 30kB/s eta 0:00:18\r",
      "\u001b[K     |█████████████                   | 358kB 30kB/s eta 0:00:18\r",
      "\u001b[K     |█████████████▍                  | 368kB 30kB/s eta 0:00:17\r",
      "\u001b[K     |█████████████▊                  | 378kB 30kB/s eta 0:00:17\r",
      "\u001b[K     |██████████████                  | 389kB 30kB/s eta 0:00:17\r",
      "\u001b[K     |██████████████▌                 | 399kB 30kB/s eta 0:00:16\r",
      "\u001b[K     |██████████████▉                 | 409kB 30kB/s eta 0:00:16\r",
      "\u001b[K     |███████████████▏                | 419kB 30kB/s eta 0:00:16\r",
      "\u001b[K     |███████████████▋                | 430kB 30kB/s eta 0:00:15\r",
      "\u001b[K     |████████████████                | 440kB 30kB/s eta 0:00:15\r",
      "\u001b[K     |████████████████▎               | 450kB 30kB/s eta 0:00:15\r",
      "\u001b[K     |████████████████▊               | 460kB 30kB/s eta 0:00:14\r",
      "\u001b[K     |█████████████████               | 471kB 30kB/s eta 0:00:14\r",
      "\u001b[K     |█████████████████▍              | 481kB 30kB/s eta 0:00:14\r",
      "\u001b[K     |█████████████████▉              | 491kB 30kB/s eta 0:00:13\r",
      "\u001b[K     |██████████████████▏             | 501kB 30kB/s eta 0:00:13\r",
      "\u001b[K     |██████████████████▌             | 512kB 30kB/s eta 0:00:13\r",
      "\u001b[K     |███████████████████             | 522kB 30kB/s eta 0:00:12\r",
      "\u001b[K     |███████████████████▎            | 532kB 30kB/s eta 0:00:12\r",
      "\u001b[K     |███████████████████▋            | 542kB 30kB/s eta 0:00:12\r",
      "\u001b[K     |████████████████████            | 552kB 30kB/s eta 0:00:11\r",
      "\u001b[K     |████████████████████▍           | 563kB 30kB/s eta 0:00:11\r",
      "\u001b[K     |████████████████████▊           | 573kB 30kB/s eta 0:00:11\r",
      "\u001b[K     |█████████████████████▏          | 583kB 30kB/s eta 0:00:10\r",
      "\u001b[K     |█████████████████████▌          | 593kB 30kB/s eta 0:00:10\r",
      "\u001b[K     |█████████████████████▉          | 604kB 30kB/s eta 0:00:10\r",
      "\u001b[K     |██████████████████████▎         | 614kB 30kB/s eta 0:00:09\r",
      "\u001b[K     |██████████████████████▋         | 624kB 30kB/s eta 0:00:09\r",
      "\u001b[K     |███████████████████████         | 634kB 30kB/s eta 0:00:09\r",
      "\u001b[K     |███████████████████████▍        | 645kB 30kB/s eta 0:00:08\r",
      "\u001b[K     |███████████████████████▊        | 655kB 30kB/s eta 0:00:08\r",
      "\u001b[K     |████████████████████████        | 665kB 30kB/s eta 0:00:08\r",
      "\u001b[K     |████████████████████████▌       | 675kB 30kB/s eta 0:00:07\r",
      "\u001b[K     |████████████████████████▉       | 686kB 30kB/s eta 0:00:07\r",
      "\u001b[K     |█████████████████████████▏      | 696kB 30kB/s eta 0:00:07\r",
      "\u001b[K     |█████████████████████████▋      | 706kB 30kB/s eta 0:00:06\r",
      "\u001b[K     |██████████████████████████      | 716kB 30kB/s eta 0:00:06\r",
      "\u001b[K     |██████████████████████████▎     | 727kB 30kB/s eta 0:00:06\r",
      "\u001b[K     |██████████████████████████▊     | 737kB 30kB/s eta 0:00:05\r",
      "\u001b[K     |███████████████████████████     | 747kB 30kB/s eta 0:00:05\r",
      "\u001b[K     |███████████████████████████▌    | 757kB 30kB/s eta 0:00:05\r",
      "\u001b[K     |███████████████████████████▉    | 768kB 30kB/s eta 0:00:04\r",
      "\u001b[K     |████████████████████████████▏   | 778kB 30kB/s eta 0:00:04\r",
      "\u001b[K     |████████████████████████████▋   | 788kB 30kB/s eta 0:00:04\r",
      "\u001b[K     |█████████████████████████████   | 798kB 30kB/s eta 0:00:03\r",
      "\u001b[K     |█████████████████████████████▎  | 808kB 30kB/s eta 0:00:03\r",
      "\u001b[K     |█████████████████████████████▊  | 819kB 30kB/s eta 0:00:03\r",
      "\u001b[K     |██████████████████████████████  | 829kB 30kB/s eta 0:00:02\r",
      "\u001b[K     |██████████████████████████████▍ | 839kB 30kB/s eta 0:00:02\r",
      "\u001b[K     |██████████████████████████████▉ | 849kB 30kB/s eta 0:00:02\r",
      "\u001b[K     |███████████████████████████████▏| 860kB 30kB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▌| 870kB 30kB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 880kB 30kB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 890kB 30kB/s \n",
      "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
      "Collecting tokenizers==0.5.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7MB 9.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.40 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.40)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.40->boto3->transformers) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.40->boto3->transformers) (2.8.1)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.41-cp36-none-any.whl size=893334 sha256=03328ed54d8631c92acce316e47d6e34fb5f414c3b8853c74344893b9cbddc3b\n",
      "  Stored in directory: /root/.cache/pip/wheels/22/5a/d4/b020a81249de7dc63758a34222feaa668dbe8ebfe9170cc9b1\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
      "Successfully installed sacremoses-0.0.41 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.8.0\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.85)\n",
      "Collecting utils\n",
      "  Downloading https://files.pythonhosted.org/packages/55/e6/c2d2b2703e7debc8b501caae0e6f7ead148fd0faa3c8131292a599930029/utils-1.0.1-py2.py3-none-any.whl\n",
      "Installing collected packages: utils\n",
      "Successfully installed utils-1.0.1\n",
      "--2020-04-23 12:41:03--  https://docs.google.com/uc?export=download&id=1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW\n",
      "Resolving docs.google.com (docs.google.com)... 172.217.203.139, 172.217.203.100, 172.217.203.113, ...\n",
      "Connecting to docs.google.com (docs.google.com)|172.217.203.139|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-10-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ouq5vevseujv66pgvfgkeqj9uuirjolp/1587645600000/16970776037313924126/*/1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2020-04-23 12:41:04--  https://doc-10-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ouq5vevseujv66pgvfgkeqj9uuirjolp/1587645600000/16970776037313924126/*/1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW?e=download\n",
      "Resolving doc-10-2s-docs.googleusercontent.com (doc-10-2s-docs.googleusercontent.com)... 172.217.204.132, 2607:f8b0:400c:c15::84\n",
      "Connecting to doc-10-2s-docs.googleusercontent.com (doc-10-2s-docs.googleusercontent.com)|172.217.204.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 955428 (933K) [application/octet-stream]\n",
      "Saving to: ‘data/AD_NMT-master/english-Arabic-both.pkl’\n",
      "\n",
      "data/AD_NMT-master/ 100%[===================>] 933.04K  --.-KB/s    in 0.006s  \n",
      "\n",
      "2020-04-23 12:41:04 (162 MB/s) - ‘data/AD_NMT-master/english-Arabic-both.pkl’ saved [955428/955428]\n",
      "\n",
      "--2020-04-23 12:41:06--  https://docs.google.com/uc?export=download&id=1V8_tp8ZlWUYaX7QQL46t0uSRNrVehSf1\n",
      "Resolving docs.google.com (docs.google.com)... 172.217.203.113, 172.217.203.100, 172.217.203.138, ...\n",
      "Connecting to docs.google.com (docs.google.com)|172.217.203.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-14-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/73oh5kvladenb4e0q297ekj4no7k7r3b/1587645600000/16970776037313924126/*/1V8_tp8ZlWUYaX7QQL46t0uSRNrVehSf1?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2020-04-23 12:41:06--  https://doc-14-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/73oh5kvladenb4e0q297ekj4no7k7r3b/1587645600000/16970776037313924126/*/1V8_tp8ZlWUYaX7QQL46t0uSRNrVehSf1?e=download\n",
      "Resolving doc-14-2s-docs.googleusercontent.com (doc-14-2s-docs.googleusercontent.com)... 172.217.204.132, 2607:f8b0:400c:c15::84\n",
      "Connecting to doc-14-2s-docs.googleusercontent.com (doc-14-2s-docs.googleusercontent.com)|172.217.204.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 95497 (93K) [application/octet-stream]\n",
      "Saving to: ‘data/AD_NMT-master/english-Arabic-test.pkl’\n",
      "\n",
      "data/AD_NMT-master/ 100%[===================>]  93.26K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2020-04-23 12:41:06 (130 MB/s) - ‘data/AD_NMT-master/english-Arabic-test.pkl’ saved [95497/95497]\n",
      "\n",
      "--2020-04-23 12:41:08--  https://docs.google.com/uc?export=download&id=1V7X0qtuDIyjTHY0wh-ZNoVwsiF4lId2e\n",
      "Resolving docs.google.com (docs.google.com)... 172.217.204.101, 172.217.204.102, 172.217.204.139, ...\n",
      "Connecting to docs.google.com (docs.google.com)|172.217.204.101|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-10-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/nl6jkil28so51po8j9adf3tib0cob65n/1587645600000/16970776037313924126/*/1V7X0qtuDIyjTHY0wh-ZNoVwsiF4lId2e?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2020-04-23 12:41:08--  https://doc-10-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/nl6jkil28so51po8j9adf3tib0cob65n/1587645600000/16970776037313924126/*/1V7X0qtuDIyjTHY0wh-ZNoVwsiF4lId2e?e=download\n",
      "Resolving doc-10-2s-docs.googleusercontent.com (doc-10-2s-docs.googleusercontent.com)... 172.217.204.132, 2607:f8b0:400c:c15::84\n",
      "Connecting to doc-10-2s-docs.googleusercontent.com (doc-10-2s-docs.googleusercontent.com)|172.217.204.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 859172 (839K) [application/octet-stream]\n",
      "Saving to: ‘data/AD_NMT-master/english-Arabic-train.pkl’\n",
      "\n",
      "data/AD_NMT-master/ 100%[===================>] 839.04K  --.-KB/s    in 0.008s  \n",
      "\n",
      "2020-04-23 12:41:09 (102 MB/s) - ‘data/AD_NMT-master/english-Arabic-train.pkl’ saved [859172/859172]\n",
      "\n",
      "--2020-04-23 12:41:10--  https://docs.google.com/uc?export=download&id=1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV\n",
      "Resolving docs.google.com (docs.google.com)... 172.217.204.102, 172.217.204.113, 172.217.204.139, ...\n",
      "Connecting to docs.google.com (docs.google.com)|172.217.204.102|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-00-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/91rv341cbjaf3ftcej5bfrobns52u7dj/1587645600000/16970776037313924126/*/1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2020-04-23 12:41:10--  https://doc-00-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/91rv341cbjaf3ftcej5bfrobns52u7dj/1587645600000/16970776037313924126/*/1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV?e=download\n",
      "Resolving doc-00-2s-docs.googleusercontent.com (doc-00-2s-docs.googleusercontent.com)... 172.217.204.132, 2607:f8b0:400c:c15::84\n",
      "Connecting to doc-00-2s-docs.googleusercontent.com (doc-00-2s-docs.googleusercontent.com)|172.217.204.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/octet-stream]\n",
      "Saving to: ‘data/AD_NMT-master/LAV-MSA-2-both.pkl’\n",
      "\n",
      "data/AD_NMT-master/     [ <=>                ]   2.33M  --.-KB/s    in 0.01s   \n",
      "\n",
      "2020-04-23 12:41:11 (243 MB/s) - ‘data/AD_NMT-master/LAV-MSA-2-both.pkl’ saved [2447014]\n",
      "\n",
      "--2020-04-23 12:41:12--  https://docs.google.com/uc?export=download&id=1UpfCbkxhztof7dvNjeAs1bHjD4SER6h3\n",
      "Resolving docs.google.com (docs.google.com)... 172.217.204.101, 172.217.204.102, 172.217.204.113, ...\n",
      "Connecting to docs.google.com (docs.google.com)|172.217.204.101|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-0c-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/6blutrm38lsft49t4522eri5u3a8avc3/1587645600000/16970776037313924126/*/1UpfCbkxhztof7dvNjeAs1bHjD4SER6h3?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2020-04-23 12:41:12--  https://doc-0c-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/6blutrm38lsft49t4522eri5u3a8avc3/1587645600000/16970776037313924126/*/1UpfCbkxhztof7dvNjeAs1bHjD4SER6h3?e=download\n",
      "Resolving doc-0c-2s-docs.googleusercontent.com (doc-0c-2s-docs.googleusercontent.com)... 172.217.204.132, 2607:f8b0:400c:c15::84\n",
      "Connecting to doc-0c-2s-docs.googleusercontent.com (doc-0c-2s-docs.googleusercontent.com)|172.217.204.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 304332 (297K) [application/octet-stream]\n",
      "Saving to: ‘data/AD_NMT-master/LAV-MSA-2-test.pkl’\n",
      "\n",
      "data/AD_NMT-master/ 100%[===================>] 297.20K  --.-KB/s    in 0.002s  \n",
      "\n",
      "2020-04-23 12:41:12 (144 MB/s) - ‘data/AD_NMT-master/LAV-MSA-2-test.pkl’ saved [304332/304332]\n",
      "\n",
      "--2020-04-23 12:41:14--  https://docs.google.com/uc?export=download&id=1UlAZGtYsSfXzK7hrC_PbxQFqTSXD0DMw\n",
      "Resolving docs.google.com (docs.google.com)... 172.217.203.113, 172.217.203.100, 172.217.203.138, ...\n",
      "Connecting to docs.google.com (docs.google.com)|172.217.203.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-0c-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/oc5o3gn8nvm5fhrgn09fc40viri7jhe3/1587645600000/16970776037313924126/*/1UlAZGtYsSfXzK7hrC_PbxQFqTSXD0DMw?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2020-04-23 12:41:14--  https://doc-0c-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/oc5o3gn8nvm5fhrgn09fc40viri7jhe3/1587645600000/16970776037313924126/*/1UlAZGtYsSfXzK7hrC_PbxQFqTSXD0DMw?e=download\n",
      "Resolving doc-0c-2s-docs.googleusercontent.com (doc-0c-2s-docs.googleusercontent.com)... 172.217.204.132, 2607:f8b0:400c:c15::84\n",
      "Connecting to doc-0c-2s-docs.googleusercontent.com (doc-0c-2s-docs.googleusercontent.com)|172.217.204.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/octet-stream]\n",
      "Saving to: ‘data/AD_NMT-master/LAV-MSA-2-train.pkl’\n",
      "\n",
      "data/AD_NMT-master/     [ <=>                ]   2.04M  --.-KB/s    in 0.01s   \n",
      "\n",
      "2020-04-23 12:41:15 (214 MB/s) - ‘data/AD_NMT-master/LAV-MSA-2-train.pkl’ saved [2141923]\n",
      "\n",
      "--2020-04-23 12:41:15--  https://docs.google.com/uc?export=download&id=1UjDX7cCG2S23SPfSHxSPdVayMTxB5Y16\n",
      "Resolving docs.google.com (docs.google.com)... 172.217.203.101, 172.217.203.139, 172.217.203.102, ...\n",
      "Connecting to docs.google.com (docs.google.com)|172.217.203.101|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-04-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/d9g1ah7mi3lgspj8cmktd3qmrkah11k0/1587645675000/16970776037313924126/*/1UjDX7cCG2S23SPfSHxSPdVayMTxB5Y16?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2020-04-23 12:41:16--  https://doc-04-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/d9g1ah7mi3lgspj8cmktd3qmrkah11k0/1587645675000/16970776037313924126/*/1UjDX7cCG2S23SPfSHxSPdVayMTxB5Y16?e=download\n",
      "Resolving doc-04-2s-docs.googleusercontent.com (doc-04-2s-docs.googleusercontent.com)... 172.217.204.132, 2607:f8b0:400c:c15::84\n",
      "Connecting to doc-04-2s-docs.googleusercontent.com (doc-04-2s-docs.googleusercontent.com)|172.217.204.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/octet-stream]\n",
      "Saving to: ‘data/AD_NMT-master/Magribi_MSA-both.pkl’\n",
      "\n",
      "data/AD_NMT-master/     [ <=>                ]   2.81M  --.-KB/s    in 0.01s   \n",
      "\n",
      "2020-04-23 12:41:16 (223 MB/s) - ‘data/AD_NMT-master/Magribi_MSA-both.pkl’ saved [2944107]\n",
      "\n",
      "--2020-04-23 12:41:17--  https://docs.google.com/uc?export=download&id=1UaVWIqRXo0rxuxDF4KArA4bEK1TaLX3l\n",
      "Resolving docs.google.com (docs.google.com)... 172.217.203.139, 172.217.203.101, 172.217.203.113, ...\n",
      "Connecting to docs.google.com (docs.google.com)|172.217.203.139|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-0o-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/01fom3nbt4a0pdva8qab2f8qq20o3e6a/1587645675000/16970776037313924126/*/1UaVWIqRXo0rxuxDF4KArA4bEK1TaLX3l?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2020-04-23 12:41:18--  https://doc-0o-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/01fom3nbt4a0pdva8qab2f8qq20o3e6a/1587645675000/16970776037313924126/*/1UaVWIqRXo0rxuxDF4KArA4bEK1TaLX3l?e=download\n",
      "Resolving doc-0o-2s-docs.googleusercontent.com (doc-0o-2s-docs.googleusercontent.com)... 172.217.204.132, 2607:f8b0:400c:c15::84\n",
      "Connecting to doc-0o-2s-docs.googleusercontent.com (doc-0o-2s-docs.googleusercontent.com)|172.217.204.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 290840 (284K) [application/octet-stream]\n",
      "Saving to: ‘data/AD_NMT-master/Magribi_MSA-test.pkl’\n",
      "\n",
      "data/AD_NMT-master/ 100%[===================>] 284.02K  --.-KB/s    in 0.002s  \n",
      "\n",
      "2020-04-23 12:41:18 (113 MB/s) - ‘data/AD_NMT-master/Magribi_MSA-test.pkl’ saved [290840/290840]\n",
      "\n",
      "--2020-04-23 12:41:19--  https://docs.google.com/uc?export=download&id=1UYvlhdYAdfa4riP_4hn3-IEVd1ZUXVTQ\n",
      "Resolving docs.google.com (docs.google.com)... 172.217.204.102, 172.217.204.113, 172.217.204.139, ...\n",
      "Connecting to docs.google.com (docs.google.com)|172.217.204.102|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-0s-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/gg0bilsc4g5jbhg59tmu3ad2dk93np5v/1587645675000/16970776037313924126/*/1UYvlhdYAdfa4riP_4hn3-IEVd1ZUXVTQ?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2020-04-23 12:41:20--  https://doc-0s-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/gg0bilsc4g5jbhg59tmu3ad2dk93np5v/1587645675000/16970776037313924126/*/1UYvlhdYAdfa4riP_4hn3-IEVd1ZUXVTQ?e=download\n",
      "Resolving doc-0s-2s-docs.googleusercontent.com (doc-0s-2s-docs.googleusercontent.com)... 172.217.204.132, 2607:f8b0:400c:c15::84\n",
      "Connecting to doc-0s-2s-docs.googleusercontent.com (doc-0s-2s-docs.googleusercontent.com)|172.217.204.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/octet-stream]\n",
      "Saving to: ‘data/AD_NMT-master/Magribi_MSA-train.pkl’\n",
      "\n",
      "data/AD_NMT-master/     [ <=>                ]   2.53M  --.-KB/s    in 0.02s   \n",
      "\n",
      "2020-04-23 12:41:20 (108 MB/s) - ‘data/AD_NMT-master/Magribi_MSA-train.pkl’ saved [2652508]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#installation step\n",
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install utils\n",
    "#creating the folders \n",
    "!mkdir data/\n",
    "!mkdir data/AD_NMT-master\n",
    "!mkdir data/train/\n",
    "!mkdir data/test/\n",
    "!mkdir data/val/\n",
    "!mkdir data/vocab/\n",
    "!mkdir data/model/\n",
    "#fetching the pkl files\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW' -O data/AD_NMT-master/english-Arabic-both.pkl\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1V8_tp8ZlWUYaX7QQL46t0uSRNrVehSf1' -O data/AD_NMT-master/english-Arabic-test.pkl\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1V7X0qtuDIyjTHY0wh-ZNoVwsiF4lId2e' -O data/AD_NMT-master/english-Arabic-train.pkl\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV' -O data/AD_NMT-master/LAV-MSA-2-both.pkl\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UpfCbkxhztof7dvNjeAs1bHjD4SER6h3' -O data/AD_NMT-master/LAV-MSA-2-test.pkl\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UlAZGtYsSfXzK7hrC_PbxQFqTSXD0DMw' -O data/AD_NMT-master/LAV-MSA-2-train.pkl\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UjDX7cCG2S23SPfSHxSPdVayMTxB5Y16' -O data/AD_NMT-master/Magribi_MSA-both.pkl\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UaVWIqRXo0rxuxDF4KArA4bEK1TaLX3l' -O data/AD_NMT-master/Magribi_MSA-test.pkl\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UYvlhdYAdfa4riP_4hn3-IEVd1ZUXVTQ' -O data/AD_NMT-master/Magribi_MSA-train.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-WzhRv4mIKNq"
   },
   "outputs": [],
   "source": [
    "#James Chartouni\n",
    "#Joey Park\n",
    "#Raef Khan\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os, io, glob\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtext.datasets.translation import TranslationDataset\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config, T5Model\n",
    "\n",
    "\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.vocab import Vocab\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T67vZ0Nv4-uw"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "aha0xureIKNw",
    "outputId": "5d7fd491-112e-4d7e-a35d-0bef23512fdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAV-MSA-2-both.pkl        Magribi_MSA-train.pkl     english-arabic-test.pkl\r\n",
      "LAV-MSA-2-test.pkl        README.md                 lav_formatted_.txt\r\n",
      "LAV-MSA-2-train.pkl       en_formatted_.txt         mag_formatted_.txt\r\n",
      "Magribi_MSA-both.pkl      english-Arabic-both.pkl   msa_formatted_.txt\r\n",
      "Magribi_MSA-test.pkl      english-Arabic-train.pkl\r\n"
     ]
    }
   ],
   "source": [
    "ls data/AD_NMT-master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XWKnGRlLIKN9"
   },
   "outputs": [],
   "source": [
    "file_path = 'data/AD_NMT-master/'\n",
    "\n",
    "with open(file_path + \"english-Arabic-train.pkl\", 'rb') as handle:\n",
    "    data_English_MSA_trainval = pickle.load(handle)\n",
    "\n",
    "with open(file_path + \"english-Arabic-test.pkl\", 'rb') as handle:\n",
    "    data_English_MSA_test = pickle.load(handle)\n",
    "\n",
    "with open(file_path + \"english-Arabic-both.pkl\", 'rb') as handle:\n",
    "    data_English_MSA_both = pickle.load(handle) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(file_path + \"LAV-MSA-2-train.pkl\", 'rb') as handle:\n",
    "    data_LAV_MSA_trainval = pickle.load(handle) \n",
    "\n",
    "with open(file_path + \"LAV-MSA-2-test.pkl\", 'rb') as handle:\n",
    "    data_LAV_MSA_test = pickle.load(handle) \n",
    "\n",
    "with open(file_path + \"LAV-MSA-2-both.pkl\", 'rb') as handle:\n",
    "    data_LAV_MSA_both = pickle.load(handle) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(file_path + \"Magribi_MSA-train.pkl\", 'rb') as handle:\n",
    "    data_Magribi_MSA_trainval = pickle.load(handle) \n",
    "    \n",
    "with open(file_path + \"Magribi_MSA-test.pkl\", 'rb') as handle:\n",
    "    data_Magribi_MSA_test = pickle.load(handle) \n",
    "\n",
    "with open(file_path + \"Magribi_MSA-both.pkl\", 'rb') as handle:\n",
    "    data_Magribi_MSA_both = pickle.load(handle) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "Ch3APJadIKOH",
    "outputId": "560319c8-3dc1-4003-fc3b-d2324e794050"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Tom was also there', 'كان توم هنا ايضا'], ['That old woman lives by herself', 'تلك المراة العجوز تسكن بمفردها'], ['He went abroad for the purpose of studying English', 'سافر خارج البلد ليتعلم الانجليزية'], ['There is a fork missing', 'هناك شوكة ناقصة'], [\"I don't know this game\", 'لا اعرف هذه اللعبة']]\n",
      "[['لا انا بعرف وحدة راحت ع فرنسا و معا شنتا حطت فيها الفرش', 'لا اعرف واحدة ذهبت الى فرنسا و لها غرفة و ضعت فيها الافرشة'], ['روح بوشك و فتول عاليسار', 'اذهب تقدم و استدر يسارا'], ['لا لا لازم انه يكون عندك موضوع ما في اشي', ' لا لا يجب ان يكون لديك موضوع هذا ضروري'], ['اوعي تبعدي من هون بلاش تضيعي ', 'لا تبتعد عن هنا حتى لا تفقد الطريق '], ['قصدي صراحة يما انا كمان كرهته من يوم ما عملتيه زي ما بتعمله خالتي كرهته و صرت ما باطيقه بالمرة', 'اقصد صراحة يا امي انا ايضا كرهته من يوم حضرته مثلما تحضره خالتي كرهته و اصبحت لا اطيقه ابدا']]\n",
      "9000\n",
      "10001\n"
     ]
    }
   ],
   "source": [
    "print(data_English_MSA_both[0:5])\n",
    "print(data_LAV_MSA_both[0:5])\n",
    "print(len(data_English_MSA_trainval))\n",
    "print(len(data_English_MSA_both))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "irQQ9E_7IKOM"
   },
   "source": [
    "## Prepare Datasets\n",
    "\n",
    "example: https://iwslt2010.fbk.eu/node/32/\n",
    "\n",
    "We need to take our training and test sets from the pkl files and create new .txt files that are formatted so that the standard torchtext Dataset class can read them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "raw",
    "id": "ikpu9B5dIKON"
   },
   "source": [
    "Data format:\n",
    "each line consists of three fields divided by the character '\\'\n",
    "sentences consisting of words divided by single spaces\n",
    "format: <SENTENCE_ID>\\<PARAPHRASE_ID>\\<TEXT>\n",
    "Field_1: sentence ID\n",
    "Field_2: paraphrase ID\n",
    "Field_3: MT develop sentence / reference translation\n",
    "Text input example:\n",
    "DEV_001\\01\\This is the first develop sentence.\n",
    "DEV_002\\01\\This is the second develop sentence.\n",
    "Reference translation example:\n",
    "DEV_001\\01\\1st reference translation for 1st input\n",
    "DEV_001\\02\\2nd reference translation for 1st input\n",
    "...\n",
    "DEV_002\\01\\1st reference translation for 2nd input\n",
    "DEV_002\\02\\2nd reference translation for 2nd input\n",
    "...\n",
    "Languages:\n",
    "Arabic-English\n",
    "CSTAR03 testset: 506 sentences, 16 reference translations\n",
    "IWSLT04 testset: 500 sentences, 16 reference translations\n",
    "IWSLT05 testset: 506 sentences, 16 reference translations\n",
    "IWSLT07 testset: 489 sentences, 6 reference translations\n",
    "IWSLT08 testset: 507 sentences, 16 reference translations\n",
    "French-English\n",
    "CSTAR03 testset: 506 sentences, 16 reference translations\n",
    "IWSLT04 testset: 500 sentences, 16 reference translations\n",
    "IWSLT05 testset: 506 sentences, 16 reference translations\n",
    "Turkish-English\n",
    "CSTAR03 testset: 506 sentences, 16 reference translations\n",
    "IWSLT04 testset: 500 sentences, 16 reference translations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "7wcnDJDKwqED",
    "outputId": "319a799d-eb12-498a-d919-5354624dad11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAV-MSA-2-both.pkl        Magribi_MSA-train.pkl     english-arabic-test.pkl\r\n",
      "LAV-MSA-2-test.pkl        README.md                 lav_formatted_.txt\r\n",
      "LAV-MSA-2-train.pkl       en_formatted_.txt         mag_formatted_.txt\r\n",
      "Magribi_MSA-both.pkl      english-Arabic-both.pkl   msa_formatted_.txt\r\n",
      "Magribi_MSA-test.pkl      english-Arabic-train.pkl\r\n"
     ]
    }
   ],
   "source": [
    "ls data/AD_NMT-master/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lp7ncVY6wqET"
   },
   "outputs": [],
   "source": [
    "#splits the train dataset into train and validation sets, define test set as datafile\n",
    "eng_msa_train, eng_msa_val = train_test_split(data_English_MSA_trainval, test_size=.2, random_state=22)\n",
    "eng_msa_test = data_English_MSA_test\n",
    "\n",
    "lav_msa_train, lav_msa_val = train_test_split(data_LAV_MSA_trainval, test_size=.2, random_state=22)\n",
    "lav_msa_test = data_LAV_MSA_test\n",
    "\n",
    "mag_msa_train, mag_msa_val = train_test_split(data_Magribi_MSA_trainval, test_size=.2, random_state=22)\n",
    "mag_msa_test = data_Magribi_MSA_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "9Ue2fSILwqEW",
    "outputId": "04d6b2f9-f91f-4731-a589-d6d83d02b52a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200\n",
      "1800\n",
      "11044\n",
      "2761\n",
      "14188\n",
      "3548\n"
     ]
    }
   ],
   "source": [
    "print(len(eng_msa_train))\n",
    "print(len(eng_msa_val))\n",
    "\n",
    "print(len(lav_msa_train))\n",
    "print(len(lav_msa_val))\n",
    "\n",
    "print(len(mag_msa_train))\n",
    "print(len(mag_msa_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W3bWU46TIKOb"
   },
   "outputs": [],
   "source": [
    "file_path = 'data/'\n",
    "\n",
    "def pytorch_format(ds, src='en', trg='msa', datatype=''):\n",
    "    src_formatted = datatype + '_' + src + '_' + trg + '.' + src\n",
    "    trg_formatted = datatype + '_' + src + '_' + trg + '.' + trg\n",
    "    \n",
    "    with open(file_path + datatype + \"/\" + src_formatted, 'wt') as srctxt, open(file_path + datatype + \"/\" + trg_formatted, 'wt') as trgtxt:\n",
    "        for i, arr in enumerate(ds):\n",
    "            srctxt.write(datatype.upper() + '_' + str(i).zfill( len(str(len(ds))) - len(str(i))) + '\\\\01\\\\' + arr[0] + '\\n')\n",
    "            trgtxt.write(datatype.upper() + '_' + str(i).zfill( len(str(len(ds))) - len(str(i))) + '\\\\01\\\\' + arr[1] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AqwDB9BuKEnV"
   },
   "outputs": [],
   "source": [
    "#splits each language pair file into datasets of single language, to be merged again by the pytorch dataset class later\n",
    "\n",
    "pytorch_format(eng_msa_train, 'eng', 'msa', 'train')\n",
    "pytorch_format(eng_msa_val, 'eng', 'msa', 'val')\n",
    "pytorch_format(eng_msa_test, 'eng', 'msa', 'test')\n",
    "\n",
    "pytorch_format(lav_msa_train, 'lav', 'msa', 'train')\n",
    "pytorch_format(lav_msa_val, 'lav', 'msa', 'val')\n",
    "pytorch_format(lav_msa_test, 'lav', 'msa', 'test')\n",
    "\n",
    "pytorch_format(mag_msa_train, 'mag', 'msa', 'train')\n",
    "pytorch_format(mag_msa_val, 'mag', 'msa', 'val')\n",
    "pytorch_format(mag_msa_test, 'mag', 'msa', 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lDPvs9zxIKO3"
   },
   "source": [
    "## Build Vocabulary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ihevk2SWIKO5"
   },
   "source": [
    "Sentence Piece Google Colab\n",
    "https://github.com/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I25H3IsyIKPB"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Testing:\n",
    "Create a text file with all the vocab available from all sources for each language for SentencePiece to create a library \n",
    "\n",
    "TODO: implement bpemb model for MSA vocab/train SPM on larger datasets for dialects\n",
    "\"\"\"\n",
    "\n",
    "en_vocab = open(\"data/vocab/eng_vocab.txt\", \"wt\")\n",
    "msa_vocab = open(\"data/vocab/msa_vocab.txt\", \"wt\")\n",
    "lav_vocab = open(\"data/vocab/lav_vocab.txt\", \"wt\")\n",
    "mag_vocab = open(\"data/vocab/mag_vocab.txt\", \"wt\")\n",
    "\n",
    "MSA_text = \"\"\n",
    "EN_text = \"\"\n",
    "\n",
    "def create_vocab(file='', src='en_vocab', tgt='msa_vocab'):\n",
    "  for line in file:\n",
    "        src_sent = line[0]\n",
    "        src_words = src_sent.split(\" \")\n",
    "        for count, word in enumerate(src_words):\n",
    "            src.write(word)\n",
    "        src.write(\"\\n\")\n",
    "        \n",
    "        tgt_sent = line[1]\n",
    "        tgt_words = tgt_sent.split(\" \")\n",
    "        for count, word in enumerate(tgt_words):\n",
    "            tgt.write(word)\n",
    "        tgt.write(\"\\n\")\n",
    "\n",
    "create_vocab(data_English_MSA_both, en_vocab, msa_vocab)\n",
    "create_vocab(data_LAV_MSA_both, lav_vocab, msa_vocab)\n",
    "create_vocab(data_Magribi_MSA_both, mag_vocab, msa_vocab)\n",
    "\n",
    "en_vocab.close()\n",
    "msa_vocab.close()\n",
    "lav_vocab.close()\n",
    "mag_vocab.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Q0qG8N68IKPI",
    "outputId": "f850842c-8c28-445a-88c7-9e62fcece427"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spm.SentencePieceTrainer.train('--input=data/vocab/eng_vocab.txt,data/vocab/msa_vocab.txt,data/vocab/lav_vocab.txt,data/vocab/mag_vocab.txt --model_prefix=data/model/spm --vocab_size=' + str(VOCAB_SIZE))\n",
    "spm.SentencePieceTrainer.train('--input=data/vocab/eng_vocab.txt --model_prefix=data/model/eng --vocab_size=' + str(VOCAB_SIZE))\n",
    "spm.SentencePieceTrainer.train('--input=data/vocab/msa_vocab.txt --model_prefix=data/model/msa --vocab_size=' + str(VOCAB_SIZE))\n",
    "spm.SentencePieceTrainer.train('--input=data/vocab/lav_vocab.txt --model_prefix=data/model/lav --vocab_size=' + str(VOCAB_SIZE))\n",
    "spm.SentencePieceTrainer.train('--input=data/vocab/mag_vocab.txt --model_prefix=data/model/mag --vocab_size=' + str(VOCAB_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=data/vocab/eng_vocab.txt,data/vocab/msa_vocab.txt,data/vocab/lav_vocab.txt,data/vocab/mag_vocab.txt --model_prefix=data/model/spm --vocab_size=' + str(VOCAB_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "BOuH1-Tl4-vt",
    "outputId": "8a77b489-02fd-4835-e6e6-ab5b1ec11191"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng.model  lav.model  mag.model  msa.model  spm.model\r\n",
      "eng.vocab  lav.vocab  mag.vocab  msa.vocab  spm.vocab\r\n"
     ]
    }
   ],
   "source": [
    "ls data/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "y05LILRSIKPN",
    "outputId": "f152e018-c63e-4383-b82f-95d58f5c352e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('data/model/spm.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Egx5FaOsIKPW",
    "outputId": "4c001726-3189-49e9-ea35-70917a5a2a68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁This', '▁', 'is', '▁', 'a', '▁', 't', 'est']\n",
      "[1256, 3, 123, 3, 64, 3, 49, 2328]\n"
     ]
    }
   ],
   "source": [
    "print(sp.encode_as_pieces('This is a test'))\n",
    "print(sp.encode_as_ids('This is a test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁هناك', '▁شو', 'ك', 'ة', '▁', 'نا', 'قصة']\n",
      "[818, 931, 11, 12, 3, 23, 1167]\n"
     ]
    }
   ],
   "source": [
    "print(sp.encode_as_pieces('هناك شوكة ناقصة'))\n",
    "print(sp.encode_as_ids('هناك شوكة ناقصة'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sp.encode_as_pieces('This is a test'))\n",
    "print(sp.encode_as_ids('This is a test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vPy6YE06wqFA"
   },
   "source": [
    "## Spacy Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_U6-4yB1wqFG"
   },
   "outputs": [],
   "source": [
    "eng_vocab_data = open(\"data/model/eng.vocab\", \"r\")\n",
    "msa_vocab_data = open(\"data/model/msa.vocab\", \"r\")\n",
    "lav_vocab_data = open(\"data/model/lav.vocab\", \"r\")\n",
    "mag_vocab_data = open(\"data/model/mag.vocab\", \"r\")\n",
    "spm_vocab_data = open(\"data/model/spm.vocab\", \"r\")\n",
    "\n",
    "eng_vocab_list = []\n",
    "msa_vocab_list = []\n",
    "lav_vocab_list = []\n",
    "mag_vocab_list = []\n",
    "spm_vocab_list = []\n",
    "\n",
    "for line in eng_vocab_data.readlines():\n",
    "    eng_vocab_list.append(line.split(\"\\t\")[0])\n",
    "\n",
    "for line in msa_vocab_data.readlines():\n",
    "    msa_vocab_list.append(line.split(\"\\t\")[0])\n",
    "\n",
    "for line in lav_vocab_data.readlines():\n",
    "    lav_vocab_list.append(line.split(\"\\t\")[0])\n",
    "\n",
    "for line in mag_vocab_data.readlines():\n",
    "    mag_vocab_list.append(line.split(\"\\t\")[0])\n",
    "    \n",
    "for line in spm_vocab_data.readlines():\n",
    "    spm_vocab_list.append(line.split(\"\\t\")[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9IkkH0xQwqFK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " '<s>',\n",
       " '</s>',\n",
       " '▁',\n",
       " 'ا',\n",
       " 'و',\n",
       " 'ي',\n",
       " 'ت',\n",
       " 'ال',\n",
       " 'ب',\n",
       " 'م',\n",
       " 'ك',\n",
       " 'ة',\n",
       " 'ه',\n",
       " 'ن',\n",
       " 'ل',\n",
       " 'في',\n",
       " 'ها',\n",
       " 'من',\n",
       " 'ما',\n",
       " 'ش',\n",
       " 'ان',\n",
       " 'ف',\n",
       " 'نا',\n",
       " '؟',\n",
       " 'ين',\n",
       " 'وا',\n",
       " 'ون',\n",
       " 'لي',\n",
       " 'لا',\n",
       " 'س',\n",
       " 'ع',\n",
       " 'على',\n",
       " \"'\",\n",
       " 'ني',\n",
       " 's',\n",
       " 'انا',\n",
       " 'هم',\n",
       " '▁و',\n",
       " 'ى',\n",
       " 'هو',\n",
       " 'ح',\n",
       " 'د',\n",
       " 'انت',\n",
       " 'ق',\n",
       " 'يا',\n",
       " 'ات',\n",
       " '▁لا',\n",
       " 'فيال',\n",
       " 't',\n",
       " 'هذا',\n",
       " '▁انا',\n",
       " 'الم',\n",
       " '?',\n",
       " '▁نعم',\n",
       " 'مع',\n",
       " 'ر',\n",
       " 'ز',\n",
       " 'عن',\n",
       " 'بال',\n",
       " 'سي',\n",
       " '▁I',\n",
       " 'هي',\n",
       " '▁ا',\n",
       " 'a',\n",
       " 'كان',\n",
       " 'ج',\n",
       " 'خ',\n",
       " 'ص',\n",
       " 'انه',\n",
       " 'كل',\n",
       " 'منال',\n",
       " 'تي',\n",
       " 'غ',\n",
       " 'لل',\n",
       " 'لك',\n",
       " 'ط',\n",
       " 'ية',\n",
       " 'حتى',\n",
       " 'to',\n",
       " 'سا',\n",
       " 'بس',\n",
       " 'او',\n",
       " 'ذلك',\n",
       " 'بي',\n",
       " 'ض',\n",
       " 'اني',\n",
       " 'm',\n",
       " 'شي',\n",
       " 'اليوم',\n",
       " 'اذا',\n",
       " 'ار',\n",
       " 'الى',\n",
       " 'واحد',\n",
       " 'ست',\n",
       " 'لو',\n",
       " 'هناك',\n",
       " 'بت',\n",
       " 'اء',\n",
       " 'كنت',\n",
       " 'كم',\n",
       " 'وال',\n",
       " 'غير',\n",
       " 'ور',\n",
       " 'ولا',\n",
       " 'اللي',\n",
       " 'مت',\n",
       " '▁اه',\n",
       " 'لما',\n",
       " 'the',\n",
       " 'you',\n",
       " 'بعد',\n",
       " 'كي',\n",
       " 'الله',\n",
       " 'هنا',\n",
       " 'هل',\n",
       " '▁هل',\n",
       " 'عندما',\n",
       " 'يت',\n",
       " 'اد',\n",
       " 'الان',\n",
       " '▁ايه',\n",
       " 'd',\n",
       " 'is',\n",
       " 'جدا',\n",
       " 'ing',\n",
       " 'كانت',\n",
       " 'مش',\n",
       " 'لكن',\n",
       " 'كيف',\n",
       " 'ام',\n",
       " 'علىال',\n",
       " 'r',\n",
       " 'فى',\n",
       " 'ذ',\n",
       " 'الا',\n",
       " 'تو',\n",
       " 'سليمة',\n",
       " 'انها',\n",
       " 'me',\n",
       " 'كتير',\n",
       " 'الي',\n",
       " 'لات',\n",
       " 'ماذا',\n",
       " 'ed',\n",
       " 'فيها',\n",
       " 'مو',\n",
       " 'تها',\n",
       " 'n',\n",
       " '▁هذا',\n",
       " 'قال',\n",
       " 'اي',\n",
       " 'it',\n",
       " 'ير',\n",
       " 'in',\n",
       " 'ئ',\n",
       " '▁لكن',\n",
       " 'ته',\n",
       " 'شيء',\n",
       " 'e',\n",
       " 'توم',\n",
       " 'وش',\n",
       " 'له',\n",
       " 'فقط',\n",
       " 'فيه',\n",
       " 'ائ',\n",
       " 'ولكن',\n",
       " 'جد',\n",
       " 'يكون',\n",
       " 'اب',\n",
       " 'عمل',\n",
       " 'بر',\n",
       " 'تك',\n",
       " 're',\n",
       " 'قبل',\n",
       " 'لها',\n",
       " 'هيك',\n",
       " 'c',\n",
       " 'تر',\n",
       " '▁ما',\n",
       " 'وقت',\n",
       " '▁اي',\n",
       " 'اس',\n",
       " 'عند',\n",
       " 'دي',\n",
       " 'هذه',\n",
       " 'اخر',\n",
       " 'الذي',\n",
       " 'اش',\n",
       " '▁قالتلي',\n",
       " 'وم',\n",
       " 'انك',\n",
       " 'دو',\n",
       " '▁انت',\n",
       " 'درس',\n",
       " 'ول',\n",
       " 'ايضا',\n",
       " 'f',\n",
       " 'ث',\n",
       " 'مات']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm_vocab_list[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MpMYD4zGwqFP"
   },
   "outputs": [],
   "source": [
    "en_vocab = Vocab(strings=eng_vocab_list)\n",
    "spacy_en_tokenizer = Tokenizer(en_vocab)\n",
    "\n",
    "msa_vocab = Vocab(strings=msa_vocab_list)\n",
    "spacy_msa_tokenizer = Tokenizer(msa_vocab)\n",
    "\n",
    "lav_vocab = Vocab(strings=lav_vocab_list)\n",
    "spacy_lav_tokenizer = Tokenizer(lav_vocab)\n",
    "\n",
    "mag_vocab = Vocab(strings=mag_vocab_list)\n",
    "spacy_mag_tokenizer = Tokenizer(mag_vocab)\n",
    "\n",
    "spm_vocab = Vocab(strings=mag_vocab_list)\n",
    "spacy_spm_tokenizer = Tokenizer(spm_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0uDxsOI4IKPd"
   },
   "source": [
    "## TF Tokenizer\n",
    "\n",
    "https://huggingface.co/transformers/model_doc/t5.html#t5tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zDuf6tVBIKPe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng.model  lav.model  mag.model  msa.model  spm.model\r\n",
      "eng.vocab  lav.vocab  mag.vocab  msa.vocab  spm.vocab\r\n"
     ]
    }
   ],
   "source": [
    "ls data/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LH26oj-KIKPl"
   },
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer('data/model/spm.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "txeYu0e0IKPs"
   },
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode('تعلمت كيفية ركوب الدراجة عندما كان عمري ست سنوات ', return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D7IuJeL0IKPx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   3,  320,    7,  505,   77,    3, 1062,  233,    3,    8, 3872,  800,\n",
       "          440,    3, 2065,    3,   94,    3, 2168]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fi6rWTe0wqFc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "تعلمت كيفية ركوب الدراجة عندما كان عمري ست سنوات\n"
     ]
    }
   ],
   "source": [
    "#decode to make sure you can go back and forth between the encoding properly \n",
    "#@raef can you verify the Arabic?\n",
    "output = tokenizer.decode(input_ids[0])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z2TpLlDln28C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4024, 159, 3, 1657, 3, 79, 729, 132, 2324, 915, 1117, 302, 3, 574, 2332, 35, 3, 235, 173, 1100, 3, 1222, 255, 255, 326, 35]\n",
      "[749, 47, 3, 355, 3, 238, 15, 1060, 3, 8, 777, 619]\n"
     ]
    }
   ],
   "source": [
    "#from transformers import T5WithLMHeadModel\n",
    "#model = T5WithLMHeadModel.from_pretrained('t5-small')\n",
    "input_ids = tokenizer.encode('translate English to Arabic: She hates green peppers', return_tensor='pt')\n",
    "lm_labels = tokenizer.encode('انها لا تحب الفلفل الاخضر', return_tensor='pt')\n",
    "\n",
    "print(input_ids)\n",
    "print(lm_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UuAey7jyIKP1"
   },
   "source": [
    "## Pytorch Data Set and Data Loader\n",
    "\n",
    "https://github.com/google-research/text-to-text-transfer-transformer\n",
    "\n",
    "pytorch dataset: https://pytorch.org/text/_modules/torchtext/datasets/translation.html\n",
    "\n",
    "pytorch dataset documentation: https://torchtext.readthedocs.io/en/latest/datasets.html#iwslt\n",
    "\n",
    "example dataset: https://iwslt2010.fbk.eu/node/32/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "god7BjRnwqFh"
   },
   "source": [
    "Field API: https://pytorch.org/text/data.html#torchtext.data.Field\n",
    "Field Source: https://pytorch.org/text/_modules/torchtext/data/field.html#Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TmvgEGoLwqFi"
   },
   "source": [
    "https://pytorch.org/text/_modules/torchtext/data/utils.html#get_tokenizer\n",
    "    \n",
    "we need to pass our tokenizer as a function     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fVIqommkIKQF"
   },
   "outputs": [],
   "source": [
    "SRC = Field(tokenize = spacy_spm_tokenizer,\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = False)\n",
    "\n",
    "TRG = Field(tokenize = spacy_spm_tokenizer,\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = False)\n",
    "\n",
    "# SRC = Field(tokenize = spacy_lav_tokenizer,\n",
    "#             init_token = '<sos>',\n",
    "#             eos_token = '<eos>',\n",
    "#             lower = False)\n",
    "\n",
    "# SRC = Field(tokenize = spacy_mag_tokenizer,\n",
    "#             init_token = '<sos>',\n",
    "#             eos_token = '<eos>',\n",
    "#             lower = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UiS3YbFGwqFl"
   },
   "outputs": [],
   "source": [
    "eng_msa_train_dataset = TranslationDataset(path='data/train/train_eng_msa.', exts=('eng', 'msa'), fields=(SRC, TRG))\n",
    "lav_msa_train_dataset = TranslationDataset(path='data/train/train_lav_msa.', exts=('lav', 'msa'), fields=(SRC, TRG))\n",
    "mag_msa_train_dataset = TranslationDataset(path='data/train/train_mag_msa.', exts=('mag', 'msa'), fields=(SRC, TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dEnHMRQxwqFo"
   },
   "outputs": [],
   "source": [
    "SRC.build_vocab(eng_msa_train_dataset)\n",
    "TRG.build_vocab(eng_msa_train_dataset)\n",
    "SRC.build_vocab(lav_msa_train_dataset)\n",
    "TRG.build_vocab(lav_msa_train_dataset)\n",
    "SRC.build_vocab(mag_msa_train_dataset)\n",
    "TRG.build_vocab(mag_msa_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "08ZvS7DYwqFq"
   },
   "outputs": [],
   "source": [
    "ex = eng_msa_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "SAUEnQd6wqFu",
    "outputId": "44c82d4b-e548-477a-9514-e4b42dfbe186"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN_000\\01\\I don't remember what happened anymore\n",
      "TRAIN_000\\01\\لم اعد اتذكر ما حصل\n"
     ]
    }
   ],
   "source": [
    "print(ex.src) \n",
    "print(ex.trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "2nsz1PhXwqF2",
    "outputId": "d2f61c41-2c0a-463c-a6e0-f569aeaf5353"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_formatted_.txt   train_eng_msa.eng   train_lav_msa.msa\r\n",
      "lav_formatted_.txt  train_eng_msa.msa   train_mag_msa.mag\r\n",
      "msa_formatted_.txt  train_lav_msa.lav   train_mag_msa.msa\r\n"
     ]
    }
   ],
   "source": [
    "ls data/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pps4KrlSIKQK"
   },
   "outputs": [],
   "source": [
    "#train, validation and test are probably wrong.  \n",
    "#https://github.com/pytorch/text/blob/master/torchtext/datasets/translation.py\n",
    "\n",
    "'''\n",
    "Arguments:\n",
    "            exts: A tuple containing the extension to path for each language.\n",
    "            fields: A tuple containing the fields that will be used for data\n",
    "                in each language.\n",
    "            path (str): Common prefix of the splits' file paths, or None to use\n",
    "                the result of cls.download(root).\n",
    "            root: Root dataset storage directory. Default is '.data'.\n",
    "            train: The prefix of the train data. Default: 'train'.\n",
    "            validation: The prefix of the validation data. Default: 'val'.\n",
    "            test: The prefix of the test data. Default: 'test'.\n",
    "            Remaining keyword arguments: Passed to the splits method of\n",
    "                Dataset.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "train_data, valid_data, test_data = eng_msa_train_dataset.splits(path= 'data/', train='train/train_eng_msa', validation='val/val_eng_msa', test='test/test_eng_msa', exts=('.eng', '.msa'),\n",
    "                                                    fields = (SRC, TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D86hmOgsIKQS"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE =32\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "15gKWRTZIKQW"
   },
   "source": [
    "## Model Training\n",
    "\n",
    "https://huggingface.co/transformers/model_doc/t5.html#training\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "u47qb7PG6WTM",
    "outputId": "6beede7b-5059-4777-9b3e-7a60e7eb6a22",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 32]\n",
      "\t[.src]:[torch.LongTensor of size 16x32]\n",
      "\t[.trg]:[torch.LongTensor of size 11x32]\n",
      "tensor([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "         2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 3, 0, 0],\n",
      "        [0, 3, 0, 0, 0, 3, 3, 0, 1, 0, 3, 3, 0, 3, 0, 3, 0, 0, 3, 3, 0, 0, 0, 0,\n",
      "         3, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [3, 1, 3, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 3, 0, 1, 1, 3, 0, 3, 3,\n",
      "         1, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [1, 1, 1, 0, 3, 1, 1, 3, 1, 3, 1, 1, 3, 1, 3, 1, 1, 0, 1, 1, 1, 3, 1, 1,\n",
      "         1, 0, 0, 0, 3, 1, 0, 0],\n",
      "        [1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1,\n",
      "         1, 3, 0, 0, 1, 1, 0, 3],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 1, 1, 3, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 3, 0, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 3, 1, 1, 1, 1]])\n",
      "tensor([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "         2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 3, 0, 0],\n",
      "        [0, 0, 3, 0, 0, 3, 3, 0, 1, 0, 1, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3,\n",
      "         3, 3, 0, 0, 0, 1, 0, 0],\n",
      "        [3, 0, 1, 0, 3, 1, 1, 0, 1, 3, 1, 1, 3, 3, 0, 1, 0, 3, 0, 3, 0, 0, 3, 1,\n",
      "         1, 1, 0, 0, 0, 1, 0, 0],\n",
      "        [1, 3, 1, 3, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 0, 1, 3, 1, 3, 1, 0, 3, 1, 1,\n",
      "         1, 1, 3, 0, 3, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 3, 1, 1, 1,\n",
      "         1, 1, 1, 0, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 1, 1, 0, 3],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 3, 1, 1, 0, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 3, 1]])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 32]\n",
      "\t[.src]:[torch.LongTensor of size 16x32]\n",
      "\t[.trg]:[torch.LongTensor of size 14x32]\n",
      "tensor([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "         2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 3, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0,\n",
      "         0, 0, 0, 3, 0, 0, 0, 0],\n",
      "        [0, 3, 0, 3, 1, 0, 0, 1, 3, 0, 3, 0, 3, 3, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0,\n",
      "         3, 0, 0, 1, 0, 3, 0, 0],\n",
      "        [0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 3, 0, 0, 0, 3, 1, 0, 1, 0,\n",
      "         1, 0, 0, 1, 0, 1, 0, 3],\n",
      "        [0, 1, 3, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 3, 0, 0, 1, 1, 0, 1, 0,\n",
      "         1, 0, 0, 1, 3, 1, 0, 1],\n",
      "        [0, 1, 1, 1, 1, 0, 0, 1, 1, 3, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 3, 1, 0,\n",
      "         1, 0, 0, 1, 1, 1, 0, 1],\n",
      "        [3, 1, 1, 1, 1, 3, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 3, 1, 1, 1, 1, 0,\n",
      "         1, 0, 0, 1, 1, 1, 3, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
      "         1, 0, 3, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
      "         1, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 3,\n",
      "         1, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 3, 1, 1, 1, 1, 1, 1,\n",
      "         1, 3, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "         2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 3, 0, 3, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         3, 0, 0, 3, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 1, 3, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0,\n",
      "         1, 0, 0, 1, 0, 0, 0, 0],\n",
      "        [0, 1, 3, 1, 1, 0, 0, 1, 3, 0, 3, 0, 0, 1, 0, 3, 0, 0, 0, 3, 3, 3, 1, 0,\n",
      "         1, 0, 0, 1, 0, 0, 0, 0],\n",
      "        [3, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 3, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,\n",
      "         1, 0, 0, 1, 0, 3, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 3, 0, 1, 1, 3, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,\n",
      "         1, 0, 0, 1, 3, 1, 3, 3],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 3, 0, 3, 1, 1, 1, 1, 0,\n",
      "         1, 0, 0, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 0, 1, 1, 3, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
      "         1, 0, 3, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 3,\n",
      "         1, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1,\n",
      "         1, 3, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 32]\n",
      "\t[.src]:[torch.LongTensor of size 14x32]\n",
      "\t[.trg]:[torch.LongTensor of size 11x32]\n",
      "tensor([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "         2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 3, 0, 0, 3, 3, 0, 0, 0, 1, 3, 1, 0, 3, 0, 0, 3, 0, 0, 0, 0, 0, 1, 3,\n",
      "         0, 3, 0, 0, 0, 0, 0, 3],\n",
      "        [3, 1, 0, 3, 1, 1, 3, 3, 0, 1, 1, 1, 3, 1, 3, 0, 1, 0, 0, 3, 3, 3, 1, 1,\n",
      "         0, 1, 0, 0, 3, 0, 0, 1],\n",
      "        [1, 1, 0, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 1, 3, 0, 1, 1, 1, 1, 1,\n",
      "         0, 1, 3, 3, 1, 0, 3, 1],\n",
      "        [1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "         0, 1, 1, 1, 1, 0, 1, 1],\n",
      "        [1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "         3, 1, 1, 1, 1, 3, 1, 1],\n",
      "        [1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "         2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 3,\n",
      "         0, 0, 0, 0, 0, 0, 0, 3],\n",
      "        [3, 0, 0, 0, 3, 0, 3, 3, 0, 1, 0, 3, 0, 3, 0, 3, 3, 0, 0, 1, 3, 0, 1, 1,\n",
      "         0, 0, 3, 0, 0, 0, 0, 1],\n",
      "        [1, 3, 0, 3, 1, 3, 1, 1, 3, 1, 0, 1, 0, 1, 3, 1, 1, 3, 0, 1, 1, 3, 1, 1,\n",
      "         0, 3, 1, 0, 3, 3, 0, 1],\n",
      "        [1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 3, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "         0, 1, 1, 3, 1, 1, 0, 1],\n",
      "        [1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "         3, 1, 1, 1, 1, 1, 3, 1],\n",
      "        [1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n",
      "[torchtext.data.batch.Batch of size 32]\n",
      "\t[.src]:[torch.LongTensor of size 12x32]\n",
      "\t[.trg]:[torch.LongTensor of size 11x32]\n",
      "tensor([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "         2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 3, 0, 0, 3, 0, 0, 0, 3, 0, 0, 3, 1, 3, 0, 0, 3, 3, 0, 0, 0, 3, 3, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 3, 1, 1, 3, 0, 0, 1, 1, 0,\n",
      "         3, 3, 0, 3, 0, 0, 0, 3],\n",
      "        [0, 1, 0, 3, 1, 3, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 3, 0, 1, 1, 0,\n",
      "         1, 1, 0, 1, 0, 3, 0, 1],\n",
      "        [0, 1, 0, 1, 1, 1, 0, 3, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 3, 1, 1, 3,\n",
      "         1, 1, 0, 1, 0, 1, 0, 1],\n",
      "        [3, 1, 3, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 3, 1, 0, 1, 3, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 1, 1, 3, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 3, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "         2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0,\n",
      "         0, 3, 0, 0, 0, 3, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 1, 1, 0, 0, 3, 0, 3, 0, 0, 3, 1, 0,\n",
      "         3, 1, 0, 0, 0, 1, 0, 3],\n",
      "        [0, 1, 3, 3, 3, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 3, 1, 0, 3, 1, 1, 3,\n",
      "         1, 1, 0, 0, 0, 1, 0, 1],\n",
      "        [3, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 3, 1, 1, 1, 3, 1, 1, 1, 1,\n",
      "         1, 1, 0, 3, 0, 1, 0, 1],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 3, 1, 3, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 3, 1, 0, 1, 0, 1],\n",
      "        [1, 1, 1, 1, 1, 3, 0, 1, 1, 1, 3, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 3, 1, 0, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 3, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for x in train_iterator:\n",
    "    print(x)\n",
    "    print(x.src)\n",
    "    print(x.trg)\n",
    "    #print(spacy_en_tokenizer.decode(x.src))\n",
    "    count += 1\n",
    "    \n",
    "    if count > 3:\n",
    "        break\n",
    "    \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kmd4r1ce4-wq"
   },
   "outputs": [],
   "source": [
    "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qBOsxexa4-wu"
   },
   "outputs": [],
   "source": [
    "def train(model: nn.Module,\n",
    "          iterator: BucketIterator,\n",
    "          optimizer: optim.Optimizer,\n",
    "          criterion: nn.Module,\n",
    "          clip: float):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for _, batch in enumerate(iterator):\n",
    "\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2OvKSZ214-wy"
   },
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module,\n",
    "             iterator: BucketIterator,\n",
    "             criterion: nn.Module):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for _, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing?\n",
    "\n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6oKXrTpU4-w1"
   },
   "outputs": [],
   "source": [
    "#declare model\n",
    "config = T5Config(vocab_size=VOCAB_SIZE)\n",
    "model = T5Model(config)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "9HwOuA3y4-w5",
    "outputId": "20590d68-bfdd-4f9c-b2b6-15abd5da19de"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-bcffbac6928e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-48bbe32f17dc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, inputs_embeds, decoder_inputs_embeds, head_mask)\u001b[0m\n\u001b[1;32m    833\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoder_outputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             encoder_outputs = self.encoder(\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m             )\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask)\u001b[0m\n\u001b[1;32m    666\u001b[0m                 \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_extended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m                 \u001b[0mencoder_decoder_position_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_decoder_position_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m                 \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m             )\n\u001b[1;32m    670\u001b[0m             \u001b[0;31m# layer_outputs is a tuple with:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, head_mask)\u001b[0m\n\u001b[1;32m    424\u001b[0m     ):\n\u001b[1;32m    425\u001b[0m         self_attention_outputs = self.layer[0](\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_bias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         )\n\u001b[1;32m    428\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, head_mask)\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0mnorm_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         attention_output = self.SelfAttention(\n\u001b[0;32m--> 377\u001b[0;31m             \u001b[0mnorm_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_bias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m         )\n\u001b[1;32m    379\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, mask, kv, position_bias, cache, head_mask)\u001b[0m\n\u001b[1;32m    344\u001b[0m                 \u001b[0mposition_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposition_bias\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmask\u001b[0m  \u001b[0;31m# (bs, n_heads, qlen, klen)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mposition_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, qlen, klen)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, qlen, klen)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (16) must match the size of tensor b (13) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "import time\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B0iYXG2c4-w9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d27X3PDM4-w_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RsKKhepA4-xC"
   },
   "outputs": [],
   "source": [
    "def trainIters(model, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    optimizer = AdamW.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX) #CHECK IF THE BEST LOSS FUNCTION\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        \n",
    "\n",
    "        loss = train(decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6s5rRYCg4-xI"
   },
   "outputs": [],
   "source": [
    "model = \n",
    "n_iteres = 10\n",
    "trainIters(model, n_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1A10gV1R4-xM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y6hpf6xY4-xR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mPFePDoUIKQ1"
   },
   "source": [
    "## Model Inference\n",
    "\n",
    "https://github.com/huggingface/transformers/blob/master/examples/translation/t5/evaluate_wmt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GXw9evdHyYhz"
   },
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i : i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4xt_iDQ4IKQa"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def generate_translations(lns, output_file_path, model_size, batch_size, device):\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_size)\n",
    "    model.to(device)\n",
    "    #returned to using T5 tokenizer\n",
    "    tokenizer = tokenizer\n",
    "\n",
    "    task_specific_params = model.config.task_specific_params\n",
    "\n",
    "    with Path(output_file_path).open(\"w\") as output_file:\n",
    "        for batch in tqdm(list(chunks(lns, batch_size))):\n",
    "        #modify to have prefix 'translate English to Arabic: ...'\n",
    "            batch = [ text for text in batch]\n",
    "            \n",
    "            dct = tokenizer.batch_encode_plus(batch, max_length=512, return_tensors=\"pt\", pad_to_max_length=True)\n",
    "\n",
    "            input_ids = dct[\"input_ids\"].to(device)\n",
    "            attention_mask = dct[\"attention_mask\"].to(device)\n",
    "\n",
    "            translations = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            dec = [\n",
    "                tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in translations\n",
    "            ]\n",
    "\n",
    "            for hypothesis in dec:\n",
    "                output_file.write(hypothesis + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8T4t7_HXy44y"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def calculate_bleu_score(output_lns, refs_lns, score_path):\n",
    "    bleu = corpus_bleu(output_lns, [refs_lns])\n",
    "    result = \"BLEU score: {}\".format(bleu.score)\n",
    "    with Path(score_path).open(\"w\") as score_file:\n",
    "        score_file.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hrpvKdBwNUYG"
   },
   "outputs": [],
   "source": [
    "!mkdir data/output/\n",
    "!touch data/output/translated.msa\n",
    "!touch data/output/score_eng_msa.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GslDGSi2zADM"
   },
   "outputs": [],
   "source": [
    "#got rid of the argparse and just put it in as params\n",
    "def run_generate(model_size='t5-base', input_path='data/train/train_eng_msa.eng', output_path='data/output/translated.msa', ref_path='data/train/train_eng_msa.eng', score_path='data/output/score_eng_msa.txt', batch_size=BATCH_SIZE):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "\n",
    "    # Read input lines into python\n",
    "    with open(input_path, \"r\") as input_file:\n",
    "        input_lns = [x.strip() for x in input_file.readlines()]\n",
    "\n",
    "    generate_translations(input_lns, output_path, model_size, batch_size, device)\n",
    "\n",
    "    # Read generated lines into python\n",
    "    with open(output_path, \"r\") as output_file:\n",
    "        output_lns = [x.strip() for x in output_file.readlines()]\n",
    "\n",
    "    # Read reference lines into python\n",
    "    with open(ref_path, \"r\") as reference_file:\n",
    "        refs_lns = [x.strip() for x in reference_file.readlines()]\n",
    "\n",
    "    calculate_bleu_score(output_lns, refs_lns, args.score_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_sKwIFVT0itp"
   },
   "outputs": [],
   "source": [
    "run_generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SpfOCjlt4-x7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "name": "Model_1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:fastai] *",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
