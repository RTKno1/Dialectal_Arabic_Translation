{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BQflUKhjIKNp"
   },
   "source": [
    "### Notes \n",
    "\n",
    "T5 Paper: https://arxiv.org/pdf/1910.10683.pdf\n",
    "\n",
    "T5 Tokenizer: https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_t5.py\n",
    "\n",
    "Important Tasks: https://docs.google.com/document/d/1weIZM6QTlnitpPQmpg-WeV2RW70TnYmDuogBQPr5mB0/edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "XLOmiOta6MJp",
    "outputId": "ff45d283-9268-43e2-8f22-6f20312d5c4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/James/anaconda/envs/fastai/lib/python3.7/site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in /Users/James/anaconda/envs/fastai/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/James/anaconda/envs/fastai/lib/python3.7/site-packages (from transformers) (2018.8.29)\n",
      "Requirement already satisfied: requests in /Users/James/anaconda/envs/fastai/lib/python3.7/site-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: boto3 in /Users/James/anaconda/envs/fastai/lib/python3.7/site-packages (from transformers) (1.12.39)\n",
      "Requirement already satisfied: numpy in /Users/James/anaconda/envs/fastai/lib/python3.7/site-packages (from transformers) (1.15.4)\n",
      "Requirement already satisfied: tokenizers==0.7.0rc3 in /Users/James/anaconda/envs/fastai/lib/python3.7/site-packages (from transformers) (0.7.0rc3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/James/anaconda/envs/fastai/lib/python3.7/site-packages (from transformers) (4.44.1)\n",
      "Requirement already satisfied: sacremoses in /Users/James/anaconda/envs/fastai/lib/python3.7/site-packages (from transformers) (0.0.38)\n",
      "Requirement already satisfied: sentencepiece in /Users/James/anaconda/envs/fastai/lib/python3.7/site-packages (from transformers) (0.1.85)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/James/anaconda/envs/fastai/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/James/anaconda/envs/fastai/lib/python3.7/site-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/James/anaconda/envs/fastai/lib/python3.7/site-packages (from requests->transformers) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/James/anaconda/envs/fastai/lib/python3.7/site-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Users/James/anaconda/envs/fastai/lib/python3.7/site-packages (from boto3->transformers) (0.9.5)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.39 in /Users/James/anaconda/envs/fastai/lib/python3.7/site-packages (from boto3->transformers) (1.15.39)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /Users/James/anaconda/envs/fastai/lib/python3.7/site-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: click in /Users/James/anaconda/envs/fastai/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.1)\n",
      "Requirement already satisfied: joblib in /Users/James/anaconda/envs/fastai/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: six in /Users/James/anaconda/envs/fastai/lib/python3.7/site-packages (from sacremoses->transformers) (1.14.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /Users/James/anaconda/envs/fastai/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/James/anaconda/envs/fastai/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers) (2.8.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/James/anaconda/envs/fastai/lib/python3.7/site-packages (0.1.85)\n",
      "Collecting utils\n",
      "  Downloading utils-1.0.1-py2.py3-none-any.whl (21 kB)\n",
      "Installing collected packages: utils\n",
      "Successfully installed utils-1.0.1\n",
      "mkdir: data/: File exists\n",
      "mkdir: data/AD_NMT-master: File exists\n",
      "mkdir: data/train/: File exists\n",
      "mkdir: data/test/: File exists\n",
      "mkdir: data/val/: File exists\n",
      "mkdir: data/vocab/: File exists\n",
      "mkdir: data/model/: File exists\n",
      "/bin/sh: wget: command not found\n",
      "/bin/sh: wget: command not found\n",
      "/bin/sh: wget: command not found\n",
      "/bin/sh: wget: command not found\n",
      "/bin/sh: wget: command not found\n",
      "/bin/sh: wget: command not found\n",
      "/bin/sh: wget: command not found\n",
      "/bin/sh: wget: command not found\n",
      "/bin/sh: wget: command not found\n"
     ]
    }
   ],
   "source": [
    "#installation step\n",
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install utils\n",
    "#creating the folders \n",
    "!mkdir data/\n",
    "!mkdir data/AD_NMT-master\n",
    "!mkdir data/train/\n",
    "!mkdir data/test/\n",
    "!mkdir data/val/\n",
    "!mkdir data/vocab/\n",
    "!mkdir data/model/\n",
    "#fetching the pkl files\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW' -O data/AD_NMT-master/english-Arabic-both.pkl\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1V8_tp8ZlWUYaX7QQL46t0uSRNrVehSf1' -O data/AD_NMT-master/english-Arabic-test.pkl\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1V7X0qtuDIyjTHY0wh-ZNoVwsiF4lId2e' -O data/AD_NMT-master/english-Arabic-train.pkl\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV' -O data/AD_NMT-master/LAV-MSA-2-both.pkl\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UpfCbkxhztof7dvNjeAs1bHjD4SER6h3' -O data/AD_NMT-master/LAV-MSA-2-test.pkl\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UlAZGtYsSfXzK7hrC_PbxQFqTSXD0DMw' -O data/AD_NMT-master/LAV-MSA-2-train.pkl\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UjDX7cCG2S23SPfSHxSPdVayMTxB5Y16' -O data/AD_NMT-master/Magribi_MSA-both.pkl\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UaVWIqRXo0rxuxDF4KArA4bEK1TaLX3l' -O data/AD_NMT-master/Magribi_MSA-test.pkl\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UYvlhdYAdfa4riP_4hn3-IEVd1ZUXVTQ' -O data/AD_NMT-master/Magribi_MSA-train.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-WzhRv4mIKNq"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-25-0079697290a7>, line 34)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-25-0079697290a7>\"\u001b[0;36m, line \u001b[0;32m34\u001b[0m\n\u001b[0;31m    In [2]: %autoreload 2\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#James Chartouni\n",
    "#Joey Park\n",
    "#Raef Khan\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os, io, glob, time\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtext.datasets.translation import TranslationDataset\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config, T5Model\n",
    "\n",
    "\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.vocab import Vocab\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import *\n",
    "from custom_field import *\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T67vZ0Nv4-uw"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "aha0xureIKNw",
    "outputId": "5d7fd491-112e-4d7e-a35d-0bef23512fdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAV-MSA-2-both.pkl        Magribi_MSA-train.pkl     english-arabic-test.pkl\r\n",
      "LAV-MSA-2-test.pkl        README.md                 lav_formatted_.txt\r\n",
      "LAV-MSA-2-train.pkl       en_formatted_.txt         mag_formatted_.txt\r\n",
      "Magribi_MSA-both.pkl      english-Arabic-both.pkl   msa_formatted_.txt\r\n",
      "Magribi_MSA-test.pkl      english-Arabic-train.pkl\r\n"
     ]
    }
   ],
   "source": [
    "ls data/AD_NMT-master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XWKnGRlLIKN9"
   },
   "outputs": [],
   "source": [
    "file_path = 'data/AD_NMT-master/'\n",
    "\n",
    "with open(file_path + \"english-Arabic-train.pkl\", 'rb') as handle:\n",
    "    data_English_MSA_trainval = pickle.load(handle)\n",
    "\n",
    "with open(file_path + \"english-Arabic-test.pkl\", 'rb') as handle:\n",
    "    data_English_MSA_test = pickle.load(handle)\n",
    "\n",
    "with open(file_path + \"english-Arabic-both.pkl\", 'rb') as handle:\n",
    "    data_English_MSA_both = pickle.load(handle) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(file_path + \"LAV-MSA-2-train.pkl\", 'rb') as handle:\n",
    "    data_LAV_MSA_trainval = pickle.load(handle) \n",
    "\n",
    "with open(file_path + \"LAV-MSA-2-test.pkl\", 'rb') as handle:\n",
    "    data_LAV_MSA_test = pickle.load(handle) \n",
    "\n",
    "with open(file_path + \"LAV-MSA-2-both.pkl\", 'rb') as handle:\n",
    "    data_LAV_MSA_both = pickle.load(handle) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(file_path + \"Magribi_MSA-train.pkl\", 'rb') as handle:\n",
    "    data_Magribi_MSA_trainval = pickle.load(handle) \n",
    "    \n",
    "with open(file_path + \"Magribi_MSA-test.pkl\", 'rb') as handle:\n",
    "    data_Magribi_MSA_test = pickle.load(handle) \n",
    "\n",
    "with open(file_path + \"Magribi_MSA-both.pkl\", 'rb') as handle:\n",
    "    data_Magribi_MSA_both = pickle.load(handle) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "Ch3APJadIKOH",
    "outputId": "560319c8-3dc1-4003-fc3b-d2324e794050"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Tom was also there', 'كان توم هنا ايضا'], ['That old woman lives by herself', 'تلك المراة العجوز تسكن بمفردها'], ['He went abroad for the purpose of studying English', 'سافر خارج البلد ليتعلم الانجليزية'], ['There is a fork missing', 'هناك شوكة ناقصة'], [\"I don't know this game\", 'لا اعرف هذه اللعبة']]\n",
      "[['لا انا بعرف وحدة راحت ع فرنسا و معا شنتا حطت فيها الفرش', 'لا اعرف واحدة ذهبت الى فرنسا و لها غرفة و ضعت فيها الافرشة'], ['روح بوشك و فتول عاليسار', 'اذهب تقدم و استدر يسارا'], ['لا لا لازم انه يكون عندك موضوع ما في اشي', ' لا لا يجب ان يكون لديك موضوع هذا ضروري'], ['اوعي تبعدي من هون بلاش تضيعي ', 'لا تبتعد عن هنا حتى لا تفقد الطريق '], ['قصدي صراحة يما انا كمان كرهته من يوم ما عملتيه زي ما بتعمله خالتي كرهته و صرت ما باطيقه بالمرة', 'اقصد صراحة يا امي انا ايضا كرهته من يوم حضرته مثلما تحضره خالتي كرهته و اصبحت لا اطيقه ابدا']]\n",
      "9000\n",
      "10001\n"
     ]
    }
   ],
   "source": [
    "print(data_English_MSA_both[0:5])\n",
    "print(data_LAV_MSA_both[0:5])\n",
    "print(len(data_English_MSA_trainval))\n",
    "print(len(data_English_MSA_both))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "irQQ9E_7IKOM"
   },
   "source": [
    "## Prepare Datasets\n",
    "\n",
    "example: https://iwslt2010.fbk.eu/node/32/\n",
    "\n",
    "We need to take our training and test sets from the pkl files and create new .txt files that are formatted so that the standard torchtext Dataset class can read them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "raw",
    "id": "ikpu9B5dIKON"
   },
   "source": [
    "Data format:\n",
    "each line consists of three fields divided by the character '\\'\n",
    "sentences consisting of words divided by single spaces\n",
    "format: <SENTENCE_ID>\\<PARAPHRASE_ID>\\<TEXT>\n",
    "Field_1: sentence ID\n",
    "Field_2: paraphrase ID\n",
    "Field_3: MT develop sentence / reference translation\n",
    "Text input example:\n",
    "DEV_001\\01\\This is the first develop sentence.\n",
    "DEV_002\\01\\This is the second develop sentence.\n",
    "Reference translation example:\n",
    "DEV_001\\01\\1st reference translation for 1st input\n",
    "DEV_001\\02\\2nd reference translation for 1st input\n",
    "...\n",
    "DEV_002\\01\\1st reference translation for 2nd input\n",
    "DEV_002\\02\\2nd reference translation for 2nd input\n",
    "...\n",
    "Languages:\n",
    "Arabic-English\n",
    "CSTAR03 testset: 506 sentences, 16 reference translations\n",
    "IWSLT04 testset: 500 sentences, 16 reference translations\n",
    "IWSLT05 testset: 506 sentences, 16 reference translations\n",
    "IWSLT07 testset: 489 sentences, 6 reference translations\n",
    "IWSLT08 testset: 507 sentences, 16 reference translations\n",
    "French-English\n",
    "CSTAR03 testset: 506 sentences, 16 reference translations\n",
    "IWSLT04 testset: 500 sentences, 16 reference translations\n",
    "IWSLT05 testset: 506 sentences, 16 reference translations\n",
    "Turkish-English\n",
    "CSTAR03 testset: 506 sentences, 16 reference translations\n",
    "IWSLT04 testset: 500 sentences, 16 reference translations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "7wcnDJDKwqED",
    "outputId": "319a799d-eb12-498a-d919-5354624dad11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAV-MSA-2-both.pkl        Magribi_MSA-train.pkl     english-arabic-test.pkl\r\n",
      "LAV-MSA-2-test.pkl        README.md                 lav_formatted_.txt\r\n",
      "LAV-MSA-2-train.pkl       en_formatted_.txt         mag_formatted_.txt\r\n",
      "Magribi_MSA-both.pkl      english-Arabic-both.pkl   msa_formatted_.txt\r\n",
      "Magribi_MSA-test.pkl      english-Arabic-train.pkl\r\n"
     ]
    }
   ],
   "source": [
    "ls data/AD_NMT-master/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lp7ncVY6wqET"
   },
   "outputs": [],
   "source": [
    "#splits the train dataset into train and validation sets, define test set as datafile\n",
    "eng_msa_train, eng_msa_val = train_test_split(data_English_MSA_trainval, test_size=.2, random_state=22)\n",
    "eng_msa_test = data_English_MSA_test\n",
    "\n",
    "lav_msa_train, lav_msa_val = train_test_split(data_LAV_MSA_trainval, test_size=.2, random_state=22)\n",
    "lav_msa_test = data_LAV_MSA_test\n",
    "\n",
    "mag_msa_train, mag_msa_val = train_test_split(data_Magribi_MSA_trainval, test_size=.2, random_state=22)\n",
    "mag_msa_test = data_Magribi_MSA_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "9Ue2fSILwqEW",
    "outputId": "04d6b2f9-f91f-4731-a589-d6d83d02b52a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200\n",
      "1800\n",
      "11044\n",
      "2761\n",
      "14188\n",
      "3548\n"
     ]
    }
   ],
   "source": [
    "print(len(eng_msa_train))\n",
    "print(len(eng_msa_val))\n",
    "\n",
    "print(len(lav_msa_train))\n",
    "print(len(lav_msa_val))\n",
    "\n",
    "print(len(mag_msa_train))\n",
    "print(len(mag_msa_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W3bWU46TIKOb"
   },
   "outputs": [],
   "source": [
    "file_path = 'data/'\n",
    "\n",
    "def pytorch_format(ds, src='en', trg='msa', datatype=''):\n",
    "    src_formatted = datatype + '_' + src + '_' + trg + '.' + src\n",
    "    trg_formatted = datatype + '_' + src + '_' + trg + '.' + trg\n",
    "    \n",
    "    with open(file_path + datatype + \"/\" + src_formatted, 'wt') as srctxt, open(file_path + datatype + \"/\" + trg_formatted, 'wt') as trgtxt:\n",
    "        for i, arr in enumerate(ds):\n",
    "            srctxt.write(datatype.upper() + '_' + str(i).zfill( len(str(len(ds))) - len(str(i))) + '\\\\01\\\\' + arr[0] + '\\n')\n",
    "            trgtxt.write(datatype.upper() + '_' + str(i).zfill( len(str(len(ds))) - len(str(i))) + '\\\\01\\\\' + arr[1] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AqwDB9BuKEnV"
   },
   "outputs": [],
   "source": [
    "#splits each language pair file into datasets of single language, to be merged again by the pytorch dataset class later\n",
    "\n",
    "pytorch_format(eng_msa_train, 'eng', 'msa', 'train')\n",
    "pytorch_format(eng_msa_val, 'eng', 'msa', 'val')\n",
    "pytorch_format(eng_msa_test, 'eng', 'msa', 'test')\n",
    "\n",
    "pytorch_format(lav_msa_train, 'lav', 'msa', 'train')\n",
    "pytorch_format(lav_msa_val, 'lav', 'msa', 'val')\n",
    "pytorch_format(lav_msa_test, 'lav', 'msa', 'test')\n",
    "\n",
    "pytorch_format(mag_msa_train, 'mag', 'msa', 'train')\n",
    "pytorch_format(mag_msa_val, 'mag', 'msa', 'val')\n",
    "pytorch_format(mag_msa_test, 'mag', 'msa', 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lDPvs9zxIKO3"
   },
   "source": [
    "## Build Vocabulary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ihevk2SWIKO5"
   },
   "source": [
    "Sentence Piece Google Colab\n",
    "https://github.com/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I25H3IsyIKPB"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Testing:\n",
    "Create a text file with all the vocab available from all sources for each language for SentencePiece to create a library \n",
    "\n",
    "TODO: implement bpemb model for MSA vocab/train SPM on larger datasets for dialects\n",
    "\"\"\"\n",
    "\n",
    "en_vocab = open(\"data/vocab/eng_vocab.txt\", \"wt\")\n",
    "msa_vocab = open(\"data/vocab/msa_vocab.txt\", \"wt\")\n",
    "lav_vocab = open(\"data/vocab/lav_vocab.txt\", \"wt\")\n",
    "mag_vocab = open(\"data/vocab/mag_vocab.txt\", \"wt\")\n",
    "\n",
    "MSA_text = \"\"\n",
    "EN_text = \"\"\n",
    "\n",
    "def create_vocab(file='', src='en_vocab', tgt='msa_vocab'):\n",
    "  for line in file:\n",
    "        src_sent = line[0]\n",
    "        src_words = src_sent.split(\" \")\n",
    "        for count, word in enumerate(src_words):\n",
    "            src.write(word)\n",
    "        src.write(\"\\n\")\n",
    "        \n",
    "        tgt_sent = line[1]\n",
    "        tgt_words = tgt_sent.split(\" \")\n",
    "        for count, word in enumerate(tgt_words):\n",
    "            tgt.write(word)\n",
    "        tgt.write(\"\\n\")\n",
    "\n",
    "create_vocab(data_English_MSA_both, en_vocab, msa_vocab)\n",
    "create_vocab(data_LAV_MSA_both, lav_vocab, msa_vocab)\n",
    "create_vocab(data_Magribi_MSA_both, mag_vocab, msa_vocab)\n",
    "\n",
    "en_vocab.close()\n",
    "msa_vocab.close()\n",
    "lav_vocab.close()\n",
    "mag_vocab.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Q0qG8N68IKPI",
    "outputId": "f850842c-8c28-445a-88c7-9e62fcece427"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spm.SentencePieceTrainer.train('--input=data/vocab/eng_vocab.txt,data/vocab/msa_vocab.txt,data/vocab/lav_vocab.txt,data/vocab/mag_vocab.txt --model_prefix=data/model/spm --vocab_size=' + str(VOCAB_SIZE))\n",
    "spm.SentencePieceTrainer.train('--input=data/vocab/eng_vocab.txt --model_prefix=data/model/eng --vocab_size=' + str(VOCAB_SIZE))\n",
    "spm.SentencePieceTrainer.train('--input=data/vocab/msa_vocab.txt --model_prefix=data/model/msa --vocab_size=' + str(VOCAB_SIZE))\n",
    "spm.SentencePieceTrainer.train('--input=data/vocab/lav_vocab.txt --model_prefix=data/model/lav --vocab_size=' + str(VOCAB_SIZE))\n",
    "spm.SentencePieceTrainer.train('--input=data/vocab/mag_vocab.txt --model_prefix=data/model/mag --vocab_size=' + str(VOCAB_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=data/vocab/eng_vocab.txt,data/vocab/msa_vocab.txt,data/vocab/lav_vocab.txt,data/vocab/mag_vocab.txt --model_prefix=data/model/spm --vocab_size=' + str(VOCAB_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "BOuH1-Tl4-vt",
    "outputId": "8a77b489-02fd-4835-e6e6-ab5b1ec11191"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng.model  lav.model  mag.model  msa.model  spm.model\r\n",
      "eng.vocab  lav.vocab  mag.vocab  msa.vocab  spm.vocab\r\n"
     ]
    }
   ],
   "source": [
    "ls data/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "y05LILRSIKPN",
    "outputId": "f152e018-c63e-4383-b82f-95d58f5c352e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('data/model/spm.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Egx5FaOsIKPW",
    "outputId": "4c001726-3189-49e9-ea35-70917a5a2a68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁This', '▁', 'is', '▁', 'a', '▁', 't', 'est']\n",
      "[1256, 3, 123, 3, 64, 3, 49, 2328]\n"
     ]
    }
   ],
   "source": [
    "print(sp.encode_as_pieces('This is a test'))\n",
    "print(sp.encode_as_ids('This is a test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁هناك', '▁شو', 'ك', 'ة', '▁', 'نا', 'قصة']\n",
      "[818, 931, 11, 12, 3, 23, 1167]\n"
     ]
    }
   ],
   "source": [
    "print(sp.encode_as_pieces('هناك شوكة ناقصة'))\n",
    "print(sp.encode_as_ids('هناك شوكة ناقصة'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁This', '▁', 'is', '▁', 'a', '▁', 't', 'est']\n",
      "[1256, 3, 123, 3, 64, 3, 49, 2328]\n"
     ]
    }
   ],
   "source": [
    "print(sp.encode_as_pieces('This is a test'))\n",
    "print(sp.encode_as_ids('This is a test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vPy6YE06wqFA"
   },
   "source": [
    "## Spacy Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_U6-4yB1wqFG"
   },
   "outputs": [],
   "source": [
    "eng_vocab_data = open(\"data/model/eng.vocab\", \"r\")\n",
    "msa_vocab_data = open(\"data/model/msa.vocab\", \"r\")\n",
    "lav_vocab_data = open(\"data/model/lav.vocab\", \"r\")\n",
    "mag_vocab_data = open(\"data/model/mag.vocab\", \"r\")\n",
    "spm_vocab_data = open(\"data/model/spm.vocab\", \"r\")\n",
    "\n",
    "eng_vocab_list = []\n",
    "msa_vocab_list = []\n",
    "lav_vocab_list = []\n",
    "mag_vocab_list = []\n",
    "spm_vocab_list = []\n",
    "\n",
    "for line in eng_vocab_data.readlines():\n",
    "    eng_vocab_list.append(line.split(\"\\t\")[0])\n",
    "\n",
    "for line in msa_vocab_data.readlines():\n",
    "    msa_vocab_list.append(line.split(\"\\t\")[0])\n",
    "\n",
    "for line in lav_vocab_data.readlines():\n",
    "    lav_vocab_list.append(line.split(\"\\t\")[0])\n",
    "\n",
    "for line in mag_vocab_data.readlines():\n",
    "    mag_vocab_list.append(line.split(\"\\t\")[0])\n",
    "    \n",
    "for line in spm_vocab_data.readlines():\n",
    "    spm_vocab_list.append(line.split(\"\\t\")[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9IkkH0xQwqFK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " '<s>',\n",
       " '</s>',\n",
       " '▁',\n",
       " 'ا',\n",
       " 'و',\n",
       " 'ي',\n",
       " 'ت',\n",
       " 'ال',\n",
       " 'ب',\n",
       " 'م',\n",
       " 'ك',\n",
       " 'ة',\n",
       " 'ه',\n",
       " 'ن',\n",
       " 'ل',\n",
       " 'في',\n",
       " 'ها',\n",
       " 'من',\n",
       " 'ما',\n",
       " 'ش',\n",
       " 'ان',\n",
       " 'ف',\n",
       " 'نا',\n",
       " '؟',\n",
       " 'ين',\n",
       " 'وا',\n",
       " 'ون',\n",
       " 'لي',\n",
       " 'لا',\n",
       " 'س',\n",
       " 'ع',\n",
       " 'على',\n",
       " \"'\",\n",
       " 'ني',\n",
       " 's',\n",
       " 'انا',\n",
       " 'هم',\n",
       " '▁و',\n",
       " 'ى',\n",
       " 'هو',\n",
       " 'ح',\n",
       " 'د',\n",
       " 'انت',\n",
       " 'ق',\n",
       " 'يا',\n",
       " 'ات',\n",
       " '▁لا',\n",
       " 'فيال',\n",
       " 't',\n",
       " 'هذا',\n",
       " '▁انا',\n",
       " 'الم',\n",
       " '?',\n",
       " '▁نعم',\n",
       " 'مع',\n",
       " 'ر',\n",
       " 'ز',\n",
       " 'عن',\n",
       " 'بال',\n",
       " 'سي',\n",
       " '▁I',\n",
       " 'هي',\n",
       " '▁ا',\n",
       " 'a',\n",
       " 'كان',\n",
       " 'ج',\n",
       " 'خ',\n",
       " 'ص',\n",
       " 'انه',\n",
       " 'كل',\n",
       " 'منال',\n",
       " 'تي',\n",
       " 'غ',\n",
       " 'لل',\n",
       " 'لك',\n",
       " 'ط',\n",
       " 'ية',\n",
       " 'حتى',\n",
       " 'to',\n",
       " 'سا',\n",
       " 'بس',\n",
       " 'او',\n",
       " 'ذلك',\n",
       " 'بي',\n",
       " 'ض',\n",
       " 'اني',\n",
       " 'm',\n",
       " 'شي',\n",
       " 'اليوم',\n",
       " 'اذا',\n",
       " 'ار',\n",
       " 'الى',\n",
       " 'واحد',\n",
       " 'ست',\n",
       " 'لو',\n",
       " 'هناك',\n",
       " 'بت',\n",
       " 'اء',\n",
       " 'كنت',\n",
       " 'كم',\n",
       " 'وال',\n",
       " 'غير',\n",
       " 'ور',\n",
       " 'ولا',\n",
       " 'اللي',\n",
       " 'مت',\n",
       " '▁اه',\n",
       " 'لما',\n",
       " 'the',\n",
       " 'you',\n",
       " 'بعد',\n",
       " 'كي',\n",
       " 'الله',\n",
       " 'هنا',\n",
       " 'هل',\n",
       " '▁هل',\n",
       " 'عندما',\n",
       " 'يت',\n",
       " 'اد',\n",
       " 'الان',\n",
       " '▁ايه',\n",
       " 'd',\n",
       " 'is',\n",
       " 'جدا',\n",
       " 'ing',\n",
       " 'كانت',\n",
       " 'مش',\n",
       " 'لكن',\n",
       " 'كيف',\n",
       " 'ام',\n",
       " 'علىال',\n",
       " 'r',\n",
       " 'فى',\n",
       " 'ذ',\n",
       " 'الا',\n",
       " 'تو',\n",
       " 'سليمة',\n",
       " 'انها',\n",
       " 'me',\n",
       " 'كتير',\n",
       " 'الي',\n",
       " 'لات',\n",
       " 'ماذا',\n",
       " 'ed',\n",
       " 'فيها',\n",
       " 'مو',\n",
       " 'تها',\n",
       " 'n',\n",
       " '▁هذا',\n",
       " 'قال',\n",
       " 'اي',\n",
       " 'it',\n",
       " 'ير',\n",
       " 'in',\n",
       " 'ئ',\n",
       " '▁لكن',\n",
       " 'ته',\n",
       " 'شيء',\n",
       " 'e',\n",
       " 'توم',\n",
       " 'وش',\n",
       " 'له',\n",
       " 'فقط',\n",
       " 'فيه',\n",
       " 'ائ',\n",
       " 'ولكن',\n",
       " 'جد',\n",
       " 'يكون',\n",
       " 'اب',\n",
       " 'عمل',\n",
       " 'بر',\n",
       " 'تك',\n",
       " 're',\n",
       " 'قبل',\n",
       " 'لها',\n",
       " 'هيك',\n",
       " 'c',\n",
       " 'تر',\n",
       " '▁ما',\n",
       " 'وقت',\n",
       " '▁اي',\n",
       " 'اس',\n",
       " 'عند',\n",
       " 'دي',\n",
       " 'هذه',\n",
       " 'اخر',\n",
       " 'الذي',\n",
       " 'اش',\n",
       " '▁قالتلي',\n",
       " 'وم',\n",
       " 'انك',\n",
       " 'دو',\n",
       " '▁انت',\n",
       " 'درس',\n",
       " 'ول',\n",
       " 'ايضا',\n",
       " 'f',\n",
       " 'ث',\n",
       " 'مات']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm_vocab_list[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MpMYD4zGwqFP"
   },
   "outputs": [],
   "source": [
    "en_vocab = Vocab(strings=eng_vocab_list)\n",
    "spacy_en_tokenizer = Tokenizer(en_vocab)\n",
    "\n",
    "msa_vocab = Vocab(strings=msa_vocab_list)\n",
    "spacy_msa_tokenizer = Tokenizer(msa_vocab)\n",
    "\n",
    "lav_vocab = Vocab(strings=lav_vocab_list)\n",
    "spacy_lav_tokenizer = Tokenizer(lav_vocab)\n",
    "\n",
    "mag_vocab = Vocab(strings=mag_vocab_list)\n",
    "spacy_mag_tokenizer = Tokenizer(mag_vocab)\n",
    "\n",
    "spm_vocab = Vocab(strings=spm_vocab_list)\n",
    "spacy_spm_tokenizer = Tokenizer(spm_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0uDxsOI4IKPd"
   },
   "source": [
    "## TF Tokenizer\n",
    "\n",
    "https://huggingface.co/transformers/model_doc/t5.html#t5tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zDuf6tVBIKPe"
   },
   "outputs": [],
   "source": [
    "ls data/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LH26oj-KIKPl"
   },
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer('data/model/spm.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "txeYu0e0IKPs"
   },
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode('تعلمت كيفية ركوب الدراجة عندما كان عمري ست سنوات ', return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D7IuJeL0IKPx"
   },
   "outputs": [],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fi6rWTe0wqFc"
   },
   "outputs": [],
   "source": [
    "#decode to make sure you can go back and forth between the encoding properly \n",
    "#@raef can you verify the Arabic?\n",
    "output = tokenizer.decode(input_ids[0])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z2TpLlDln28C"
   },
   "outputs": [],
   "source": [
    "#from transformers import T5WithLMHeadModel\n",
    "#model = T5WithLMHeadModel.from_pretrained('t5-small')\n",
    "input_ids = tokenizer.encode('translate English to Arabic: She hates green peppers', return_tensor='pt')\n",
    "lm_labels = tokenizer.encode('انها لا تحب الفلفل الاخضر', return_tensor='pt')\n",
    "\n",
    "print(input_ids)\n",
    "print(lm_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UuAey7jyIKP1"
   },
   "source": [
    "## Pytorch Data Set and Data Loader\n",
    "\n",
    "https://github.com/google-research/text-to-text-transfer-transformer\n",
    "\n",
    "pytorch dataset: https://pytorch.org/text/_modules/torchtext/datasets/translation.html\n",
    "\n",
    "pytorch dataset documentation: https://torchtext.readthedocs.io/en/latest/datasets.html#iwslt\n",
    "\n",
    "example dataset: https://iwslt2010.fbk.eu/node/32/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "god7BjRnwqFh"
   },
   "source": [
    "Field API: https://pytorch.org/text/data.html#torchtext.data.Field\n",
    "Field Source: https://pytorch.org/text/_modules/torchtext/data/field.html#Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TmvgEGoLwqFi"
   },
   "source": [
    "https://pytorch.org/text/_modules/torchtext/data/utils.html#get_tokenizer\n",
    "    \n",
    "we need to pass our tokenizer as a function     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fVIqommkIKQF"
   },
   "outputs": [],
   "source": [
    "SRC = Field(tokenize = spacy_spm_tokenizer,\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = False)\n",
    "\n",
    "TRG = Field(tokenize = spacy_spm_tokenizer,\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = False)\n",
    "\n",
    "\"\"\"SRC = Field(tokenize =  T5Tokenizer,\n",
    "             init_token = '<sos>',\n",
    "             eos_token = '<eos>',\n",
    "             lower = False)\n",
    "\n",
    "TRG = Field(tokenize =  T5Tokenizer,\n",
    "             init_token = '<sos>',\n",
    "             eos_token = '<eos>',\n",
    "             lower = False)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-45f379fb43bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m             \u001b[0minit_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'<sos>'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0meos_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'<eos>'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m             lower = False)\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m TRG = Custom_Field(tokenize = \"spacy\",\n",
      "\u001b[0;32m~/Code/GitHub/arabic_translation/custom_field.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sequential, use_vocab, init_token, eos_token, fix_length, dtype, preprocessing, postprocessing, lower, tokenize, tokenizer_language, include_lengths, batch_first, pad_token, unk_token, pad_first, truncate_first, stop_words, is_target)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# in case the tokenizer isn't picklable (e.g. spacy)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# removed tokenizer_language\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_get_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#*** This is what I changed ****\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclude_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minclude_lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/GitHub/arabic_translation/custom_field.py\u001b[0m in \u001b[0;36mcustom_get_tokenizer\u001b[0;34m(tokenizer)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;31m#spm_vocab = Vocab(strings=spm_vocab_list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0;31m#spacy_spm_tokenizer = Tokenizer(spm_vocab)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mspm_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspm_vocab_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "SRC = Custom_Field(tokenize = \"spacy\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = False)\n",
    "\n",
    "TRG = Custom_Field(tokenize = \"spacy\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UiS3YbFGwqFl"
   },
   "outputs": [],
   "source": [
    "eng_msa_train_dataset = TranslationDataset(path='data/train/train_eng_msa.', exts=('eng', 'msa'), fields=(SRC, TRG))\n",
    "lav_msa_train_dataset = TranslationDataset(path='data/train/train_lav_msa.', exts=('lav', 'msa'), fields=(SRC, TRG))\n",
    "mag_msa_train_dataset = TranslationDataset(path='data/train/train_mag_msa.', exts=('mag', 'msa'), fields=(SRC, TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(eng_msa_train_dataset, lav_msa_train_dataset, mag_msa_train_dataset)\n",
    "TRG.build_vocab(eng_msa_train_dataset, lav_msa_train_dataset, mag_msa_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dEnHMRQxwqFo"
   },
   "outputs": [],
   "source": [
    "\"\"\"SRC.build_vocab(eng_msa_train_dataset)\n",
    "TRG.build_vocab(eng_msa_train_dataset)\n",
    "SRC.build_vocab(lav_msa_train_dataset)\n",
    "TRG.build_vocab(lav_msa_train_dataset)\n",
    "SRC.build_vocab(mag_msa_train_dataset)\n",
    "TRG.build_vocab(mag_msa_train_dataset)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "08ZvS7DYwqFq"
   },
   "outputs": [],
   "source": [
    "ex = eng_msa_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "SAUEnQd6wqFu",
    "outputId": "44c82d4b-e548-477a-9514-e4b42dfbe186"
   },
   "outputs": [],
   "source": [
    "print(ex.src) \n",
    "print(ex.trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "2nsz1PhXwqF2",
    "outputId": "d2f61c41-2c0a-463c-a6e0-f569aeaf5353"
   },
   "outputs": [],
   "source": [
    "ls data/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pps4KrlSIKQK"
   },
   "outputs": [],
   "source": [
    "#train, validation and test are probably wrong.  \n",
    "#https://github.com/pytorch/text/blob/master/torchtext/datasets/translation.py\n",
    "\n",
    "'''\n",
    "Arguments:\n",
    "            exts: A tuple containing the extension to path for each language.\n",
    "            fields: A tuple containing the fields that will be used for data\n",
    "                in each language.\n",
    "            path (str): Common prefix of the splits' file paths, or None to use\n",
    "                the result of cls.download(root).\n",
    "            root: Root dataset storage directory. Default is '.data'.\n",
    "            train: The prefix of the train data. Default: 'train'.\n",
    "            validation: The prefix of the validation data. Default: 'val'.\n",
    "            test: The prefix of the test data. Default: 'test'.\n",
    "            Remaining keyword arguments: Passed to the splits method of\n",
    "                Dataset.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "train_data, valid_data, test_data = eng_msa_train_dataset.splits(path= 'data/', train='train/train_eng_msa', validation='val/val_eng_msa', test='test/test_eng_msa', exts=('.eng', '.msa'),\n",
    "                                                    fields = (SRC, TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D86hmOgsIKQS"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE =32\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "15gKWRTZIKQW"
   },
   "source": [
    "## Model Training\n",
    "\n",
    "https://huggingface.co/transformers/model_doc/t5.html#training\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "u47qb7PG6WTM",
    "outputId": "6beede7b-5059-4777-9b3e-7a60e7eb6a22",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "for x in train_iterator:\n",
    "    print(x.src)\n",
    "    print(x.trg)\n",
    "    #print(spacy_en_tokenizer.decode(x.src))\n",
    "    count += 1\n",
    "    \n",
    "    if count > 3:\n",
    "        break\n",
    "    \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kmd4r1ce4-wq"
   },
   "outputs": [],
   "source": [
    "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qBOsxexa4-wu"
   },
   "outputs": [],
   "source": [
    "def train(model: nn.Module,\n",
    "          iterator: BucketIterator,\n",
    "          optimizer: optim.Optimizer,\n",
    "          criterion: nn.Module,\n",
    "          clip: float):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for _, batch in enumerate(iterator):\n",
    "\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2OvKSZ214-wy"
   },
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module,\n",
    "             iterator: BucketIterator,\n",
    "             criterion: nn.Module):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for _, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing?\n",
    "\n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6oKXrTpU4-w1"
   },
   "outputs": [],
   "source": [
    "#declare model\n",
    "config = T5Config(vocab_size=VOCAB_SIZE)\n",
    "model = T5Model(config)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "9HwOuA3y4-w5",
    "outputId": "20590d68-bfdd-4f9c-b2b6-15abd5da19de"
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B0iYXG2c4-w9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d27X3PDM4-w_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RsKKhepA4-xC"
   },
   "outputs": [],
   "source": [
    "def trainIters(model, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    optimizer = AdamW.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX) #CHECK IF THE BEST LOSS FUNCTION\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        \n",
    "\n",
    "        loss = train(decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6s5rRYCg4-xI"
   },
   "outputs": [],
   "source": [
    "model = \n",
    "n_iteres = 10\n",
    "trainIters(model, n_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1A10gV1R4-xM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y6hpf6xY4-xR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mPFePDoUIKQ1"
   },
   "source": [
    "## Model Inference\n",
    "\n",
    "https://github.com/huggingface/transformers/blob/master/examples/translation/t5/evaluate_wmt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GXw9evdHyYhz"
   },
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i : i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4xt_iDQ4IKQa"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def generate_translations(lns, output_file_path, model_size, batch_size, device):\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_size)\n",
    "    model.to(device)\n",
    "    #returned to using T5 tokenizer\n",
    "    tokenizer = tokenizer\n",
    "\n",
    "    task_specific_params = model.config.task_specific_params\n",
    "\n",
    "    with Path(output_file_path).open(\"w\") as output_file:\n",
    "        for batch in tqdm(list(chunks(lns, batch_size))):\n",
    "        #modify to have prefix 'translate English to Arabic: ...'\n",
    "            batch = [ text for text in batch]\n",
    "            \n",
    "            dct = tokenizer.batch_encode_plus(batch, max_length=512, return_tensors=\"pt\", pad_to_max_length=True)\n",
    "\n",
    "            input_ids = dct[\"input_ids\"].to(device)\n",
    "            attention_mask = dct[\"attention_mask\"].to(device)\n",
    "\n",
    "            translations = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            dec = [\n",
    "                tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in translations\n",
    "            ]\n",
    "\n",
    "            for hypothesis in dec:\n",
    "                output_file.write(hypothesis + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8T4t7_HXy44y"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def calculate_bleu_score(output_lns, refs_lns, score_path):\n",
    "    bleu = corpus_bleu(output_lns, [refs_lns])\n",
    "    result = \"BLEU score: {}\".format(bleu.score)\n",
    "    with Path(score_path).open(\"w\") as score_file:\n",
    "        score_file.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hrpvKdBwNUYG"
   },
   "outputs": [],
   "source": [
    "!mkdir data/output/\n",
    "!touch data/output/translated.msa\n",
    "!touch data/output/score_eng_msa.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GslDGSi2zADM"
   },
   "outputs": [],
   "source": [
    "#got rid of the argparse and just put it in as params\n",
    "def run_generate(model_size='t5-base', input_path='data/train/train_eng_msa.eng', output_path='data/output/translated.msa', ref_path='data/train/train_eng_msa.eng', score_path='data/output/score_eng_msa.txt', batch_size=BATCH_SIZE):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "\n",
    "    # Read input lines into python\n",
    "    with open(input_path, \"r\") as input_file:\n",
    "        input_lns = [x.strip() for x in input_file.readlines()]\n",
    "\n",
    "    generate_translations(input_lns, output_path, model_size, batch_size, device)\n",
    "\n",
    "    # Read generated lines into python\n",
    "    with open(output_path, \"r\") as output_file:\n",
    "        output_lns = [x.strip() for x in output_file.readlines()]\n",
    "\n",
    "    # Read reference lines into python\n",
    "    with open(ref_path, \"r\") as reference_file:\n",
    "        refs_lns = [x.strip() for x in reference_file.readlines()]\n",
    "\n",
    "    calculate_bleu_score(output_lns, refs_lns, args.score_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_sKwIFVT0itp"
   },
   "outputs": [],
   "source": [
    "run_generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SpfOCjlt4-x7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "name": "Model_1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:fastai] *",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
