{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes \n",
    "\n",
    "T5 Paper: https://arxiv.org/pdf/1910.10683.pdf\n",
    "\n",
    "T5 Tokenizer: https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_t5.py\n",
    "\n",
    "Important Tasks: https://docs.google.com/document/d/1weIZM6QTlnitpPQmpg-WeV2RW70TnYmDuogBQPr5mB0/edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#James Chartouni\n",
    "#Joey Park\n",
    "#Raef Khan\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtext.datasets.translation import TranslationDataset\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAV-MSA-2-both.pkl        Magribi_MSA-test.pkl      english-Arabic-train.pkl\r\n",
      "LAV-MSA-2-test.pkl        Magribi_MSA-train.pkl     english-arabic-test.pkl\r\n",
      "LAV-MSA-2-train.pkl       README.md\r\n",
      "Magribi_MSA-both.pkl      english-Arabic-both.pkl\r\n"
     ]
    }
   ],
   "source": [
    "ls data/AD_NMT-master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/AD_NMT-master/'\n",
    "\n",
    "with open(file_path + \"english-Arabic-train.pkl\", 'rb') as handle:\n",
    "    train_ds = pickle.load(handle) \n",
    "    \n",
    "with open(file_path + \"english-Arabic-test.pkl\", 'rb') as handle:\n",
    "    test_ds = pickle.load(handle) \n",
    "    \n",
    "with open(file_path + \"LAV-MSA-2-both.pkl\", 'rb') as handle:\n",
    "    data_LAV_MSA = pickle.load(handle) \n",
    "\n",
    "with open(file_path + \"english-Arabic-both.pkl\", 'rb') as handle:\n",
    "    data_English_MSA = pickle.load(handle) \n",
    "    \n",
    "with open(file_path + \"Magribi_MSA-both.pkl\", 'rb') as handle:\n",
    "    data_Magribi_MSA = pickle.load(handle) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Tom was also there', 'كان توم هنا ايضا'], ['That old woman lives by herself', 'تلك المراة العجوز تسكن بمفردها'], ['He went abroad for the purpose of studying English', 'سافر خارج البلد ليتعلم الانجليزية'], ['There is a fork missing', 'هناك شوكة ناقصة'], [\"I don't know this game\", 'لا اعرف هذه اللعبة']]\n",
      "[[\"Where's your money?\", 'اين مالك؟'], ['Be prepared', 'كن مستعدا'], [\"I figured you'd be impressed\", 'توقعت انك ستنبهر'], ['May I come in?', 'هل بامكاني الدخول؟'], ['Read through the article', 'اقرا المقالة']]\n"
     ]
    }
   ],
   "source": [
    "print(train_ds[0:5])\n",
    "print(test_ds[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Datasets\n",
    "\n",
    "example: https://iwslt2010.fbk.eu/node/32/\n",
    "\n",
    "We need to take our training and test sets from the pkl files and create new .txt files that are formatted so that the standard torchtext Dataset class can read them"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Data format:\n",
    "each line consists of three fields divided by the character '\\'\n",
    "sentences consisting of words divided by single spaces\n",
    "format: <SENTENCE_ID>\\<PARAPHRASE_ID>\\<TEXT>\n",
    "Field_1: sentence ID\n",
    "Field_2: paraphrase ID\n",
    "Field_3: MT develop sentence / reference translation\n",
    "Text input example:\n",
    "DEV_001\\01\\This is the first develop sentence.\n",
    "DEV_002\\01\\This is the second develop sentence.\n",
    "Reference translation example:\n",
    "DEV_001\\01\\1st reference translation for 1st input\n",
    "DEV_001\\02\\2nd reference translation for 1st input\n",
    "...\n",
    "DEV_002\\01\\1st reference translation for 2nd input\n",
    "DEV_002\\02\\2nd reference translation for 2nd input\n",
    "...\n",
    "Languages:\n",
    "Arabic-English\n",
    "CSTAR03 testset: 506 sentences, 16 reference translations\n",
    "IWSLT04 testset: 500 sentences, 16 reference translations\n",
    "IWSLT05 testset: 506 sentences, 16 reference translations\n",
    "IWSLT07 testset: 489 sentences, 6 reference translations\n",
    "IWSLT08 testset: 507 sentences, 16 reference translations\n",
    "French-English\n",
    "CSTAR03 testset: 506 sentences, 16 reference translations\n",
    "IWSLT04 testset: 500 sentences, 16 reference translations\n",
    "IWSLT05 testset: 506 sentences, 16 reference translations\n",
    "Turkish-English\n",
    "CSTAR03 testset: 506 sentences, 16 reference translations\n",
    "IWSLT04 testset: 500 sentences, 16 reference translations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/AD_NMT-master/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocabulary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence Piece Google Colab\n",
    "https://github.com/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10001"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_English_MSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create a text file with all the MSA vocab available for SentencePiece to create a library \n",
    "\"\"\"\n",
    "\n",
    "text_file_en = open(\"data/english_data.txt\", \"wt\")\n",
    "text_file_msa = open(\"data/arabic_data.txt\", \"wt\")\n",
    "\n",
    "MSA_text = \"\"\n",
    "EN_text = \"\"\n",
    "\n",
    "for line in data_English_MSA:\n",
    "        english = line[0]\n",
    "        english_words = english.split(\" \")\n",
    "        for count, word in enumerate(english_words):\n",
    "            text_file_en.write(word)\n",
    "        text_file_en.write(\"\\n\")\n",
    "        \n",
    "        arabic = line[1]\n",
    "        arabic_words = arabic.split(\" \")\n",
    "        for count, word in enumerate(arabic_words):\n",
    "            text_file_msa.write(word)\n",
    "        text_file_msa.write(\"\\n\")\n",
    "\n",
    "\n",
    "text_file_en.close()\n",
    "text_file_msa.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=data/arabic_data.txt --model_prefix=data/msa --vocab_size=2000')\n",
    "spm.SentencePieceTrainer.train('--input=data/english_data.txt --model_prefix=data/en --vocab_size=2000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('data/en.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env_Setup_Instructions.txt  \u001b[34mdata\u001b[m\u001b[m/\r\n",
      "Model_1.ipynb               \u001b[34mtransformers\u001b[m\u001b[m/\r\n",
      "README.md\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁This', '▁', 'is', '▁', 'a', '▁', 't', 'est']\n",
      "[89, 83, 12, 83, 8, 83, 6, 309]\n"
     ]
    }
   ],
   "source": [
    "print(sp.encode_as_pieces('This is a test'))\n",
    "print(sp.encode_as_ids('This is a test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF Tokenizer\n",
    "\n",
    "https://huggingface.co/transformers/model_doc/t5.html#t5tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mAD_NMT-master\u001b[m\u001b[m/\r\n",
      "AD_NMT-master.zip\r\n",
      "PADIC.xml\r\n",
      "UNv1.0.ar-en.tar.gz.00\r\n",
      "UNv1.0.ar-en.tar.gz.01\r\n",
      "arabic-to-english-translation-sentences.zip\r\n",
      "arabic_data.txt\r\n",
      "en.model\r\n",
      "en.vocab\r\n",
      "english_data.txt\r\n",
      "msa.model\r\n",
      "msa.vocab\r\n"
     ]
    }
   ],
   "source": [
    "ls data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_tokenizer = T5Tokenizer(\"data/msa.model\")\n",
    "en_tokenizer = T5Tokenizer(\"data/en.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = msa_tokenizer.encode('هل بامكاني الدخول؟ </s>', return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 41,   4, 330,   6,  93, 964,   8,   4,   2]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Data Set and Data Loader\n",
    "\n",
    "https://github.com/google-research/text-to-text-transfer-transformer\n",
    "\n",
    "pytorch dataset: https://pytorch.org/text/_modules/torchtext/datasets/translation.html\n",
    "\n",
    "pytorch dataset documentation: https://torchtext.readthedocs.io/en/latest/datasets.html#iwslt\n",
    "\n",
    "example dataset: https://iwslt2010.fbk.eu/node/32/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-8311c072c41a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTranslationDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data/arabic_data.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data/english_data.txt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/envs/fastai/lib/python3.7/site-packages/torchtext/datasets/translation.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, exts, fields, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \"\"\"\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'src'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'trg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "dataset = TranslationDataset(path=['data/arabic_data.txt', 'data/english_data.txt'], exts=(), fields=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "https://huggingface.co/transformers/model_doc/t5.html#training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = T5Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5Model.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inference\n",
    "\n",
    "https://github.com/huggingface/transformers/blob/master/examples/translation/t5/evaluate_wmt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai] *",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
