{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XLM_Model.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jameschartouni/arabic_translation/blob/xlm/XLM_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQGE8JtI2EAt",
        "colab_type": "code",
        "outputId": "f6f3ec05-cf0a-4766-e2b3-4061bd399515",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#installation step\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install bpemb\n",
        "#creating the folders \n",
        "!mkdir data/\n",
        "!mkdir data/AD_NMT-master\n",
        "!mkdir data/train/\n",
        "!mkdir data/test/\n",
        "!mkdir data/val/\n",
        "!mkdir data/vocab\n",
        "!mkdir data/model\n",
        "\n",
        "#fetching the pkl files\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW' -O data/AD_NMT-master/english-Arabic-both.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1V8_tp8ZlWUYaX7QQL46t0uSRNrVehSf1' -O data/AD_NMT-master/english-Arabic-test.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1V7X0qtuDIyjTHY0wh-ZNoVwsiF4lId2e' -O data/AD_NMT-master/english-Arabic-train.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV' -O data/AD_NMT-master/LAV-MSA-2-both.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UpfCbkxhztof7dvNjeAs1bHjD4SER6h3' -O data/AD_NMT-master/LAV-MSA-2-test.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UlAZGtYsSfXzK7hrC_PbxQFqTSXD0DMw' -O data/AD_NMT-master/LAV-MSA-2-train.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UjDX7cCG2S23SPfSHxSPdVayMTxB5Y16' -O data/AD_NMT-master/Magribi_MSA-both.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UaVWIqRXo0rxuxDF4KArA4bEK1TaLX3l' -O data/AD_NMT-master/Magribi_MSA-test.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UYvlhdYAdfa4riP_4hn3-IEVd1ZUXVTQ' -O data/AD_NMT-master/Magribi_MSA-train.pkl"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.91)\n",
            "Requirement already satisfied: bpemb in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from bpemb) (3.6.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from bpemb) (0.1.91)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from bpemb) (1.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from bpemb) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from bpemb) (2.23.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim->bpemb) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim->bpemb) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->bpemb) (2.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (2.9)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->bpemb) (2.49.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->bpemb) (1.13.13)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->bpemb) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->bpemb) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.13 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->bpemb) (1.16.13)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->smart-open>=1.2.1->gensim->bpemb) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->smart-open>=1.2.1->gensim->bpemb) (2.8.1)\n",
            "mkdir: cannot create directory ‘data/’: File exists\n",
            "mkdir: cannot create directory ‘data/AD_NMT-master’: File exists\n",
            "mkdir: cannot create directory ‘data/train/’: File exists\n",
            "mkdir: cannot create directory ‘data/test/’: File exists\n",
            "mkdir: cannot create directory ‘data/val/’: File exists\n",
            "mkdir: cannot create directory ‘data/vocab’: File exists\n",
            "mkdir: cannot create directory ‘data/model’: File exists\n",
            "--2020-05-23 20:46:57--  https://docs.google.com/uc?export=download&id=1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.111.139, 108.177.111.101, 108.177.111.102, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.111.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-10-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/9frmjboc7hjc3a1b02jivnsdcm803nbm/1590266775000/16970776037313924126/*/1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-05-23 20:46:57--  https://doc-10-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/9frmjboc7hjc3a1b02jivnsdcm803nbm/1590266775000/16970776037313924126/*/1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW?e=download\n",
            "Resolving doc-10-2s-docs.googleusercontent.com (doc-10-2s-docs.googleusercontent.com)... 108.177.112.132, 2607:f8b0:4001:c12::84\n",
            "Connecting to doc-10-2s-docs.googleusercontent.com (doc-10-2s-docs.googleusercontent.com)|108.177.112.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 955428 (933K) [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/english-Arabic-both.pkl’\n",
            "\n",
            "data/AD_NMT-master/ 100%[===================>] 933.04K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2020-05-23 20:46:57 (134 MB/s) - ‘data/AD_NMT-master/english-Arabic-both.pkl’ saved [955428/955428]\n",
            "\n",
            "--2020-05-23 20:46:58--  https://docs.google.com/uc?export=download&id=1V8_tp8ZlWUYaX7QQL46t0uSRNrVehSf1\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.111.139, 108.177.111.113, 108.177.111.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.111.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-14-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/hotc154sr1s9vk8h14gk30mqrshlrkna/1590266775000/16970776037313924126/*/1V8_tp8ZlWUYaX7QQL46t0uSRNrVehSf1?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-05-23 20:46:58--  https://doc-14-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/hotc154sr1s9vk8h14gk30mqrshlrkna/1590266775000/16970776037313924126/*/1V8_tp8ZlWUYaX7QQL46t0uSRNrVehSf1?e=download\n",
            "Resolving doc-14-2s-docs.googleusercontent.com (doc-14-2s-docs.googleusercontent.com)... 108.177.112.132, 2607:f8b0:4001:c12::84\n",
            "Connecting to doc-14-2s-docs.googleusercontent.com (doc-14-2s-docs.googleusercontent.com)|108.177.112.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 95497 (93K) [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/english-Arabic-test.pkl’\n",
            "\n",
            "data/AD_NMT-master/ 100%[===================>]  93.26K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2020-05-23 20:46:58 (132 MB/s) - ‘data/AD_NMT-master/english-Arabic-test.pkl’ saved [95497/95497]\n",
            "\n",
            "--2020-05-23 20:46:59--  https://docs.google.com/uc?export=download&id=1V7X0qtuDIyjTHY0wh-ZNoVwsiF4lId2e\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.111.113, 108.177.111.139, 108.177.111.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.111.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-10-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/9msa12vbamrccoo03rehnb0nsp8n08oi/1590266775000/16970776037313924126/*/1V7X0qtuDIyjTHY0wh-ZNoVwsiF4lId2e?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-05-23 20:46:59--  https://doc-10-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/9msa12vbamrccoo03rehnb0nsp8n08oi/1590266775000/16970776037313924126/*/1V7X0qtuDIyjTHY0wh-ZNoVwsiF4lId2e?e=download\n",
            "Resolving doc-10-2s-docs.googleusercontent.com (doc-10-2s-docs.googleusercontent.com)... 108.177.112.132, 2607:f8b0:4001:c12::84\n",
            "Connecting to doc-10-2s-docs.googleusercontent.com (doc-10-2s-docs.googleusercontent.com)|108.177.112.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 859172 (839K) [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/english-Arabic-train.pkl’\n",
            "\n",
            "data/AD_NMT-master/ 100%[===================>] 839.04K  --.-KB/s    in 0.006s  \n",
            "\n",
            "2020-05-23 20:47:00 (134 MB/s) - ‘data/AD_NMT-master/english-Arabic-train.pkl’ saved [859172/859172]\n",
            "\n",
            "--2020-05-23 20:47:00--  https://docs.google.com/uc?export=download&id=1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV\n",
            "Resolving docs.google.com (docs.google.com)... 172.217.214.139, 172.217.214.100, 172.217.214.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.214.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-00-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/1iim09rh3mif9o7qcq4k0kfs8gt9n63q/1590266775000/16970776037313924126/*/1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-05-23 20:47:01--  https://doc-00-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/1iim09rh3mif9o7qcq4k0kfs8gt9n63q/1590266775000/16970776037313924126/*/1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV?e=download\n",
            "Resolving doc-00-2s-docs.googleusercontent.com (doc-00-2s-docs.googleusercontent.com)... 108.177.112.132, 2607:f8b0:4001:c12::84\n",
            "Connecting to doc-00-2s-docs.googleusercontent.com (doc-00-2s-docs.googleusercontent.com)|108.177.112.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/LAV-MSA-2-both.pkl’\n",
            "\n",
            "data/AD_NMT-master/     [ <=>                ]   2.33M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-05-23 20:47:01 (201 MB/s) - ‘data/AD_NMT-master/LAV-MSA-2-both.pkl’ saved [2447014]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHBIZHP717oD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#James Chartouni\n",
        "#Joey Park\n",
        "#Raef Khan\n",
        "\n",
        "import torch\n",
        "from torch.optim import SGD\n",
        "from torch import nn, optim\n",
        "from torchtext import data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os, io, glob\n",
        "import functools\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from bpemb import BPEmb\n",
        "\n",
        "import transformers\n",
        "from transformers import XLMTokenizer, XLMModel\n",
        "from transformers import T5Tokenizer, T5Model, T5Config\n",
        "import torchtext\n",
        "from torchtext.data import Field, BucketIterator\n",
        "from torchtext.datasets import TranslationDataset\n",
        "\n",
        "\n",
        "import sentencepiece as spm\n",
        "\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0C_7QhY2CpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_path = 'data/AD_NMT-master/'\n",
        "\n",
        "with open(file_path + \"english-Arabic-train.pkl\", 'rb') as handle:\n",
        "    data_English_MSA_trainval = pickle.load(handle)\n",
        "\n",
        "with open(file_path + \"english-Arabic-test.pkl\", 'rb') as handle:\n",
        "    data_English_MSA_test = pickle.load(handle)\n",
        "\n",
        "with open(file_path + \"english-Arabic-both.pkl\", 'rb') as handle:\n",
        "    data_English_MSA_both = pickle.load(handle) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with open(file_path + \"LAV-MSA-2-train.pkl\", 'rb') as handle:\n",
        "    data_LAV_MSA_trainval = pickle.load(handle) \n",
        "\n",
        "with open(file_path + \"LAV-MSA-2-test.pkl\", 'rb') as handle:\n",
        "    data_LAV_MSA_test = pickle.load(handle) \n",
        "\n",
        "with open(file_path + \"LAV-MSA-2-both.pkl\", 'rb') as handle:\n",
        "    data_LAV_MSA_both = pickle.load(handle) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with open(file_path + \"Magribi_MSA-train.pkl\", 'rb') as handle:\n",
        "    data_Magribi_MSA_trainval = pickle.load(handle) \n",
        "    \n",
        "with open(file_path + \"Magribi_MSA-test.pkl\", 'rb') as handle:\n",
        "    data_Magribi_MSA_test = pickle.load(handle) \n",
        "\n",
        "with open(file_path + \"Magribi_MSA-both.pkl\", 'rb') as handle:\n",
        "    data_Magribi_MSA_both = pickle.load(handle) \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAqmicdq2DAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(data_English_MSA_both[0:5])\n",
        "print(data_LAV_MSA_both[0:5])\n",
        "print(len(data_English_MSA_trainval))\n",
        "print(len(data_English_MSA_both))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYZYCLQC6trn",
        "colab_type": "text"
      },
      "source": [
        "##Prepare Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my3IwzMYlwOH",
        "colab_type": "text"
      },
      "source": [
        "SPM Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmCGcvChlwBH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls data/AD_NMT-master/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YK09XFxHlv3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#splits the train dataset into train and validation sets, define test set as datafile\n",
        "eng_msa_train, eng_msa_val = train_test_split(data_English_MSA_trainval, test_size=.2, random_state=22)\n",
        "eng_msa_test = data_English_MSA_test\n",
        "\n",
        "lav_msa_train, lav_msa_val = train_test_split(data_LAV_MSA_trainval, test_size=.2, random_state=22)\n",
        "lav_msa_test = data_LAV_MSA_test\n",
        "\n",
        "mag_msa_train, mag_msa_val = train_test_split(data_Magribi_MSA_trainval, test_size=.2, random_state=22)\n",
        "mag_msa_test = data_Magribi_MSA_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76IhUXDDlvus",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(eng_msa_train))\n",
        "print(len(eng_msa_val))\n",
        "\n",
        "print(len(lav_msa_train))\n",
        "print(len(lav_msa_val))\n",
        "\n",
        "print(len(mag_msa_train))\n",
        "print(len(mag_msa_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGPhoAAwuLv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_path = 'data/'\n",
        "\n",
        "def pytorch_format(ds, src='en', trg='msa', datatype=''):\n",
        "    src_formatted = datatype + '_' + src + '_' + trg + '.' + src\n",
        "    trg_formatted = datatype + '_' + src + '_' + trg + '.' + trg\n",
        "    \n",
        "    with open(file_path + datatype + \"/\" + src_formatted, 'wt') as srctxt, open(file_path + datatype + \"/\" + trg_formatted, 'wt') as trgtxt:\n",
        "        for i, arr in enumerate(ds):\n",
        "            #srctxt.write(datatype.upper() + '_' + str(i).zfill( len(str(len(ds))) - len(str(i))) + '\\\\01\\\\' + arr[0] + '\\n')\n",
        "            #trgtxt.write(datatype.upper() + '_' + str(i).zfill( len(str(len(ds))) - len(str(i))) + '\\\\01\\\\' + arr[1] + '\\n')\n",
        "            srctxt.write(arr[0] + '\\n')\n",
        "            trgtxt.write(arr[1] + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0ZMa0wguMQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#splits each language pair file into datasets of single language, to be merged again by the pytorch dataset class later\n",
        "\n",
        "pytorch_format(eng_msa_train, 'eng', 'msa', 'train')\n",
        "pytorch_format(eng_msa_val, 'eng', 'msa', 'val')\n",
        "pytorch_format(eng_msa_test, 'eng', 'msa', 'test')\n",
        "\n",
        "pytorch_format(lav_msa_train, 'lav', 'msa', 'train')\n",
        "pytorch_format(lav_msa_val, 'lav', 'msa', 'val')\n",
        "pytorch_format(lav_msa_test, 'lav', 'msa', 'test')\n",
        "\n",
        "pytorch_format(mag_msa_train, 'mag', 'msa', 'train')\n",
        "pytorch_format(mag_msa_val, 'mag', 'msa', 'val')\n",
        "pytorch_format(mag_msa_test, 'mag', 'msa', 'test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPQRYfAguMc5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Testing:\n",
        "Create a text file with all the vocab available from all sources for each language for SentencePiece to create a library \n",
        "\n",
        "TODO: implement bpemb model for MSA vocab/train SPM on larger datasets for dialects\n",
        "\"\"\"\n",
        "\n",
        "en_vocab = open(\"data/vocab/eng_vocab.txt\", \"wt\")\n",
        "msa_vocab = open(\"data/vocab/msa_vocab.txt\", \"wt\")\n",
        "lav_vocab = open(\"data/vocab/lav_vocab.txt\", \"wt\")\n",
        "mag_vocab = open(\"data/vocab/mag_vocab.txt\", \"wt\")\n",
        "\n",
        "MSA_text = \"\"\n",
        "EN_text = \"\"\n",
        "\n",
        "def create_vocab(file='', src='en_vocab', tgt='msa_vocab'):\n",
        "  for line in file:\n",
        "        src_sent = line[0]\n",
        "        src_words = src_sent.split(\" \")\n",
        "        for count, word in enumerate(src_words):\n",
        "            src.write(word)\n",
        "        src.write(\"\\n\")\n",
        "        \n",
        "        tgt_sent = line[1]\n",
        "        tgt_words = tgt_sent.split(\" \")\n",
        "        for count, word in enumerate(tgt_words):\n",
        "            tgt.write(word)\n",
        "        tgt.write(\"\\n\")\n",
        "\n",
        "create_vocab(data_English_MSA_both, en_vocab, msa_vocab)\n",
        "create_vocab(data_LAV_MSA_both, lav_vocab, msa_vocab)\n",
        "create_vocab(data_Magribi_MSA_both, mag_vocab, msa_vocab)\n",
        "\n",
        "en_vocab.close()\n",
        "msa_vocab.close()\n",
        "lav_vocab.close()\n",
        "mag_vocab.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgod9_6BMi4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t5tok = T5Tokenizer.from_pretrained(\"t5-base\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eSxQuFaHrr9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(t5tok.bos_token, t5tok.bos_token_id)\n",
        "print(t5tok.eos_token, t5tok.eos_token_id)\n",
        "print(t5tok.pad_token, t5tok.pad_token_id)\n",
        "print(t5tok.unk_token, t5tok.unk_token_id)\n",
        "print(t5tok.sep_token, t5tok.sep_token_id)\n",
        "#print(sp.encode_as_ids(\"<unk>\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6U3XlqeuMl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://github.com/google/sentencepiece/issues/102\n",
        "VOCAB_SIZE = 32128\n",
        "spm.SentencePieceTrainer.train('--input=data/vocab/eng_vocab.txt,data/vocab/msa_vocab.txt,data/vocab/lav_vocab.txt,data/vocab/mag_vocab.txt --model_prefix=data/model/spm --vocab_size=' + str(VOCAB_SIZE) + ' --unk_id=2 --bos_id=-1 --eos_id=1 --pad_id=0')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMeYoGjJuMu_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('data/model/spm.model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1G_QuNFEuM5M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(sp.encode_as_pieces('This is a test'))\n",
        "print(sp.encode_as_ids('This is a test'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZWNSIJkwjNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(sp.encode_as_pieces('هناك شوكة ناقصة'))\n",
        "print(sp.encode_as_ids('هناك شوكة ناقصة'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wo556u17wjTd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(sp.encode_as_pieces('This is a test'))\n",
        "print(sp.encode_as_ids('This is a test'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVsYc4wj_xOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sp.tokenize(\"This is a test\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VehVGWs27UKi",
        "colab_type": "text"
      },
      "source": [
        "Pytorch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOh_Y4GZnUHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://pytorch.org/text/_modules/torchtext/data/field.html\n",
        "class CustomField(Field):\n",
        "  \n",
        "  def __init__(self, tokenize=None, init_token=None, eos_token=None, lower=False):\n",
        "        Field.__init__(self)\n",
        "        self.tokenize = tokenize\n",
        "\n",
        "  def preprocess(self, x):\n",
        "    #print(x)\n",
        "    x = sp.tokenize((x.rstrip('\\n')))\n",
        "    return x\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FZyD9qPqVY3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SRC = CustomField(tokenize = sp,\n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower = False)\n",
        "\n",
        "TRG = CustomField(tokenize = sp,\n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower = False)\n",
        "\n",
        "SRC.use_vocab = True\n",
        "TRG.use_vocab = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MadOtKgwkmhW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(SRC.tokenize.tokenize(\"this is a test\"))\n",
        "callable(SRC.tokenize.tokenize(\"this is a test\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8juNLQxUq1vM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eng_msa_train_dataset = TranslationDataset(path='data/train/train_eng_msa.', exts=('eng', 'msa'), fields=(SRC, TRG))\n",
        "lav_msa_train_dataset = TranslationDataset(path='data/train/train_lav_msa.', exts=('lav', 'msa'), fields=(SRC, TRG))\n",
        "mag_msa_train_dataset = TranslationDataset(path='data/train/train_mag_msa.', exts=('mag', 'msa'), fields=(SRC, TRG))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbjg_bj2OdVb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SRC.build_vocab(eng_msa_train_dataset, lav_msa_train_dataset, mag_msa_train_dataset)\n",
        "TRG.build_vocab(eng_msa_train_dataset, lav_msa_train_dataset, mag_msa_train_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJ-fMU08ENX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, valid_data, test_data = eng_msa_train_dataset.splits(path= 'data/', train='train/train_eng_msa', validation='val/val_eng_msa', test='test/test_eng_msa', exts=('.eng', '.msa'),\n",
        "                                                    fields = (SRC, TRG))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rmmEiTLrGO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_data[10].src)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0A4HJN4FEOdb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE =32\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device,\n",
        "    shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7nLAt5DrjLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pad_idx = TRG.vocab.stoi['<pad>']\n",
        "eos_idx = TRG.vocab.stoi['<eos>']\n",
        "sos_idx = TRG.vocab.stoi['<sos>']\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hq1ASMZt5OZN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(pad_idx)\n",
        "print(eos_idx)\n",
        "print(sos_idx)\n",
        "print(TRG.vocab.stoi)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpIy0T8SS4tZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model: nn.Module,\n",
        "          iterator: BucketIterator,\n",
        "          optimizer: optim.Optimizer,\n",
        "          criterion: nn.Module,\n",
        "          clip: float):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for _, batch in enumerate(iterator):\n",
        "\n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #output = model(src, trg)\n",
        "        output = model(input_ids=src, decoder_input_ids=trg)\n",
        "\n",
        "        output = output[1:].view(-1, output.shape[-1])\n",
        "        trg = trg[1:].view(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxuCY5pIS5C-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model: nn.Module,\n",
        "             iterator: BucketIterator,\n",
        "             criterion: nn.Module):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for _, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output = model(src, trg, 0) #turn off teacher forcing?\n",
        "\n",
        "            output = output[1:].view(-1, output.shape[-1])\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_wcMIRjkH5n",
        "colab_type": "text"
      },
      "source": [
        "##XLM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Um_9ERlDkHqh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xlmr = torch.hub.load('pytorch/fairseq', 'xlmr.base')\n",
        "xlmr.train();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kph4Nc99jg07",
        "colab_type": "text"
      },
      "source": [
        "##T5 Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B71sP10ln-bN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "model = T5Model.from_pretrained('t5-small')\n",
        "input_ids = tokenizer.encode(\"Hello, my dog is cute\", return_tensors=\"pt\")  # Batch size 1\n",
        "print(input_ids)\n",
        "outputs = model(input_ids=input_ids, decoder_input_ids=input_ids)\n",
        "last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
        "print(last_hidden_states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwjMDk5KjgSR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = sp\n",
        "model = T5Model.from_pretrained('t5-small')\n",
        "input_ids = tokenizer.encode_as_ids(\"Hello, my dog is cute\")  # Batch size 1\n",
        "input_ids = torch.LongTensor([input_ids])\n",
        "print(input_ids)\n",
        "outputs = model(input_ids=input_ids, decoder_input_ids=input_ids)\n",
        "last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
        "print(last_hidden_states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQWwA17pzWD-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = T5Config(vocab_size=VOCAB_SIZE)\n",
        "model = T5Model(config)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6Xu7n9qzVL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_SZhnYT1EDE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXREuT2XnJPQ",
        "colab_type": "text"
      },
      "source": [
        "## Model Training Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGw3PImZeQdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = T5Model.from_pretrained('t5-small')\n",
        "#model = xlmr\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.0001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH0BE6gBS5ja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ_uuRGLT7SG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tok = T5Tokenizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLDwBrtuE-XQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}