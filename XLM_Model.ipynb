{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XLM_Model.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jameschartouni/arabic_translation/blob/xlm/XLM_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQGE8JtI2EAt",
        "colab_type": "code",
        "outputId": "62aca447-df91-45c4-d6e1-67fb6f31e2c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#installation step\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install bpemb\n",
        "#creating the folders \n",
        "!mkdir data/\n",
        "!mkdir data/AD_NMT-master\n",
        "!mkdir data/train/\n",
        "!mkdir data/test/\n",
        "!mkdir data/val/\n",
        "!mkdir data/vocab\n",
        "!mkdir data/model\n",
        "\n",
        "#fetching the pkl files\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW' -O data/AD_NMT-master/english-Arabic-both.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1V8_tp8ZlWUYaX7QQL46t0uSRNrVehSf1' -O data/AD_NMT-master/english-Arabic-test.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1V7X0qtuDIyjTHY0wh-ZNoVwsiF4lId2e' -O data/AD_NMT-master/english-Arabic-train.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV' -O data/AD_NMT-master/LAV-MSA-2-both.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UpfCbkxhztof7dvNjeAs1bHjD4SER6h3' -O data/AD_NMT-master/LAV-MSA-2-test.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UlAZGtYsSfXzK7hrC_PbxQFqTSXD0DMw' -O data/AD_NMT-master/LAV-MSA-2-train.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UjDX7cCG2S23SPfSHxSPdVayMTxB5Y16' -O data/AD_NMT-master/Magribi_MSA-both.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UaVWIqRXo0rxuxDF4KArA4bEK1TaLX3l' -O data/AD_NMT-master/Magribi_MSA-test.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UYvlhdYAdfa4riP_4hn3-IEVd1ZUXVTQ' -O data/AD_NMT-master/Magribi_MSA-train.pkl"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/97/7db72a0beef1825f82188a4b923e62a146271ac2ced7928baa4d47ef2467/transformers-2.9.1-py3-none-any.whl (641kB)\n",
            "\r\u001b[K     |▌                               | 10kB 17.4MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 3.0MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 4.1MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 4.4MB/s eta 0:00:01\r\u001b[K     |██▌                             | 51kB 3.5MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 4.0MB/s eta 0:00:01\r\u001b[K     |███▋                            | 71kB 4.2MB/s eta 0:00:01\r\u001b[K     |████                            | 81kB 4.6MB/s eta 0:00:01\r\u001b[K     |████▋                           | 92kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 102kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 112kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 122kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 133kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 143kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 153kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 163kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 174kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 184kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 194kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 204kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 215kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 225kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 235kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 245kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 256kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 266kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 276kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 286kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 296kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 307kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 317kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 327kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 337kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 348kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 358kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 368kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 378kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 389kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 399kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 409kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 419kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 430kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 440kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 450kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 460kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 471kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 481kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 491kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 501kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 512kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 522kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 532kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 542kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 552kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 563kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 573kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 583kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 593kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 604kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 614kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 624kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 634kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 645kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/88/49e772d686088e1278766ad68a463513642a2a877487decbd691dec02955/sentencepiece-0.1.90-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 26.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 34.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 39.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=598c8e919a8d431f1f0449e4999e53ed77b5c04f995a72b6577b7802f687a216\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.90 tokenizers-0.7.0 transformers-2.9.1\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.90)\n",
            "Collecting bpemb\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/70/468a9652095b370f797ed37ff77e742b11565c6fd79eaeca5f2e50b164a7/bpemb-0.3.0-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from bpemb) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from bpemb) (2.23.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from bpemb) (0.1.90)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from bpemb) (3.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from bpemb) (1.18.4)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (2020.4.5.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim->bpemb) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim->bpemb) (1.12.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->bpemb) (2.0.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->bpemb) (1.13.4)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->bpemb) (2.49.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->bpemb) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->bpemb) (1.16.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->bpemb) (0.3.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.4->boto3->smart-open>=1.2.1->gensim->bpemb) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.4->boto3->smart-open>=1.2.1->gensim->bpemb) (0.15.2)\n",
            "Installing collected packages: bpemb\n",
            "Successfully installed bpemb-0.3.0\n",
            "--2020-05-16 22:23:55--  https://docs.google.com/uc?export=download&id=1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.195.138, 74.125.195.102, 74.125.195.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.195.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-10-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/imovgpgbb7gdpv16tg3i6rqn3pcg9iq3/1589667825000/16970776037313924126/*/1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-05-16 22:23:55--  https://doc-10-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/imovgpgbb7gdpv16tg3i6rqn3pcg9iq3/1589667825000/16970776037313924126/*/1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW?e=download\n",
            "Resolving doc-10-2s-docs.googleusercontent.com (doc-10-2s-docs.googleusercontent.com)... 74.125.197.132, 2607:f8b0:400e:c03::84\n",
            "Connecting to doc-10-2s-docs.googleusercontent.com (doc-10-2s-docs.googleusercontent.com)|74.125.197.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 955428 (933K) [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/english-Arabic-both.pkl’\n",
            "\n",
            "data/AD_NMT-master/ 100%[===================>] 933.04K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2020-05-16 22:23:55 (107 MB/s) - ‘data/AD_NMT-master/english-Arabic-both.pkl’ saved [955428/955428]\n",
            "\n",
            "--2020-05-16 22:23:56--  https://docs.google.com/uc?export=download&id=1V8_tp8ZlWUYaX7QQL46t0uSRNrVehSf1\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.195.139, 74.125.195.101, 74.125.195.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.195.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-14-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/qqd9f9vvml0kiio1sqvh5ia99ru1sc5o/1589667825000/16970776037313924126/*/1V8_tp8ZlWUYaX7QQL46t0uSRNrVehSf1?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-05-16 22:23:56--  https://doc-14-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/qqd9f9vvml0kiio1sqvh5ia99ru1sc5o/1589667825000/16970776037313924126/*/1V8_tp8ZlWUYaX7QQL46t0uSRNrVehSf1?e=download\n",
            "Resolving doc-14-2s-docs.googleusercontent.com (doc-14-2s-docs.googleusercontent.com)... 74.125.197.132, 2607:f8b0:400e:c03::84\n",
            "Connecting to doc-14-2s-docs.googleusercontent.com (doc-14-2s-docs.googleusercontent.com)|74.125.197.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 95497 (93K) [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/english-Arabic-test.pkl’\n",
            "\n",
            "data/AD_NMT-master/ 100%[===================>]  93.26K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2020-05-16 22:23:57 (100 MB/s) - ‘data/AD_NMT-master/english-Arabic-test.pkl’ saved [95497/95497]\n",
            "\n",
            "--2020-05-16 22:23:58--  https://docs.google.com/uc?export=download&id=1V7X0qtuDIyjTHY0wh-ZNoVwsiF4lId2e\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.195.139, 74.125.195.113, 74.125.195.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.195.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-10-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/mj9vm4d288ss8c21560skv9e8a4l2d37/1589667825000/16970776037313924126/*/1V7X0qtuDIyjTHY0wh-ZNoVwsiF4lId2e?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-05-16 22:23:59--  https://doc-10-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/mj9vm4d288ss8c21560skv9e8a4l2d37/1589667825000/16970776037313924126/*/1V7X0qtuDIyjTHY0wh-ZNoVwsiF4lId2e?e=download\n",
            "Resolving doc-10-2s-docs.googleusercontent.com (doc-10-2s-docs.googleusercontent.com)... 74.125.197.132, 2607:f8b0:400e:c03::84\n",
            "Connecting to doc-10-2s-docs.googleusercontent.com (doc-10-2s-docs.googleusercontent.com)|74.125.197.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 859172 (839K) [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/english-Arabic-train.pkl’\n",
            "\n",
            "data/AD_NMT-master/ 100%[===================>] 839.04K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2020-05-16 22:23:59 (106 MB/s) - ‘data/AD_NMT-master/english-Arabic-train.pkl’ saved [859172/859172]\n",
            "\n",
            "--2020-05-16 22:23:59--  https://docs.google.com/uc?export=download&id=1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.195.101, 74.125.195.113, 74.125.195.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.195.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-00-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/sa6uf7sn92679ahrp1sl0ai4icp91h8e/1589667825000/16970776037313924126/*/1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-05-16 22:24:00--  https://doc-00-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/sa6uf7sn92679ahrp1sl0ai4icp91h8e/1589667825000/16970776037313924126/*/1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV?e=download\n",
            "Resolving doc-00-2s-docs.googleusercontent.com (doc-00-2s-docs.googleusercontent.com)... 74.125.197.132, 2607:f8b0:400e:c03::84\n",
            "Connecting to doc-00-2s-docs.googleusercontent.com (doc-00-2s-docs.googleusercontent.com)|74.125.197.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/LAV-MSA-2-both.pkl’\n",
            "\n",
            "data/AD_NMT-master/     [ <=>                ]   2.33M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-05-16 22:24:00 (197 MB/s) - ‘data/AD_NMT-master/LAV-MSA-2-both.pkl’ saved [2447014]\n",
            "\n",
            "--2020-05-16 22:24:01--  https://docs.google.com/uc?export=download&id=1UpfCbkxhztof7dvNjeAs1bHjD4SER6h3\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.195.139, 74.125.195.113, 74.125.195.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.195.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0c-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/rbhgvv05r22lskfniudiss8egh59a117/1589667825000/16970776037313924126/*/1UpfCbkxhztof7dvNjeAs1bHjD4SER6h3?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-05-16 22:24:02--  https://doc-0c-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/rbhgvv05r22lskfniudiss8egh59a117/1589667825000/16970776037313924126/*/1UpfCbkxhztof7dvNjeAs1bHjD4SER6h3?e=download\n",
            "Resolving doc-0c-2s-docs.googleusercontent.com (doc-0c-2s-docs.googleusercontent.com)... 74.125.197.132, 2607:f8b0:400e:c03::84\n",
            "Connecting to doc-0c-2s-docs.googleusercontent.com (doc-0c-2s-docs.googleusercontent.com)|74.125.197.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 304332 (297K) [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/LAV-MSA-2-test.pkl’\n",
            "\n",
            "data/AD_NMT-master/ 100%[===================>] 297.20K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2020-05-16 22:24:02 (136 MB/s) - ‘data/AD_NMT-master/LAV-MSA-2-test.pkl’ saved [304332/304332]\n",
            "\n",
            "--2020-05-16 22:24:02--  https://docs.google.com/uc?export=download&id=1UlAZGtYsSfXzK7hrC_PbxQFqTSXD0DMw\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.195.101, 74.125.195.138, 74.125.195.102, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.195.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0c-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/16bkemual8d64ciuuc9ndkgqmodgc5s7/1589667825000/16970776037313924126/*/1UlAZGtYsSfXzK7hrC_PbxQFqTSXD0DMw?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-05-16 22:24:03--  https://doc-0c-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/16bkemual8d64ciuuc9ndkgqmodgc5s7/1589667825000/16970776037313924126/*/1UlAZGtYsSfXzK7hrC_PbxQFqTSXD0DMw?e=download\n",
            "Resolving doc-0c-2s-docs.googleusercontent.com (doc-0c-2s-docs.googleusercontent.com)... 74.125.197.132, 2607:f8b0:400e:c03::84\n",
            "Connecting to doc-0c-2s-docs.googleusercontent.com (doc-0c-2s-docs.googleusercontent.com)|74.125.197.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/LAV-MSA-2-train.pkl’\n",
            "\n",
            "data/AD_NMT-master/     [ <=>                ]   2.04M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-05-16 22:24:03 (170 MB/s) - ‘data/AD_NMT-master/LAV-MSA-2-train.pkl’ saved [2141923]\n",
            "\n",
            "--2020-05-16 22:24:04--  https://docs.google.com/uc?export=download&id=1UjDX7cCG2S23SPfSHxSPdVayMTxB5Y16\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.195.139, 74.125.195.113, 74.125.195.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.195.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-04-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/pkl248mk8dcgj2rlu7ssh1ebc1p961sd/1589667825000/16970776037313924126/*/1UjDX7cCG2S23SPfSHxSPdVayMTxB5Y16?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-05-16 22:24:04--  https://doc-04-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/pkl248mk8dcgj2rlu7ssh1ebc1p961sd/1589667825000/16970776037313924126/*/1UjDX7cCG2S23SPfSHxSPdVayMTxB5Y16?e=download\n",
            "Resolving doc-04-2s-docs.googleusercontent.com (doc-04-2s-docs.googleusercontent.com)... 74.125.197.132, 2607:f8b0:400e:c03::84\n",
            "Connecting to doc-04-2s-docs.googleusercontent.com (doc-04-2s-docs.googleusercontent.com)|74.125.197.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/Magribi_MSA-both.pkl’\n",
            "\n",
            "data/AD_NMT-master/     [ <=>                ]   2.81M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-05-16 22:24:05 (215 MB/s) - ‘data/AD_NMT-master/Magribi_MSA-both.pkl’ saved [2944107]\n",
            "\n",
            "--2020-05-16 22:24:06--  https://docs.google.com/uc?export=download&id=1UaVWIqRXo0rxuxDF4KArA4bEK1TaLX3l\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.195.101, 74.125.195.100, 74.125.195.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.195.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0o-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/0tbadets4lr0c8vc2tsl9rti1n7ovrjr/1589667825000/16970776037313924126/*/1UaVWIqRXo0rxuxDF4KArA4bEK1TaLX3l?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-05-16 22:24:06--  https://doc-0o-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/0tbadets4lr0c8vc2tsl9rti1n7ovrjr/1589667825000/16970776037313924126/*/1UaVWIqRXo0rxuxDF4KArA4bEK1TaLX3l?e=download\n",
            "Resolving doc-0o-2s-docs.googleusercontent.com (doc-0o-2s-docs.googleusercontent.com)... 74.125.197.132, 2607:f8b0:400e:c03::84\n",
            "Connecting to doc-0o-2s-docs.googleusercontent.com (doc-0o-2s-docs.googleusercontent.com)|74.125.197.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 290840 (284K) [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/Magribi_MSA-test.pkl’\n",
            "\n",
            "data/AD_NMT-master/ 100%[===================>] 284.02K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2020-05-16 22:24:06 (127 MB/s) - ‘data/AD_NMT-master/Magribi_MSA-test.pkl’ saved [290840/290840]\n",
            "\n",
            "--2020-05-16 22:24:07--  https://docs.google.com/uc?export=download&id=1UYvlhdYAdfa4riP_4hn3-IEVd1ZUXVTQ\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.195.101, 74.125.195.100, 74.125.195.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.195.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0s-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/58n0sglkmqbtgpu690fs64ln6m2gcoe3/1589667825000/16970776037313924126/*/1UYvlhdYAdfa4riP_4hn3-IEVd1ZUXVTQ?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-05-16 22:24:07--  https://doc-0s-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/58n0sglkmqbtgpu690fs64ln6m2gcoe3/1589667825000/16970776037313924126/*/1UYvlhdYAdfa4riP_4hn3-IEVd1ZUXVTQ?e=download\n",
            "Resolving doc-0s-2s-docs.googleusercontent.com (doc-0s-2s-docs.googleusercontent.com)... 74.125.197.132, 2607:f8b0:400e:c03::84\n",
            "Connecting to doc-0s-2s-docs.googleusercontent.com (doc-0s-2s-docs.googleusercontent.com)|74.125.197.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/Magribi_MSA-train.pkl’\n",
            "\n",
            "data/AD_NMT-master/     [ <=>                ]   2.53M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-05-16 22:24:07 (226 MB/s) - ‘data/AD_NMT-master/Magribi_MSA-train.pkl’ saved [2652508]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHBIZHP717oD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#James Chartouni\n",
        "#Joey Park\n",
        "#Raef Khan\n",
        "\n",
        "import torch\n",
        "from torch.optim import SGD\n",
        "from torchtext import data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os, io, glob\n",
        "import functools\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from bpemb import BPEmb\n",
        "\n",
        "import transformers\n",
        "from transformers import XLMTokenizer, XLMModel\n",
        "import torchtext\n",
        "from torchtext.data import Field\n",
        "from torchtext.datasets import TranslationDataset\n",
        "\n",
        "\n",
        "import sentencepiece as spm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0C_7QhY2CpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_path = 'data/AD_NMT-master/'\n",
        "\n",
        "with open(file_path + \"english-Arabic-train.pkl\", 'rb') as handle:\n",
        "    data_English_MSA_trainval = pickle.load(handle)\n",
        "\n",
        "with open(file_path + \"english-Arabic-test.pkl\", 'rb') as handle:\n",
        "    data_English_MSA_test = pickle.load(handle)\n",
        "\n",
        "with open(file_path + \"english-Arabic-both.pkl\", 'rb') as handle:\n",
        "    data_English_MSA_both = pickle.load(handle) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with open(file_path + \"LAV-MSA-2-train.pkl\", 'rb') as handle:\n",
        "    data_LAV_MSA_trainval = pickle.load(handle) \n",
        "\n",
        "with open(file_path + \"LAV-MSA-2-test.pkl\", 'rb') as handle:\n",
        "    data_LAV_MSA_test = pickle.load(handle) \n",
        "\n",
        "with open(file_path + \"LAV-MSA-2-both.pkl\", 'rb') as handle:\n",
        "    data_LAV_MSA_both = pickle.load(handle) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with open(file_path + \"Magribi_MSA-train.pkl\", 'rb') as handle:\n",
        "    data_Magribi_MSA_trainval = pickle.load(handle) \n",
        "    \n",
        "with open(file_path + \"Magribi_MSA-test.pkl\", 'rb') as handle:\n",
        "    data_Magribi_MSA_test = pickle.load(handle) \n",
        "\n",
        "with open(file_path + \"Magribi_MSA-both.pkl\", 'rb') as handle:\n",
        "    data_Magribi_MSA_both = pickle.load(handle) \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAqmicdq2DAt",
        "colab_type": "code",
        "outputId": "60115254-9c53-489b-9c48-8184e3ee9a55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "print(data_English_MSA_both[0:5])\n",
        "print(data_LAV_MSA_both[0:5])\n",
        "print(len(data_English_MSA_trainval))\n",
        "print(len(data_English_MSA_both))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Tom was also there', 'كان توم هنا ايضا'], ['That old woman lives by herself', 'تلك المراة العجوز تسكن بمفردها'], ['He went abroad for the purpose of studying English', 'سافر خارج البلد ليتعلم الانجليزية'], ['There is a fork missing', 'هناك شوكة ناقصة'], [\"I don't know this game\", 'لا اعرف هذه اللعبة']]\n",
            "[['لا انا بعرف وحدة راحت ع فرنسا و معا شنتا حطت فيها الفرش', 'لا اعرف واحدة ذهبت الى فرنسا و لها غرفة و ضعت فيها الافرشة'], ['روح بوشك و فتول عاليسار', 'اذهب تقدم و استدر يسارا'], ['لا لا لازم انه يكون عندك موضوع ما في اشي', ' لا لا يجب ان يكون لديك موضوع هذا ضروري'], ['اوعي تبعدي من هون بلاش تضيعي ', 'لا تبتعد عن هنا حتى لا تفقد الطريق '], ['قصدي صراحة يما انا كمان كرهته من يوم ما عملتيه زي ما بتعمله خالتي كرهته و صرت ما باطيقه بالمرة', 'اقصد صراحة يا امي انا ايضا كرهته من يوم حضرته مثلما تحضره خالتي كرهته و اصبحت لا اطيقه ابدا']]\n",
            "9000\n",
            "10001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYZYCLQC6trn",
        "colab_type": "text"
      },
      "source": [
        "##Prepare Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my3IwzMYlwOH",
        "colab_type": "text"
      },
      "source": [
        "SPM Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmCGcvChlwBH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "190d066c-4b13-4cd0-d445-034526dd4ddc"
      },
      "source": [
        "ls data/AD_NMT-master/"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "english-Arabic-both.pkl   LAV-MSA-2-both.pkl   Magribi_MSA-both.pkl\n",
            "english-Arabic-test.pkl   LAV-MSA-2-test.pkl   Magribi_MSA-test.pkl\n",
            "english-Arabic-train.pkl  LAV-MSA-2-train.pkl  Magribi_MSA-train.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YK09XFxHlv3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#splits the train dataset into train and validation sets, define test set as datafile\n",
        "eng_msa_train, eng_msa_val = train_test_split(data_English_MSA_trainval, test_size=.2, random_state=22)\n",
        "eng_msa_test = data_English_MSA_test\n",
        "\n",
        "lav_msa_train, lav_msa_val = train_test_split(data_LAV_MSA_trainval, test_size=.2, random_state=22)\n",
        "lav_msa_test = data_LAV_MSA_test\n",
        "\n",
        "mag_msa_train, mag_msa_val = train_test_split(data_Magribi_MSA_trainval, test_size=.2, random_state=22)\n",
        "mag_msa_test = data_Magribi_MSA_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76IhUXDDlvus",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "56301dbb-08dd-4293-9290-19e972230976"
      },
      "source": [
        "print(len(eng_msa_train))\n",
        "print(len(eng_msa_val))\n",
        "\n",
        "print(len(lav_msa_train))\n",
        "print(len(lav_msa_val))\n",
        "\n",
        "print(len(mag_msa_train))\n",
        "print(len(mag_msa_val))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7200\n",
            "1800\n",
            "11044\n",
            "2761\n",
            "14188\n",
            "3548\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGPhoAAwuLv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_path = 'data/'\n",
        "\n",
        "def pytorch_format(ds, src='en', trg='msa', datatype=''):\n",
        "    src_formatted = datatype + '_' + src + '_' + trg + '.' + src\n",
        "    trg_formatted = datatype + '_' + src + '_' + trg + '.' + trg\n",
        "    \n",
        "    with open(file_path + datatype + \"/\" + src_formatted, 'wt') as srctxt, open(file_path + datatype + \"/\" + trg_formatted, 'wt') as trgtxt:\n",
        "        for i, arr in enumerate(ds):\n",
        "            srctxt.write(datatype.upper() + '_' + str(i).zfill( len(str(len(ds))) - len(str(i))) + '\\\\01\\\\' + arr[0] + '\\n')\n",
        "            trgtxt.write(datatype.upper() + '_' + str(i).zfill( len(str(len(ds))) - len(str(i))) + '\\\\01\\\\' + arr[1] + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0ZMa0wguMQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#splits each language pair file into datasets of single language, to be merged again by the pytorch dataset class later\n",
        "\n",
        "pytorch_format(eng_msa_train, 'eng', 'msa', 'train')\n",
        "pytorch_format(eng_msa_val, 'eng', 'msa', 'val')\n",
        "pytorch_format(eng_msa_test, 'eng', 'msa', 'test')\n",
        "\n",
        "pytorch_format(lav_msa_train, 'lav', 'msa', 'train')\n",
        "pytorch_format(lav_msa_val, 'lav', 'msa', 'val')\n",
        "pytorch_format(lav_msa_test, 'lav', 'msa', 'test')\n",
        "\n",
        "pytorch_format(mag_msa_train, 'mag', 'msa', 'train')\n",
        "pytorch_format(mag_msa_val, 'mag', 'msa', 'val')\n",
        "pytorch_format(mag_msa_test, 'mag', 'msa', 'test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPQRYfAguMc5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Testing:\n",
        "Create a text file with all the vocab available from all sources for each language for SentencePiece to create a library \n",
        "\n",
        "TODO: implement bpemb model for MSA vocab/train SPM on larger datasets for dialects\n",
        "\"\"\"\n",
        "\n",
        "en_vocab = open(\"data/vocab/eng_vocab.txt\", \"wt\")\n",
        "msa_vocab = open(\"data/vocab/msa_vocab.txt\", \"wt\")\n",
        "lav_vocab = open(\"data/vocab/lav_vocab.txt\", \"wt\")\n",
        "mag_vocab = open(\"data/vocab/mag_vocab.txt\", \"wt\")\n",
        "\n",
        "MSA_text = \"\"\n",
        "EN_text = \"\"\n",
        "\n",
        "def create_vocab(file='', src='en_vocab', tgt='msa_vocab'):\n",
        "  for line in file:\n",
        "        src_sent = line[0]\n",
        "        src_words = src_sent.split(\" \")\n",
        "        for count, word in enumerate(src_words):\n",
        "            src.write(word)\n",
        "        src.write(\"\\n\")\n",
        "        \n",
        "        tgt_sent = line[1]\n",
        "        tgt_words = tgt_sent.split(\" \")\n",
        "        for count, word in enumerate(tgt_words):\n",
        "            tgt.write(word)\n",
        "        tgt.write(\"\\n\")\n",
        "\n",
        "create_vocab(data_English_MSA_both, en_vocab, msa_vocab)\n",
        "create_vocab(data_LAV_MSA_both, lav_vocab, msa_vocab)\n",
        "create_vocab(data_Magribi_MSA_both, mag_vocab, msa_vocab)\n",
        "\n",
        "en_vocab.close()\n",
        "msa_vocab.close()\n",
        "lav_vocab.close()\n",
        "mag_vocab.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6U3XlqeuMl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = 32128\n",
        "spm.SentencePieceTrainer.train('--input=data/vocab/eng_vocab.txt,data/vocab/msa_vocab.txt,data/vocab/lav_vocab.txt,data/vocab/mag_vocab.txt --model_prefix=data/model/spm --vocab_size=' + str(VOCAB_SIZE))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMeYoGjJuMu_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "01ebd1a5-be80-409b-f8d8-1d0ef43a5f64"
      },
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('data/model/spm.model')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1G_QuNFEuM5M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2979d57b-cb96-4f47-bb3b-de176a418e71"
      },
      "source": [
        "print(sp.encode_as_pieces('This is a test'))\n",
        "print(sp.encode_as_ids('This is a test'))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁This', '▁', 'is', '▁', 'a', '▁', 't', 'est']\n",
            "[552, 3, 100, 3, 76, 3, 52, 3071]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZWNSIJkwjNR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fd8c7b89-ac8f-482e-cc69-d8613233ca9b"
      },
      "source": [
        "print(sp.encode_as_pieces('هناك شوكة ناقصة'))\n",
        "print(sp.encode_as_ids('هناك شوكة ناقصة'))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁هناك', '▁', 'شوكة', '▁', 'ناقصة']\n",
            "[396, 3, 10425, 3, 10999]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wo556u17wjTd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8b232dba-36e0-4ded-b3b7-31f61857f82a"
      },
      "source": [
        "print(sp.encode_as_pieces('This is a test'))\n",
        "print(sp.encode_as_ids('This is a test'))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁This', '▁', 'is', '▁', 'a', '▁', 't', 'est']\n",
            "[552, 3, 100, 3, 76, 3, 52, 3071]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVsYc4wj_xOX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dbf63f68-8f13-4c68-99e5-1254958be3bf"
      },
      "source": [
        "sp.tokenize(\"This is a test\")"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[552, 3, 100, 3, 76, 3, 52, 3071]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VehVGWs27UKi",
        "colab_type": "text"
      },
      "source": [
        "Pytorch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acW_rMS765sV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#build custom pytorch dataset class\n",
        "\n",
        "#BucketIteterator code \n",
        "#https://pytorch.org/text/_modules/torchtext/data/iterator.html"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8ZG-mKv93Zz",
        "colab_type": "code",
        "outputId": "51b4427a-267a-4001-d4b4-308b304919df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls data/"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AD_NMT-master  model  test  train  val\tvocab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOh_Y4GZnUHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://pytorch.org/text/_modules/torchtext/data/field.html\n",
        "import six\n",
        "\n",
        "class CustomField(Field):\n",
        "  \n",
        "  def __init__(self, tokenize=None, init_token=None, eos_token=None, lower=False):\n",
        "        Field.__init__(self)\n",
        "        self.tokenize = tokenize\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FZyD9qPqVY3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SRC = CustomField(tokenize = sp,\n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower = False)\n",
        "\n",
        "TRG = CustomField(tokenize = sp,\n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8juNLQxUq1vM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "91fead37-134a-4867-87f1-c96fd261fe80"
      },
      "source": [
        "eng_msa_train_dataset = TranslationDataset(path='data/train/train_eng_msa.', exts=('eng', 'msa'), fields=(SRC, TRG))\n",
        "lav_msa_train_dataset = TranslationDataset(path='data/train/train_lav_msa.', exts=('lav', 'msa'), fields=(SRC, TRG))\n",
        "mag_msa_train_dataset = TranslationDataset(path='data/train/train_mag_msa.', exts=('mag', 'msa'), fields=(SRC, TRG))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-8e774ed17a0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meng_msa_train_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTranslationDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data/train/train_eng_msa.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'eng'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'msa'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSRC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlav_msa_train_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTranslationDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data/train/train_lav_msa.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lav'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'msa'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSRC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmag_msa_train_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTranslationDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data/train/train_mag_msa.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mag'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'msa'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSRC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/datasets/translation.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, exts, fields, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msrc_line\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtrg_line\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                     examples.append(data.Example.fromlist(\n\u001b[0;32m---> 40\u001b[0;31m                         [src_line, trg_line], fields))\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTranslationDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/example.py\u001b[0m in \u001b[0;36mfromlist\u001b[0;34m(cls, data, fields)\u001b[0m\n\u001b[1;32m     50\u001b[0m                         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    177\u001b[0m         if (six.PY2 and isinstance(x, six.string_types) and\n\u001b[1;32m    178\u001b[0m                 not isinstance(x, six.text_type)):\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'SentencePieceProcessor' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_iPdJQG651R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomTranslationDataset(data.Dataset):\n",
        "\n",
        "  '''Create a CustomTranslationDataset Class\n",
        "\n",
        "  Arguements:\n",
        "    path: Common prefix of paths to the data files for both languages \n",
        "    exts: a tuple containing the extension to path for each language \n",
        "\n",
        "  '''\n",
        "\n",
        "\n",
        "  def __init__(self, path, exts):\n",
        "\n",
        "    src_path, trg_path = tuple(os.path.expanduser(path + x) for x in exts)\n",
        "    print(src_path, trg_path)\n",
        "\n",
        "    examples = []\n",
        "    with io.open(src_path, mode='r', encoding='utf-8') as src_file, io.open(trg_path, mode='r', encoding='utf-8') as trg_file:\n",
        "      for src_line, trg_line in zip(src_file, trg_file):\n",
        "                src_line, trg_line = src_line.strip(), trg_line.strip()\n",
        "                if src_line != '' and trg_line != '':\n",
        "                    examples.append(data.Example.fromlist(\n",
        "                        [src_line, trg_line]))\n",
        "                    \n",
        "  \n",
        "\n",
        "  @classmethod\n",
        "  def splits(cls, exts, path=None, root='.data',\n",
        "              train='train', validation='val', test='test', **kwargs):\n",
        "      \"\"\"Create dataset objects for splits of a TranslationDataset.\n",
        "      Arguments:\n",
        "          exts: A tuple containing the extension to path for each language.\n",
        "          fields: A tuple containing the fields that will be used for data\n",
        "              in each language.\n",
        "          path (str): Common prefix of the splits' file paths, or None to use\n",
        "              the result of cls.download(root).\n",
        "          root: Root dataset storage directory. Default is '.data'.\n",
        "          train: The prefix of the train data. Default: 'train'.\n",
        "          validation: The prefix of the validation data. Default: 'val'.\n",
        "          test: The prefix of the test data. Default: 'test'.\n",
        "          Remaining keyword arguments: Passed to the splits method of\n",
        "              Dataset.\n",
        "      \"\"\"\n",
        "      if path is None:\n",
        "          path = cls.download(root)\n",
        "\n",
        "      train_data = None if train is None else cls(\n",
        "          os.path.join(path, train), exts, **kwargs)\n",
        "      val_data = None if validation is None else cls(\n",
        "          os.path.join(path, validation), exts, **kwargs)\n",
        "      test_data = None if test is None else cls(\n",
        "          os.path.join(path, test), exts, **kwargs)\n",
        "      return tuple(d for d in (train_data, val_data, test_data)\n",
        "                    if d is not None)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlU85HZo65_W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "18faf9b4-e2ca-4a1f-9e36-c9ec6aa47011"
      },
      "source": [
        "ls data/train"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_eng_msa.eng  train_lav_msa.lav  train_mag_msa.mag\n",
            "train_eng_msa.msa  train_lav_msa.msa  train_mag_msa.msa\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6vbB_L666Id",
        "colab_type": "code",
        "outputId": "677813ff-1886-40ad-e6a6-cd8365f7715c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        }
      },
      "source": [
        "eng_msa_train_dataset = CustomTranslationDataset(path='data/train/train_eng_msa.', exts=('eng', 'msa'))\n",
        "lav_msa_train_dataset = CustomTranslationDataset(path='data/train/train_lav_msa.', exts=('lav', 'msa'))\n",
        "mag_msa_train_dataset = CustomTranslationDataset(path='data/train/train_mag_msa.', exts=('mag', 'msa'))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/train/train_eng_msa.eng data/train/train_eng_msa.msa\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-df1ae9d18e0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meng_msa_train_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomTranslationDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data/train/train_eng_msa.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'eng'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'msa'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlav_msa_train_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomTranslationDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data/train/train_lav_msa.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lav'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'msa'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmag_msa_train_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomTranslationDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data/train/train_mag_msa.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mag'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'msa'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-046a308f4676>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, exts)\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msrc_line\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtrg_line\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                     examples.append(data.Example.fromlist(\n\u001b[0;32m---> 23\u001b[0;31m                         [src_line, trg_line]))\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: fromlist() missing 1 required positional argument: 'fields'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ypsh5Dc66Rh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJ-fMU08ENX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train, validation and test are probably wrong.  \n",
        "#https://github.com/pytorch/text/blob/master/torchtext/datasets/translation.py\n",
        "\n",
        "'''\n",
        "Arguments:\n",
        "            exts: A tuple containing the extension to path for each language.\n",
        "            fields: A tuple containing the fields that will be used for data\n",
        "                in each language.\n",
        "            path (str): Common prefix of the splits' file paths, or None to use\n",
        "                the result of cls.download(root).\n",
        "            root: Root dataset storage directory. Default is '.data'.\n",
        "            train: The prefix of the train data. Default: 'train'.\n",
        "            validation: The prefix of the validation data. Default: 'val'.\n",
        "            test: The prefix of the test data. Default: 'test'.\n",
        "            Remaining keyword arguments: Passed to the splits method of\n",
        "                Dataset.\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "train_data, valid_data, test_data = eng_msa_train_dataset.splits(path= 'data/', train='train/train_eng_msa', validation='val/val_eng_msa', test='test/test_eng_msa', exts=('.eng', '.msa'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0A4HJN4FEOdb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE =32\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device,\n",
        "    shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}