{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_1.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BQflUKhjIKNp"
      },
      "source": [
        "### Notes \n",
        "\n",
        "T5 Paper: https://arxiv.org/pdf/1910.10683.pdf\n",
        "\n",
        "T5 Tokenizer: https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_t5.py\n",
        "\n",
        "Important Tasks: https://docs.google.com/document/d/1weIZM6QTlnitpPQmpg-WeV2RW70TnYmDuogBQPr5mB0/edit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLOmiOta6MJp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "95ab9906-ca9e-4a4d-8609-5e6479deb589"
      },
      "source": [
        "#installation step\n",
        "!pip install transformers\n",
        "!pip install t5\n",
        "!pip install sentencepiece\n",
        "#creating the folders \n",
        "!mkdir data/\n",
        "!mkdir data/AD_NMT-master\n",
        "!mkdir data/train/\n",
        "!mkdir data/test/\n",
        "!mkdir data/val/\n",
        "!mkdir data/model/\n",
        "!mkdir data/config/\n",
        "#fetching the pkl files\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW' -O data/AD_NMT-master/english-Arabic-both.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV' -O data/AD_NMT-master/LAV-MSA-2-both.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UjDX7cCG2S23SPfSHxSPdVayMTxB5Y16' -O data/AD_NMT-master/Magribi_MSA-both.pkl\n",
        "# !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1fEVj9jCxvcKn9zg8lO43i2sWZquegg5H' -O data/operative_config.gin\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UGKswXSqHSxWpx57cEDzvNeJaqbAuyt8' -O data/padic.xml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\r\u001b[K     |▌                               | 10kB 25.6MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 3.0MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 4.0MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 4.3MB/s eta 0:00:01\r\u001b[K     |██▍                             | 51kB 3.3MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 3.8MB/s eta 0:00:01\r\u001b[K     |███▍                            | 71kB 4.3MB/s eta 0:00:01\r\u001b[K     |███▉                            | 81kB 4.5MB/s eta 0:00:01\r\u001b[K     |████▍                           | 92kB 4.8MB/s eta 0:00:01\r\u001b[K     |████▉                           | 102kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 112kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 122kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 133kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 143kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 153kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 163kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 174kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 184kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 194kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 204kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 215kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 225kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 235kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 245kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 256kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 266kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 276kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 286kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 296kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 307kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 317kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 327kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 337kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 348kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 358kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 368kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 378kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 389kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 399kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 409kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 419kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 430kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 440kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 450kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 460kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 471kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 481kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 491kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 501kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 512kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 522kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 532kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 542kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 552kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 563kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 573kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 583kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 593kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 604kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 614kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 624kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 634kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 645kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 655kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 665kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 675kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 12.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 29.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 61.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=7ba51dda89f783ca580390abcce9c5f990b6b7735ead222397f25cc91600e9f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n",
            "Collecting t5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/55/cf4b9ad68873d28224ac9ca78bd30332b53b1f9f6c564ce8d3cc5358a0a8/t5-0.6.0-py3-none-any.whl (149kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from t5) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from t5) (1.4.1)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.6/dist-packages (from t5) (0.3.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from t5) (0.1.91)\n",
            "Collecting tfds-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/10/5b2ca5e1743068a594c667b22cac96edaeb6b367af9e0cef78d76d5c7a2f/tfds_nightly-3.1.0.dev202006220106-py3-none-any.whl (3.4MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4MB 15.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from t5) (1.5.1+cu101)\n",
            "Collecting mesh-tensorflow[transformer]>=0.1.13\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/32/ceaf1549586ccfeaf7840a166876be358bd78ab2fdd152b17c5600ed7888/mesh_tensorflow-0.1.13-py3-none-any.whl (292kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 38.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: transformers>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from t5) (2.11.0)\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/4b/6c7a0b26a48d88f56573d11aa5058808fe0d36ba40951287894f943556b5/sacrebleu-1.4.10-py3-none-any.whl (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.6MB/s \n",
            "\u001b[?25hCollecting rouge-score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.6/dist-packages (from t5) (2.8.0)\n",
            "Collecting six>=1.14\n",
            "  Downloading https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from t5) (0.9.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from t5) (1.0.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from t5) (1.18.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from t5) (3.2.5)\n",
            "Collecting tensorflow-text\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/f4/3e3968e8a19e85bea8a0fdc9bd1f6a963b29cdecb1be984af0b70fbc0690/tensorflow_text-2.2.1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 45.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->t5) (0.15.1)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (0.22.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (1.12.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (0.16.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (4.41.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (3.10.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (0.3.2)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (19.3.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (2.23.0)\n",
            "Requirement already satisfied: tensorflow-datasets; extra == \"transformer\" in /usr/local/lib/python3.6/dist-packages (from mesh-tensorflow[transformer]>=0.1.13->t5) (2.1.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5) (20.4)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5) (0.7)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/53/84/7b3146ec6378d28abc73ab484f09f47dfa008ad6f03f33d90a369f880e25/portalocker-1.7.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.6/dist-packages (from babel->t5) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->t5) (2.8.1)\n",
            "Requirement already satisfied: tensorflow<2.3,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-text->t5) (2.2.0)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tfds-nightly->t5) (1.52.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tfds-nightly->t5) (47.3.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly->t5) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly->t5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly->t5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly->t5) (2020.4.5.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.7.0->t5) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=2.7.0->t5) (2.4.7)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (3.2.1)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (2.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (1.29.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (2.2.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (0.2.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (2.10.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (0.34.2)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (1.6.3)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (0.3.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (0.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (1.6.0.post3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (3.2.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (1.17.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (1.6.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (4.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (0.4.8)\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.12.0, but you'll have six 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: six, tfds-nightly, mesh-tensorflow, portalocker, sacrebleu, rouge-score, tensorflow-text, t5\n",
            "  Found existing installation: six 1.12.0\n",
            "    Uninstalling six-1.12.0:\n",
            "      Successfully uninstalled six-1.12.0\n",
            "Successfully installed mesh-tensorflow-0.1.13 portalocker-1.7.0 rouge-score-0.0.4 sacrebleu-1.4.10 six-1.15.0 t5-0.6.0 tensorflow-text-2.2.1 tfds-nightly-3.1.0.dev202006220106\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "six"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.91)\n",
            "--2020-06-23 03:20:11--  https://docs.google.com/uc?export=download&id=1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.20.101, 74.125.20.113, 74.125.20.102, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.20.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-10-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/2dbod1808at06185u6sk3nvqbur4ssug/1592882400000/16970776037313924126/*/1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-06-23 03:20:12--  https://doc-10-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/2dbod1808at06185u6sk3nvqbur4ssug/1592882400000/16970776037313924126/*/1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW?e=download\n",
            "Resolving doc-10-2s-docs.googleusercontent.com (doc-10-2s-docs.googleusercontent.com)... 74.125.142.132, 2607:f8b0:400e:c08::84\n",
            "Connecting to doc-10-2s-docs.googleusercontent.com (doc-10-2s-docs.googleusercontent.com)|74.125.142.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 955428 (933K) [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/english-Arabic-both.pkl’\n",
            "\n",
            "data/AD_NMT-master/ 100%[===================>] 933.04K  --.-KB/s    in 0.006s  \n",
            "\n",
            "2020-06-23 03:20:12 (161 MB/s) - ‘data/AD_NMT-master/english-Arabic-both.pkl’ saved [955428/955428]\n",
            "\n",
            "--2020-06-23 03:20:13--  https://docs.google.com/uc?export=download&id=1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.20.101, 74.125.20.113, 74.125.20.102, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.20.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-00-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/u5ebnlq2go36uegh43lsfa3g4s3n3r0j/1592882400000/16970776037313924126/*/1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-06-23 03:20:14--  https://doc-00-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/u5ebnlq2go36uegh43lsfa3g4s3n3r0j/1592882400000/16970776037313924126/*/1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV?e=download\n",
            "Resolving doc-00-2s-docs.googleusercontent.com (doc-00-2s-docs.googleusercontent.com)... 74.125.142.132, 2607:f8b0:400e:c08::84\n",
            "Connecting to doc-00-2s-docs.googleusercontent.com (doc-00-2s-docs.googleusercontent.com)|74.125.142.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/LAV-MSA-2-both.pkl’\n",
            "\n",
            "data/AD_NMT-master/     [ <=>                ]   2.33M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-06-23 03:20:14 (208 MB/s) - ‘data/AD_NMT-master/LAV-MSA-2-both.pkl’ saved [2447014]\n",
            "\n",
            "--2020-06-23 03:20:15--  https://docs.google.com/uc?export=download&id=1UjDX7cCG2S23SPfSHxSPdVayMTxB5Y16\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.195.100, 74.125.195.138, 74.125.195.102, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.195.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-04-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/0g6jutn85na56qstaigm4ghrr21m5fo3/1592882400000/16970776037313924126/*/1UjDX7cCG2S23SPfSHxSPdVayMTxB5Y16?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-06-23 03:20:16--  https://doc-04-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/0g6jutn85na56qstaigm4ghrr21m5fo3/1592882400000/16970776037313924126/*/1UjDX7cCG2S23SPfSHxSPdVayMTxB5Y16?e=download\n",
            "Resolving doc-04-2s-docs.googleusercontent.com (doc-04-2s-docs.googleusercontent.com)... 74.125.142.132, 2607:f8b0:400e:c08::84\n",
            "Connecting to doc-04-2s-docs.googleusercontent.com (doc-04-2s-docs.googleusercontent.com)|74.125.142.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/Magribi_MSA-both.pkl’\n",
            "\n",
            "data/AD_NMT-master/     [ <=>                ]   2.81M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2020-06-23 03:20:16 (168 MB/s) - ‘data/AD_NMT-master/Magribi_MSA-both.pkl’ saved [2944107]\n",
            "\n",
            "--2020-06-23 03:20:17--  https://docs.google.com/uc?export=download&id=1UGKswXSqHSxWpx57cEDzvNeJaqbAuyt8\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.195.100, 74.125.195.113, 74.125.195.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.195.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-04-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/u9eka94hfn27ofk546k7d27av6aorolk/1592882400000/16970776037313924126/*/1UGKswXSqHSxWpx57cEDzvNeJaqbAuyt8?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-06-23 03:20:18--  https://doc-04-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/u9eka94hfn27ofk546k7d27av6aorolk/1592882400000/16970776037313924126/*/1UGKswXSqHSxWpx57cEDzvNeJaqbAuyt8?e=download\n",
            "Resolving doc-04-2s-docs.googleusercontent.com (doc-04-2s-docs.googleusercontent.com)... 74.125.142.132, 2607:f8b0:400e:c08::84\n",
            "Connecting to doc-04-2s-docs.googleusercontent.com (doc-04-2s-docs.googleusercontent.com)|74.125.142.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/xml]\n",
            "Saving to: ‘data/padic.xml’\n",
            "\n",
            "data/padic.xml          [ <=>                ]   2.97M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-06-23 03:20:18 (273 MB/s) - ‘data/padic.xml’ saved [3114143]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-WzhRv4mIKNq",
        "colab": {}
      },
      "source": [
        "#James Chartouni\n",
        "#Joey Park\n",
        "#Raef Khan\n",
        "\n",
        "import torch\n",
        "from torch.optim import SGD\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os, io, glob\n",
        "import functools\n",
        "\n",
        "import sentencepiece as spm\n",
        "\n",
        "import transformers\n",
        "import t5\n",
        "from t5.data import preprocessors\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "irQQ9E_7IKOM"
      },
      "source": [
        "## Prepare Datasets\n",
        "\n",
        "We need to take our training and test sets from the pkl files and create new .txt files that are formatted so that the standard torchtext Dataset class can read them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fctRFLVvs3X2",
        "colab_type": "text"
      },
      "source": [
        "### PADIC Dataset Parsing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4_RfmVTUuze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "padic_tree = ET.parse('data/padic.xml')\n",
        "\n",
        "padic_alg_msa = []\n",
        "padic_ann_msa = []\n",
        "padic_syr_msa = []\n",
        "padic_pal_msa = []\n",
        "padic_mor_msa = [] \n",
        "\n",
        "for sentence in padic_tree.getroot():\n",
        "  padic_alg_msa.append([sentence.find('ALGIERS').text.strip(), sentence.find('MODERN-STANDARD-ARABIC').text.strip()])\n",
        "  padic_ann_msa.append([sentence.find('ANNABA').text.strip(), sentence.find('MODERN-STANDARD-ARABIC').text.strip()])\n",
        "  padic_syr_msa.append([sentence.find('SYRIAN').text.strip(), sentence.find('MODERN-STANDARD-ARABIC').text.strip()])\n",
        "  padic_pal_msa.append([sentence.find('PALESTINIAN').text.strip(), sentence.find('MODERN-STANDARD-ARABIC').text.strip()])\n",
        "  padic_mor_msa.append([sentence.find('MOROCCAN').text.strip(), sentence.find('MODERN-STANDARD-ARABIC').text.strip()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iL68WL8LEtTF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "355356bf-3904-40e2-891b-203f90b6d760"
      },
      "source": [
        "print(padic_alg_msa[0])\n",
        "print(padic_ann_msa[0])\n",
        "print(padic_syr_msa[0])\n",
        "print(padic_pal_msa[0])\n",
        "print(padic_mor_msa[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['EAdw AlnAs ytbAkAw bdyt nhdr mn qlby tqwl nhdy fAlnAs', \"tEAlt >SwAt AlnAs bAlbkA'،  bd>t >tHdv bAnfEAl w k>nny >hdy fy AlnAs\"]\n",
            "['EAdwA AlnAs ytbAkAw bdyt nhdr bg$ w qwl ElyA nhdy fy AlnAs', \"tEAlt >SwAt AlnAs bAlbkA'،  bd>t >tHdv bAnfEAl w k>nny >hdy fy AlnAs\"]\n",
            "['Ely Swt AlnAs bAlbky w bl~$t >Hky bESbyp w k>ny Em Ahdy bAlnAs', \"tEAlt >SwAt AlnAs bAlbkA'،  bd>t >tHdv bAnfEAl w k>nny >hdy fy AlnAs\"]\n",
            "['SArwA AlnAs ySyHwA bSwt EAly wbdyt AHky wAnA mnfEl wk>ny bhdy fy AlnAs', \"tEAlt >SwAt AlnAs bAlbkA'،  bd>t >tHdv bAnfEAl w k>nny >hdy fy AlnAs\"]\n",
            "['nAs bdAw tytbAkAw wbdyt tnhdr b nfEl bHAl <lY tnhdy AlnAs', \"tEAlt >SwAt AlnAs bAlbkA'،  bd>t >tHdv bAnfEAl w k>nny >hdy fy AlnAs\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSfrbD9pGrZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alg_msa_train, alg_msa_val = train_test_split(padic_alg_msa, test_size=.15)\n",
        "ann_msa_train, ann_msa_val = train_test_split(padic_ann_msa, test_size=.15)\n",
        "syr_msa_train, syr_msa_val = train_test_split(padic_syr_msa, test_size=.15)\n",
        "pal_msa_train, pal_msa_val = train_test_split(padic_pal_msa, test_size=.15)\n",
        "mor_msa_train, mor_msa_val = train_test_split(padic_mor_msa, test_size=.15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5uDm27ZHOEi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "3913a09f-5599-4709-88ef-fac72af138a7"
      },
      "source": [
        "#all the translations have equal amt. of examples\n",
        "print(len(alg_msa_train))\n",
        "print(len(alg_msa_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6131\n",
            "1082\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxCLgMZ2ns4b",
        "colab_type": "text"
      },
      "source": [
        "###Initial Loading from Pickle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aha0xureIKNw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "44c3cfcc-393f-4d63-8c4f-f5b55e724517"
      },
      "source": [
        "ls data/AD_NMT-master"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "english-Arabic-both.pkl  LAV-MSA-2-both.pkl  Magribi_MSA-both.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XWKnGRlLIKN9",
        "colab": {}
      },
      "source": [
        "file_path = 'data/AD_NMT-master/'\n",
        "\n",
        "with open(file_path + \"english-Arabic-both.pkl\", 'rb') as handle:\n",
        "    data_English_MSA_both = pickle.load(handle) \n",
        "\n",
        "with open(file_path + \"LAV-MSA-2-both.pkl\", 'rb') as handle:\n",
        "    data_LAV_MSA_both = pickle.load(handle) \n",
        "\n",
        "with open(file_path + \"Magribi_MSA-both.pkl\", 'rb') as handle:\n",
        "    data_Magribi_MSA_both = pickle.load(handle) \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ch3APJadIKOH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "763de6de-2a74-4a0b-9850-17507c87c340"
      },
      "source": [
        "#few dataset examples\n",
        "print(data_English_MSA_both[0:5])\n",
        "print(data_English_MSA_both[-5:])\n",
        "print(data_LAV_MSA_both[0:5])\n",
        "print(data_Magribi_MSA_both[0:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Tom was also there', 'كان توم هنا ايضا'], ['That old woman lives by herself', 'تلك المراة العجوز تسكن بمفردها'], ['He went abroad for the purpose of studying English', 'سافر خارج البلد ليتعلم الانجليزية'], ['There is a fork missing', 'هناك شوكة ناقصة'], [\"I don't know this game\", 'لا اعرف هذه اللعبة']]\n",
            "[['Please send us more information', 'ارسل الينا المزيد من المعلومات اذا تكرمت'], ['I am an only child', 'انا طفل وحيد ابي و امي'], ['Make good use of your time', 'استفد من وقتك جيدا'], [\"Fighting won't settle anything\", 'لن يحل القتال اي شيء'], ['Practice makes perfect', 'الممارسة هي الطريق الى الاتقان']]\n",
            "[['لا انا بعرف وحدة راحت ع فرنسا و معا شنتا حطت فيها الفرش', 'لا اعرف واحدة ذهبت الى فرنسا و لها غرفة و ضعت فيها الافرشة'], ['روح بوشك و فتول عاليسار', 'اذهب تقدم و استدر يسارا'], ['لا لا لازم انه يكون عندك موضوع ما في اشي', ' لا لا يجب ان يكون لديك موضوع هذا ضروري'], ['اوعي تبعدي من هون بلاش تضيعي ', 'لا تبتعد عن هنا حتى لا تفقد الطريق '], ['قصدي صراحة يما انا كمان كرهته من يوم ما عملتيه زي ما بتعمله خالتي كرهته و صرت ما باطيقه بالمرة', 'اقصد صراحة يا امي انا ايضا كرهته من يوم حضرته مثلما تحضره خالتي كرهته و اصبحت لا اطيقه ابدا']]\n",
            "[['يا ربي متخليش حتى لبيوتا ديالهم يوصلو ل البارة', 'يارب لا تدع اهدافهم تصيب حتى العارضة'], ['يعطيك الصحة كريمة', 'يعطيك العافية كريمة'], [' لوكان جوزوزه ساعة و نص و يهنيونا ', 'لو انهم يبثونه ساعة و نصف و يريحوننا'], ['ولا عيط لك واحد و قالك راه فلان رايح يضربك غدوة بكف ', 'او انه قال لك بان فلان سيصفعك غدا'], ['عبرتي انا عقدتها مرة وحدة', 'احسنت عقدتها مرة واحدة']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lp7ncVY6wqET",
        "colab": {}
      },
      "source": [
        "#splits the train dataset into train and validation sets, define test set as datafile\n",
        "en_msa_train, en_msa_val = train_test_split(data_English_MSA_both, test_size=.2)\n",
        "\n",
        "lav_msa_train, lav_msa_val = train_test_split(data_LAV_MSA_both, test_size=.2)\n",
        "\n",
        "mag_msa_train, mag_msa_val = train_test_split(data_Magribi_MSA_both, test_size=.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9Ue2fSILwqEW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "417a09d9-b096-4bda-9c0d-0a4454e227da"
      },
      "source": [
        "print(len(en_msa_train))\n",
        "print(len(en_msa_val))\n",
        "\n",
        "print(len(lav_msa_train))\n",
        "print(len(lav_msa_val))\n",
        "\n",
        "print(len(mag_msa_train))\n",
        "print(len(mag_msa_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8000\n",
            "2001\n",
            "12644\n",
            "3161\n",
            "15788\n",
            "3948\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W3bWU46TIKOb",
        "colab": {}
      },
      "source": [
        "file_path = 'data/'\n",
        "\n",
        "def list_to_csv(ds, src='en', trg='msa', datatype=''):\n",
        "    src_formatted = datatype + '_' + src + '_' + trg + '.' + 'csv'\n",
        "    \n",
        "    with open(file_path + datatype + \"/\" + src_formatted, 'wt') as csv:\n",
        "        for i, arr in enumerate(ds):\n",
        "            csv.write(arr[1] + ',' + arr[0] + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AqwDB9BuKEnV",
        "colab": {}
      },
      "source": [
        "list_to_csv(en_msa_train, 'en', 'msa', 'train')\n",
        "list_to_csv(en_msa_val, 'en', 'msa', 'val')\n",
        "\n",
        "list_to_csv(lav_msa_train, 'lav', 'msa', 'train')\n",
        "list_to_csv(lav_msa_val, 'lav', 'msa', 'val')\n",
        "\n",
        "list_to_csv(mag_msa_train, 'mag', 'msa', 'train')\n",
        "list_to_csv(mag_msa_val, 'mag', 'msa', 'val')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGqh46GhVHfI",
        "colab_type": "text"
      },
      "source": [
        "## Training SentencePiece Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lcM0dLclt6J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#combine all the training lines of all three languages\n",
        "spm_input_ds = en_msa_train + mag_msa_train + lav_msa_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBZ5iR21etNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def list_to_input(ds):\n",
        "    src_formatted = 'spm_input' + '.' + 'txt'\n",
        "\n",
        "    with open(file_path + \"/\" + src_formatted, 'wt') as sentencelinefile:\n",
        "        for i, arr in enumerate(ds):\n",
        "            sentencelinefile.write(arr[0] + '\\n' + arr[1] + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxDKpM1H7vaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_to_input(spm_input_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3YrvcR8anQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = 32128\n",
        "spm.SentencePieceTrainer.train('--input=data/spm_input.txt --model_prefix=data/model/spm --vocab_size=' + str(VOCAB_SIZE) + ' --unk_id=2 --bos_id=-1 --eos_id=1 --pad_id=0 --hard_vocab_limit=False')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzB-_AeUUNLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath = 'data/model/spm.model'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrHAuzNW1eIh",
        "colab_type": "text"
      },
      "source": [
        "##Tensor Processing + Add to TaskRegistry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oVVzdeAZd0x",
        "colab_type": "text"
      },
      "source": [
        "### English to Arabic Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWnZcYI_491p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "en_msa_split_csv_path = {\n",
        "    \"train\": \"data/train/train_en_msa.csv\",\n",
        "    \"validation\": \"data/val/val_en_msa.csv\"\n",
        "}\n",
        "en_msa_example_count = {\n",
        "    \"train\": len(en_msa_train),\n",
        "    \"validation\": len(en_msa_val)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2e3cM_wP_M8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "da24f9ec-d450-45b3-995c-873fa6c3b8f8"
      },
      "source": [
        "def en_msa_translation_dataset_fn(split, shuffle_files=False):\n",
        "  ds = tf.data.TextLineDataset(en_msa_split_csv_path[split])\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"\",\"\"],\n",
        "                        field_delim=\",\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "  )\n",
        "  ds = ds.map(lambda *example: dict(zip([\"source\", \"target\"], example)) )\n",
        "  return ds\n",
        "\n",
        "for example in tfds.as_numpy(en_msa_translation_dataset_fn(\"train\").take(5)):\n",
        "    print(example)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'source': b'\\xd8\\xa7\\xd9\\x86\\xd8\\xaa \\xd9\\x84\\xd9\\x85 \\xd8\\xaa\\xd8\\xba\\xd8\\xb3\\xd9\\x84 \\xd9\\x8a\\xd8\\xaf\\xd9\\x8a\\xd9\\x83 \\xd8\\xa8\\xd8\\xb9\\xd8\\xaf \\xd9\\x87\\xd9\\x84 \\xd9\\x81\\xd8\\xb9\\xd9\\x84\\xd8\\xaa\\xd8\\x9f', 'target': b\"You haven't washed your hands yet have you?\"}\n",
            "{'source': b'\\xd9\\x88\\xd8\\xac\\xd8\\xaf\\xd9\\x86\\xd8\\xa7\\xd9\\x87\\xd8\\xa7 \\xd8\\xb9\\xd9\\x84\\xd9\\x89 \\xd9\\x82\\xd9\\x8a\\xd8\\xaf \\xd8\\xa7\\xd9\\x84\\xd8\\xad\\xd9\\x8a\\xd8\\xa7\\xd8\\xa9', 'target': b'We found her alive'}\n",
            "{'source': b'\\xd8\\xa7\\xd9\\x88\\xd8\\xaf \\xd8\\xa7\\xd9\\x86 \\xd8\\xaa\\xd9\\x82\\xd8\\xa7\\xd8\\xa8\\xd9\\x84 \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd8\\xaf\\xd8\\xa7\\xd9\\x8a', 'target': b'I would like you to meet my parents'}\n",
            "{'source': b'\\xd8\\xaa\\xd8\\xad\\xd8\\xa8 \\xd8\\xa7\\xd9\\x84\\xd8\\xa8\\xd8\\xb1\\xd8\\xaa\\xd9\\x82\\xd8\\xa7\\xd9\\x84', 'target': b'She likes oranges'}\n",
            "{'source': b'\\xd9\\x83\\xd9\\x84\\xd8\\xa7\\xd9\\x85\\xd9\\x83 \\xd9\\x8a\\xd8\\xb0\\xd9\\x83\\xd8\\xb1\\xd9\\x86\\xd9\\x8a \\xd8\\xa8\\xd8\\xb2\\xd9\\x88\\xd8\\xac\\xd8\\xaa\\xd9\\x8a', 'target': b'You sound like my wife'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1GXED2IO3pE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#turn the ds of dictionaries and change the keys to inputs and targets that the model\n",
        "def en_msa_translation_preprocessor(ds):\n",
        "  def to_inputs_and_targets(ex):\n",
        "    return{\n",
        "        \"inputs\": tf.strings.join([\"translate English to MSA: \",ex[\"source\"]]),\n",
        "        \"targets\": ex[\"target\"]\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-W6j-SDXcNT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t5.data.TaskRegistry.remove(\"translation_en_msa\")\n",
        "t5.data.TaskRegistry.add(\n",
        "    #name of the Task\n",
        "    \"translation_en_msa\",\n",
        "    #Supply a function which returns a tf.data.Dataset\n",
        "    dataset_fn=en_msa_translation_dataset_fn,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[en_msa_translation_preprocessor],\n",
        "    # Lowercase targets before computing metrics.\n",
        "\n",
        "    #postprocess_fn=t5.data.postprocessors.lower_text, \n",
        "\n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "\n",
        "    #metric_fns=[t5.evaluation.metrics.bleu],\n",
        "\n",
        "    # Not required, but helps for mixing and auto-caching.\n",
        "    num_input_examples=en_msa_example_count,\n",
        "    # output_features\n",
        "    output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(filepath)),\n",
        "    # specifying token processor\n",
        "    token_preprocessor=[\n",
        "      functools.partial(\n",
        "          preprocessors.select_random_chunk,\n",
        "          feature_key=\"targets\",\n",
        "          max_length=65536\n",
        "      ),\n",
        "      functools.partial(\n",
        "          preprocessors.reduce_concat_tokens,\n",
        "          feature_key=\"targets\",\n",
        "          batch_size=128\n",
        "      ),\n",
        "      preprocessors.split_tokens_to_inputs_length,\n",
        "      functools.partial(\n",
        "          preprocessors.denoise,\n",
        "          inputs_fn=preprocessors.noise_span_to_unique_sentinel,\n",
        "          targets_fn=preprocessors.nonnoise_span_to_unique_sentinel,\n",
        "          noise_density=0.15,\n",
        "          noise_mask_fn=preprocessors.iid_noise_mask,\n",
        "      )\n",
        "    ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGr8wt7naDRL",
        "colab_type": "text"
      },
      "source": [
        "###Levantine to MSA Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e3A3wQdjagH6",
        "colab": {}
      },
      "source": [
        "lav_msa_split_csv_path = {\n",
        "    \"train\": \"data/train/train_lav_msa.csv\",\n",
        "    \"validation\": \"data/val/val_lav_msa.csv\"\n",
        "}\n",
        "lav_msa_example_count = {\n",
        "    \"train\": len(lav_msa_train),\n",
        "    \"validation\": len(lav_msa_val)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xCK_j-9BagH_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "59594b4e-378d-46d3-c821-2f3e97219a19"
      },
      "source": [
        "def lav_msa_translation_dataset_fn(split, shuffle_files=False):\n",
        "  ds = tf.data.TextLineDataset(lav_msa_split_csv_path[split])\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"\",\"\"],\n",
        "                        field_delim=\",\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "  )\n",
        "  ds = ds.map(lambda *example: dict(zip([\"source\", \"target\"], example)) )\n",
        "  return ds\n",
        "\n",
        "for example in tfds.as_numpy(lav_msa_translation_dataset_fn(\"train\").take(5)):\n",
        "    print(example)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'source': b'\\xd8\\xa8\\xd9\\x82\\xd9\\x8a\\xd8\\xaa \\xd8\\xaf\\xd9\\x88\\xd9\\x86 \\xd8\\xb1\\xd8\\xac\\xd9\\x84', 'target': b'\\xd8\\xb6\\xd9\\x84\\xd8\\xaa \\xd8\\xa8\\xd8\\xaf\\xd9\\x88\\xd9\\x86 \\xd8\\xb1\\xd8\\xac\\xd9\\x84'}\n",
            "{'source': b'\\xd9\\x84\\xd9\\x85 \\xd8\\xa7\\xd8\\xb3\\xd8\\xaa\\xd8\\xb7\\xd8\\xb9 \\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd9\\x88\\xd8\\xaf\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd9\\x89 \\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd8\\xac\\xd8\\xaa\\xd9\\x85\\xd8\\xa7\\xd8\\xb9  \\xd8\\xa7\\xd9\\x82\\xd8\\xb7\\xd8\\xb9\\xd9\\x88\\xd8\\xa7 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd9\\x83\\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xa9 \\xd8\\xab\\xd9\\x85 \\xd8\\xa7\\xd8\\xb9\\xd9\\x8a\\xd8\\xaf\\xd9\\x88\\xd8\\xa7 \\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd8\\xaa\\xd8\\xb5\\xd8\\xa7\\xd9\\x84 \\xd8\\xa8\\xd9\\x8a', 'target': b'\\xd9\\x85\\xd8\\xa7 \\xd9\\x82\\xd8\\xaf\\xd8\\xb1\\xd8\\xaa\\xd8\\xb4 \\xd8\\xa7\\xd8\\xb1\\xd8\\xac\\xd8\\xb9 \\xd9\\x84\\xd9\\x84\\xd8\\xa7\\xd8\\xac\\xd8\\xaa\\xd9\\x85\\xd8\\xa7\\xd8\\xb9  \\xd8\\xa7\\xd9\\x82\\xd8\\xb7\\xd8\\xb9\\xd9\\x88\\xd8\\xa7 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd9\\x83\\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xa9 \\xd8\\xa8\\xd8\\xb9\\xd8\\xaf\\xd9\\x8a\\xd9\\x86 \\xd8\\xb9\\xd9\\x8a\\xd8\\xaf\\xd9\\x88\\xd8\\xa7 \\xd8\\xa7\\xd8\\xaa\\xd8\\xb5\\xd9\\x84\\xd9\\x88\\xd8\\xa7 \\xd9\\x81\\xd9\\x8a\\xd8\\xa7'}\n",
            "{'source': b'\\xd8\\xa7\\xd9\\x86 \\xd8\\xaf\\xd8\\xae\\xd9\\x84\\xd8\\xaa\\xd9\\x87\\xd8\\xa7 \\xd8\\xaa\\xd8\\xb4\\xd8\\xb9\\xd8\\xb1 \\xd9\\x88 \\xd9\\x83\\xd8\\xa7\\xd9\\x86\\xd9\\x87\\xd8\\xa7 \\xd9\\x85\\xd8\\xad\\xd9\\x85\\xd9\\x8a\\xd9\\x87 \\xd8\\xb7\\xd8\\xa8\\xd9\\x8a\\xd8\\xb9\\xd9\\x87 \\xd8\\xad\\xd9\\x8a\\xd8\\xab \\xd8\\xa8\\xd9\\x87\\xd8\\xa7 \\xd9\\x85\\xd9\\x86\\xd8\\xa7\\xd8\\xb8\\xd8\\xb1 \\xd8\\xb7\\xd8\\xa8\\xd9\\x8a\\xd8\\xb9\\xd9\\x8a\\xd9\\x87 \\xd9\\x83\\xd8\\xab\\xd9\\x8a\\xd8\\xb1\\xd9\\x87 ', 'target': b'\\xd8\\xa7\\xd8\\xb0\\xd8\\xa7 \\xd8\\xaf\\xd8\\xae\\xd9\\x84\\xd8\\xaa\\xd9\\x87\\xd8\\xa7 \\xd8\\xa8\\xd8\\xaa\\xd8\\xad\\xd8\\xb3 \\xd8\\xad\\xd8\\xa7\\xd9\\x84\\xd9\\x83 \\xd8\\xa8\\xd9\\x85\\xd8\\xad\\xd9\\x85\\xd9\\x8a\\xd8\\xa9 \\xd8\\xb7\\xd8\\xa8\\xd9\\x8a\\xd8\\xb9\\xd9\\x8a\\xd8\\xa9 \\xd9\\x85\\xd8\\xad\\xd8\\xaa\\xd8\\xad\\xd9\\x81 \\xd9\\x81\\xd9\\x8a \\xd9\\x85\\xd9\\x86\\xd8\\xa7\\xd8\\xb8\\xd8\\xb1 \\xd8\\xb7\\xd8\\xa8\\xd9\\x8a\\xd8\\xb9\\xd9\\x8a\\xd8\\xa9 \\xd9\\x83\\xd8\\xaa\\xd9\\x8a\\xd8\\xb1 '}\n",
            "{'source': b'\\xd8\\xb9\\xd8\\xa7\\xd8\\xa6\\xd9\\x84\\xd8\\xa9  \\xd8\\xb9\\xd9\\x85\\xd9\\x8a \\xd9\\x83\\xd8\\xa7\\xd9\\x86\\xd8\\xaa \\xd9\\x87\\xd9\\x86\\xd8\\xa7  \\xd8\\xae\\xd8\\xb1\\xd8\\xac\\xd9\\x88\\xd8\\xa7 \\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd9\\x86 \\xd9\\x81\\xd9\\x82\\xd8\\xb7', 'target': b'\\xd8\\xaf\\xd8\\xa7\\xd8\\xb1 \\xd8\\xb9\\xd9\\x85\\xd9\\x8a \\xd9\\x83\\xd8\\xa7\\xd9\\x86\\xd9\\x88 \\xd9\\x87\\xd8\\xa7\\xd9\\x86 \\xd8\\xb7\\xd9\\x84\\xd8\\xb9\\xd9\\x88\\xd8\\xa7 \\xd9\\x87\\xd9\\x84\\xd9\\x82\\xd9\\x8a\\xd8\\xaa \\xd8\\xa8\\xd8\\xb3'}\n",
            "{'source': b'\\xd8\\xaa\\xd8\\xb5\\xd8\\xa8\\xd8\\xad\\xd9\\x88\\xd9\\x86 \\xd8\\xb9\\xd9\\x84\\xd9\\x89 \\xd8\\xae\\xd9\\x8a\\xd8\\xb1 \\xd8\\xa7\\xd9\\x86 \\xd8\\xb4\\xd8\\xa7\\xd8\\xa1 \\xd8\\xa7\\xd9\\x84\\xd9\\x84\\xd9\\x87', 'target': b'\\xd8\\xaa\\xd8\\xb5\\xd8\\xa8\\xd8\\xad\\xd9\\x88\\xd8\\xa7 \\xd8\\xb9\\xd9\\x84\\xd9\\x89 \\xd8\\xae\\xd9\\x8a\\xd8\\xb1 \\xd8\\xa7\\xd9\\x86\\xd8\\xb4\\xd8\\xa7\\xd8\\xa1 \\xd8\\xa7\\xd9\\x84\\xd9\\x84\\xd9\\x87'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fW9eNYoGagID",
        "colab": {}
      },
      "source": [
        "#turn the ds of dictionaries and change the keys to inputs and targets that the model\n",
        "def lav_msa_translation_preprocessor(ds):\n",
        "  def to_inputs_and_targets(ex):\n",
        "    return{\n",
        "        \"inputs\": tf.strings.join([\"translate Levantine to MSA: \",ex[\"source\"]]),\n",
        "        \"targets\": ex[\"target\"]\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mDoUsP1AagIG",
        "colab": {}
      },
      "source": [
        "t5.data.TaskRegistry.remove(\"translation_lav_msa\")\n",
        "t5.data.TaskRegistry.add(\n",
        "    #name of the Task\n",
        "    \"translation_lav_msa\",\n",
        "    #Supply a function which returns a tf.data.Dataset\n",
        "    dataset_fn=lav_msa_translation_dataset_fn,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[lav_msa_translation_preprocessor],\n",
        "    # Lowercase targets before computing metrics.\n",
        "\n",
        "    #postprocess_fn = t5.data.postprocessors.lower_text, \n",
        "\n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "\n",
        "    #metric_fns=[t5.evaluation.metrics.bleu],\n",
        "\n",
        "    # Not required, but helps for mixing and auto-caching.\n",
        "    num_input_examples=lav_msa_example_count,\n",
        "    # output_features\n",
        "    output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(filepath)),\n",
        "    # specifying token processor\n",
        "    token_preprocessor=[\n",
        "      functools.partial(\n",
        "          preprocessors.select_random_chunk,\n",
        "          feature_key=\"targets\",\n",
        "          max_length=65536\n",
        "      ),\n",
        "      functools.partial(\n",
        "          preprocessors.reduce_concat_tokens,\n",
        "          feature_key=\"targets\",\n",
        "          batch_size=128\n",
        "      ),\n",
        "      preprocessors.split_tokens_to_inputs_length,\n",
        "      functools.partial(\n",
        "          preprocessors.denoise,\n",
        "          inputs_fn=preprocessors.noise_span_to_unique_sentinel,\n",
        "          targets_fn=preprocessors.nonnoise_span_to_unique_sentinel,\n",
        "          noise_density=0.15,\n",
        "          noise_mask_fn=preprocessors.iid_noise_mask,\n",
        "      )\n",
        "    ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wIK8EWyaLFR",
        "colab_type": "text"
      },
      "source": [
        "###Maghrib to MSA Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bVhYmQnQaj2K",
        "colab": {}
      },
      "source": [
        "mag_msa_split_csv_path = {\n",
        "    \"train\": \"data/train/train_mag_msa.csv\",\n",
        "    \"validation\": \"data/val/val_mag_msa.csv\"\n",
        "}\n",
        "mag_msa_example_count = {\n",
        "    \"train\": len(mag_msa_train),\n",
        "    \"validation\": len(mag_msa_val)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WBLF-J2Oaj2T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "outputId": "4329fe58-01d4-4757-fe39-e5a138970e54"
      },
      "source": [
        "def mag_msa_translation_dataset_fn(split, shuffle_files=False):\n",
        "  ds = tf.data.TextLineDataset(mag_msa_split_csv_path[split])\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"\",\"\"],\n",
        "                        field_delim=\",\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "  )\n",
        "  ds = ds.map(lambda *example: dict(zip([\"source\", \"target\"], example)) )\n",
        "  return ds\n",
        "\n",
        "for example in tfds.as_numpy(mag_msa_translation_dataset_fn(\"train\").take(5)):\n",
        "    print(example)\n",
        "    print(example['source'].decode())\n",
        "    print(example['target'].decode())\n",
        "    print(len(example['source']))\n",
        "    print(len(example['target']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'source': b'\\xd8\\xa7\\xd8\\xae\\xd8\\xaa\\xd9\\x84\\xd8\\xb7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd9\\x88\\xd8\\xb6\\xd8\\xa7\\xd8\\xb9 \\xd9\\x81\\xd9\\x8a \\xd9\\x85\\xd8\\xb5\\xd8\\xb1 \\xd8\\xa7\\xd9\\x8a\\xd8\\xb6\\xd8\\xa7 \\xd9\\x82\\xd8\\xa7\\xd9\\x84\\xd9\\x88\\xd8\\xa7 \\xd8\\xa7\\xd9\\x86 \\xd9\\x87\\xd8\\xb0\\xd9\\x87 \\xd8\\xa7\\xd9\\x84\\xd8\\xad\\xd9\\x83\\xd9\\x88\\xd9\\x85\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xac\\xd8\\xaf\\xd9\\x8a\\xd8\\xaf\\xd8\\xa9 \\xd8\\xa7\\xd9\\x83\\xd9\\x85\\xd9\\x84\\xd8\\xaa \\xd9\\x85\\xd8\\xb4\\xd9\\x88\\xd8\\xa7\\xd8\\xb1 \\xd9\\x85\\xd8\\xa8\\xd8\\xa7\\xd8\\xb1\\xd9\\x83', 'target': b'\\xd8\\xaa\\xd8\\xae\\xd9\\x84\\xd8\\xb7\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd9\\x88\\xd8\\xb6\\xd8\\xa7\\xd8\\xb9 \\xd9\\x81 \\xd9\\x85\\xd8\\xb5\\xd8\\xb1 \\xd8\\xad\\xd8\\xaa\\xd9\\x89 \\xd9\\x87\\xd9\\x8a \\xd9\\x82\\xd8\\xa7\\xd9\\x84\\xd9\\x88 \\xd8\\xa8\\xd9\\x84\\xd9\\x8a \\xd9\\x87\\xd8\\xa7\\xd8\\xaf \\xd8\\xa7\\xd9\\x84\\xd8\\xad\\xd9\\x83\\xd9\\x88\\xd9\\x85\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xac\\xd8\\xaf\\xd9\\x8a\\xd8\\xaf\\xd8\\xa9 \\xd9\\x83\\xd9\\x85\\xd9\\x84\\xd8\\xa7\\xd8\\xaa \\xd9\\x85\\xd8\\xb4\\xd8\\xb1\\xd9\\x88\\xd9\\x88\\xd8\\xb9 \\xd9\\x85\\xd8\\xa8\\xd8\\xa7\\xd8\\xb1\\xd9\\x83'}\n",
            "اختلطت الاوضاع في مصر ايضا قالوا ان هذه الحكومة الجديدة اكملت مشوار مبارك\n",
            "تخلطات الاوضاع ف مصر حتى هي قالو بلي هاد الحكومة الجديدة كملات مشرووع مبارك\n",
            "134\n",
            "137\n",
            "{'source': b'\\xd9\\x84\\xd9\\x88 \\xd9\\x83\\xd9\\x86\\xd8\\xaa \\xd9\\x85\\xd8\\xad\\xd9\\x84 \\xd9\\x85\\xd9\\x86\\xd8\\xa7\\xd9\\x84 \\xd9\\x84\\xd8\\xa7\\xd8\\xac\\xd9\\x87\\xd8\\xb4\\xd8\\xaa \\xd8\\xa8\\xd8\\xa7\\xd9\\x84\\xd8\\xa8\\xd9\\x83\\xd8\\xa7\\xd8\\xa1', 'target': b'\\xd9\\x84\\xd9\\x88\\xd9\\x83\\xd8\\xa7\\xd9\\x86 \\xd8\\xac\\xd9\\x8a\\xd8\\xaa  \\xd9\\x85\\xd9\\x86\\xd8\\xa7\\xd9\\x84 \\xd9\\x86\\xd9\\x88\\xd8\\xb6 \\xd9\\x86\\xd8\\xa8\\xd9\\x83\\xd9\\x8a '}\n",
            "لو كنت محل منال لاجهشت بالبكاء\n",
            "لوكان جيت  منال نوض نبكي \n",
            "55\n",
            "44\n",
            "{'source': b'\\xd8\\xa7\\xd8\\xb0\\xd8\\xa7 \\xd9\\x85\\xd8\\xa7 \\xd8\\xad\\xd8\\xa7\\xd9\\x81\\xd8\\xb8\\xd9\\x86\\xd8\\xa7 \\xd8\\xb9\\xd9\\x84\\xd9\\x89 \\xd9\\x87\\xd8\\xb0\\xd9\\x87 \\xd8\\xa7\\xd9\\x84\\xd9\\x88\\xd8\\xaa\\xd9\\x8a\\xd8\\xb1\\xd8\\xa9 \\xd8\\xb4\\xd9\\x87\\xd8\\xb1\\xd9\\x8a\\xd9\\x86 \\xd8\\xa7\\xd9\\x88 \\xd8\\xab\\xd9\\x84\\xd8\\xa7\\xd8\\xab\\xd8\\xa9 \\xd9\\x86\\xd8\\xb5\\xd9\\x8a\\xd8\\xa8 \\xd8\\xa7\\xd9\\x84\\xd9\\x87\\xd8\\xaf\\xd9\\x81', 'target': b'\\xd9\\x86\\xd9\\x83\\xd9\\x85\\xd9\\x84\\xd9\\x88 \\xd8\\xb9\\xd9\\x84\\xd9\\x89 \\xd9\\x87\\xd8\\xa7 \\xd8\\xa7\\xd9\\x84\\xd8\\xb1\\xd9\\x8a\\xd8\\xaa\\xd9\\x85 \\xd8\\xb4\\xd9\\x87\\xd8\\xb1\\xd9\\x8a\\xd9\\x86 \\xd9\\x88\\xd9\\x84\\xd8\\xa7 \\xd8\\xab\\xd9\\x84\\xd8\\xa7\\xd8\\xab\\xd8\\xa9 \\xd9\\x86\\xd9\\x88\\xd8\\xb5\\xd9\\x84\\xd9\\x88 \\xd9\\x84\\xd9\\x84\\xd9\\x8a \\xd8\\xad\\xd8\\xa7\\xd8\\xb4\\xd8\\xaa\\xd9\\x86\\xd8\\xa7 \\xd8\\xa8\\xd9\\x8a\\xd9\\x87'}\n",
            "اذا ما حافظنا على هذه الوتيرة شهرين او ثلاثة نصيب الهدف\n",
            "نكملو على ها الريتم شهرين ولا ثلاثة نوصلو للي حاشتنا بيه\n",
            "100\n",
            "102\n",
            "{'source': b' \\xd9\\x83\\xd8\\xa7\\xd9\\x86\\xd8\\xaa \\xd8\\xaa\\xd9\\x88\\xd8\\xac\\xd8\\xaf \\xd8\\xb4\\xd8\\xac\\xd8\\xb1\\xd8\\xa9 \\xd8\\xaa\\xd9\\x8a\\xd9\\x86 \\xd8\\xaa\\xd8\\xb9\\xd8\\xb7\\xd8\\xb1 \\xd9\\x83\\xd9\\x84 \\xd8\\xa7\\xd9\\x84\\xd8\\xad\\xd9\\x8a   ', 'target': b'\\xd9\\x83\\xd8\\xa7\\xd9\\x86\\xd8\\xaa \\xd9\\x88\\xd8\\xa7\\xd8\\xad\\xd8\\xaf \\xd8\\xb4\\xd8\\xac\\xd8\\xb1\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd9\\x83\\xd8\\xb1\\xd9\\x85\\xd9\\x88\\xd8\\xb5 \\xd9\\x83\\xd8\\xaa\\xd8\\xb9\\xd8\\xb7\\xd8\\xb1 \\xd8\\xa7\\xd9\\x84\\xd8\\xad\\xd9\\x88\\xd9\\x85\\xd8\\xa9 \\xd9\\x83\\xd8\\xa7\\xd9\\x85\\xd9\\x84\\xd8\\xa9'}\n",
            " كانت توجد شجرة تين تعطر كل الحي   \n",
            "كانت واحد شجرة الكرموص كتعطر الحومة كاملة\n",
            "60\n",
            "76\n",
            "{'source': b'\\xd8\\xa7\\xd8\\xac\\xd9\\x84 \\xd9\\x82\\xd8\\xa7\\xd9\\x84\\xd8\\xaa \\xd9\\x84\\xd9\\x8a \\xd8\\xa7\\xd9\\x86\\xd9\\x87\\xd8\\xa7 \\xd8\\xb1\\xd8\\xb3\\xd9\\x85\\xd8\\xaa \\xd9\\x81\\xd9\\x8a \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xb3\\xd8\\xaa\\xd8\\xb4\\xd9\\x81\\xd9\\x89', 'target': b'\\xd8\\xa7\\xd9\\x8a\\xd9\\x87 \\xd9\\x82\\xd8\\xa7\\xd9\\x84\\xd8\\xaa \\xd9\\x84\\xd9\\x8a \\xd9\\x81\\xd9\\x8a \\xd8\\xa7\\xd9\\x84\\xd9\\x84\\xd9\\x88\\xef\\xad\\x98\\xef\\xbb\\xb4\\xef\\xba\\x98\\xef\\xba\\x8e\\xd9\\x84 \\xd8\\xaa\\xd9\\x83\\xd9\\x88\\xd9\\x86\\xd9\\x81\\xd8\\xb1\\xd9\\x85\\xd8\\xa7\\xd8\\xaa'}\n",
            "اجل قالت لي انها رسمت في المستشفى\n",
            "ايه قالت لي في اللوﭘﻴﺘﺎل تكونفرمات\n",
            "60\n",
            "67\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GgyqkGfYaj2Y",
        "colab": {}
      },
      "source": [
        "  #turn the ds of dictionaries and change the keys to inputs and targets that the model\n",
        "def mag_msa_translation_preprocessor(ds):\n",
        "    def to_inputs_and_targets(ex):\n",
        "      return{\n",
        "          \"inputs\": tf.strings.join([\"translate Maghrib to MSA: \",ex[\"source\"]]),\n",
        "          \"targets\": ex[\"target\"]\n",
        "      }\n",
        "    return ds.map(to_inputs_and_targets, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bmja1M2Eaj2f",
        "colab": {}
      },
      "source": [
        "t5.data.TaskRegistry.remove(\"translation_mag_msa\")\n",
        "t5.data.TaskRegistry.add(\n",
        "    #name of the Task\n",
        "    \"translation_mag_msa\",\n",
        "    #Supply a function which returns a tf.data.Dataset\n",
        "    dataset_fn=mag_msa_translation_dataset_fn,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[mag_msa_translation_preprocessor],\n",
        "    # Lowercase targets before computing metrics.\n",
        "\n",
        "    #postprocess_fn = t5.data.postprocessors.lower_text, \n",
        "    \n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    \n",
        "    #metric_fns=[t5.evaluation.metrics.bleu],\n",
        "    \n",
        "    # Not required, but helps for mixing and auto-caching.\n",
        "    num_input_examples=mag_msa_example_count,\n",
        "    # output_features\n",
        "    output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(filepath)),\n",
        "    # specifying token processor\n",
        "    token_preprocessor=[\n",
        "      functools.partial(\n",
        "          preprocessors.select_random_chunk,\n",
        "          feature_key=\"targets\",\n",
        "          max_length=65536\n",
        "      ),\n",
        "      functools.partial(\n",
        "          preprocessors.reduce_concat_tokens,\n",
        "          feature_key=\"targets\",\n",
        "          batch_size=128\n",
        "      ),\n",
        "      preprocessors.split_tokens_to_inputs_length,\n",
        "      functools.partial(\n",
        "          preprocessors.denoise,\n",
        "          inputs_fn=preprocessors.noise_span_to_unique_sentinel,\n",
        "          targets_fn=preprocessors.nonnoise_span_to_unique_sentinel,\n",
        "          noise_density=0.15,\n",
        "          noise_mask_fn=preprocessors.iid_noise_mask,\n",
        "      )\n",
        "    ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLoe7vbFV42o",
        "colab_type": "text"
      },
      "source": [
        "##Dataset Mixture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWMTtMsXWAua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t5.data.MixtureRegistry.remove(\"translation_msa\")\n",
        "t5.data.MixtureRegistry.add(\n",
        "    \"translation_msa\",\n",
        "    [\"translation_en_msa\", \"translation_lav_msa\", \"translation_mag_msa\"],\n",
        "     default_rate=1.0\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc0bSgP1WenA",
        "colab_type": "text"
      },
      "source": [
        "##Pre-Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw146D5HkkPi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "5e0c5735-b4b6-46b9-86f1-f3aa356ec155"
      },
      "source": [
        "#gotta get the base config and add the new tasks' task params\n",
        "!wget \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-config.json\" -O data/config/t5-base-config.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-23 03:22:47--  https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-config.json\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.10.182\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.10.182|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1199 (1.2K) [application/json]\n",
            "Saving to: ‘data/config/t5-base-config.json’\n",
            "\n",
            "\r          data/conf   0%[                    ]       0  --.-KB/s               \rdata/config/t5-base 100%[===================>]   1.17K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-06-23 03:22:47 (102 MB/s) - ‘data/config/t5-base-config.json’ saved [1199/1199]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uP2fwStXK5b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "#Using the base config from Huggingface T5 Model\n",
        "config = transformers.T5Config.from_json_file(json_file=\"data/config/t5-base-config.json\")\n",
        "model = t5.models.HfPyTorchModel(config, \"/tmp/hft5/\", device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M23Y4l0tb4qM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "850aa3e2-4c20-4ab1-87a1-c6a82780a8a1"
      },
      "source": [
        "ls /tmp/hft5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "events.out.tfevents.1592882573.98f0527c499d.137.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VV2RSZGxQxYT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1fEVj9jCxvcKn9zg8lO43i2sWZquegg5H' -O data/operative_config.gin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4ZqkAzufRI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import gin\n",
        "# with gin.unlock_config():\n",
        "#   gin.parse_config_file(\"data/operative_config.gin\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLDKa6dmo7fv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "review_task = t5.data.TaskRegistry.get(\"translation_lav_msa\")\n",
        "ds = review_task.get_dataset(split=\"validation\", sequence_length={\"inputs\": 128, \"targets\": 32})\n",
        "print(\"A few preprocessed validation examples...\")\n",
        "for ex in tfds.as_numpy(ds.take(1)):\n",
        "  print(ex['source'].decode())\n",
        "  print(ex['target'].decode())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6f5uUWXWUKw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a82fdb10-29aa-4a42-c7f0-cb985ab391c4"
      },
      "source": [
        "STEPS = 10000 #@param {type: \"integer\"}\n",
        "model.train(\n",
        "    mixture_or_task_name=\"translation_msa\",\n",
        "    steps=STEPS,\n",
        "    save_steps=STEPS/5,                                                   \n",
        "    sequence_length={\"inputs\": 32, \"targets\": 32},\n",
        "    split=\"train\",\n",
        "    batch_size=32,\n",
        "    optimizer=functools.partial(transformers.AdamW, lr=1e-4),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-ca6fc2eb99a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/t5/models/hf_model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, mixture_or_task_name, steps, save_steps, sequence_length, split, batch_size, optimizer, learning_rate_scheduler)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \"\"\"\n\u001b[1;32m    307\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixture_or_task_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m     \u001b[0;31m# Repeat dataset forever\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/t5/models/hf_model.py\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(mixture_or_task_name, sequence_length, split, batch_size)\u001b[0m\n\u001b[1;32m    148\u001b[0m   \"\"\"\n\u001b[1;32m    149\u001b[0m   \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mixture_or_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixture_or_task_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m   \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m   return tokens_to_batches(\n\u001b[1;32m    152\u001b[0m       \u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/t5/data/utils.py\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(self, sequence_length, split, use_cached, shuffle, compute_stats_empirically)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m         for task in tasks]\n\u001b[0m\u001b[1;32m    991\u001b[0m     \u001b[0mrates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m     \u001b[0;31m# Sample from the dataset with the rates rates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/t5/data/utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m         for task in tasks]\n\u001b[0m\u001b[1;32m    991\u001b[0m     \u001b[0mrates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m     \u001b[0;31m# Sample from the dataset with the rates rates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/t5/data/utils.py\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(self, sequence_length, split, use_cached, shuffle, shuffle_buffer_size)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[0;31m# Post tokenization processing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    696\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_print_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/t5/data/utils.py\u001b[0m in \u001b[0;36mpreprocess_tokens\u001b[0;34m(self, dataset, sequence_length)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_token_preprocessor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m         output_features=self.output_features)\n\u001b[0m\u001b[1;32m    569\u001b[0m     dataset = self._validate_dataset(\n\u001b[1;32m    570\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/t5/data/utils.py\u001b[0m in \u001b[0;36m_preprocess_dataset\u001b[0;34m(self, dataset, preprocessors, **preprocess_kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0mpreprocessors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpreprocessors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mprep_fn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m       \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprep_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gin/config.py\u001b[0m in \u001b[0;36mgin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1046\u001b[0m       \u001b[0mminimal_selector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_REGISTRY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimal_selector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m       \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminimal_selector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_required_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \u001b[0;31m# Now, update with the caller-supplied `kwargs`, allowing the caller to have\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Required bindings for `denoise` not provided in config: ['noise_mask_fn']"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Uv-8hgBo8Lb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "review_task = t5.data.TaskRegistry.get(\"translation_lav_msa\")\n",
        "ds = review_task.get_dataset(split=\"validation\", sequence_length={\"inputs\": 128, \"targets\": 32})\n",
        "print(\"A few preprocessed validation examples...\")\n",
        "for ex in tfds.as_numpy(ds.take(1)):\n",
        "  print(ex['source'].decode())\n",
        "  print(ex['target'].decode())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev_E4ZadXxmj",
        "colab_type": "text"
      },
      "source": [
        "##Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjZnTOIrYEh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate after fine-tuning\n",
        "model.eval(\n",
        "    \"translation_msa\",\n",
        "    checkpoint_steps=STEPS,\n",
        "    sequence_length={\"inputs\": 32, \"targets\": 32},\n",
        "    batch_size=32,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF-3_4u1X-5r",
        "colab_type": "text"
      },
      "source": [
        "##Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL5yLXs4YJw7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = [\n",
        "    \"translate English to MSA: Tom was also there.\",\n",
        "    \"translate English to MSA: Please send us more information\",\n",
        "]\n",
        "model.predict(\n",
        "    inputs,\n",
        "    sequence_length={\"inputs\": 32},\n",
        "    batch_size=2,\n",
        "    output_file=\"/tmp/hft5/example_predictions.txt\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}