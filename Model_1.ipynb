{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_1.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BQflUKhjIKNp"
      },
      "source": [
        "### Notes \n",
        "\n",
        "T5 Paper: https://arxiv.org/pdf/1910.10683.pdf\n",
        "\n",
        "T5 Tokenizer: https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_t5.py\n",
        "\n",
        "Important Tasks: https://docs.google.com/document/d/1weIZM6QTlnitpPQmpg-WeV2RW70TnYmDuogBQPr5mB0/edit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLOmiOta6MJp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "878ec2d0-d60a-4788-8e79-3247e5ec8896"
      },
      "source": [
        "#installation step\n",
        "!pip install transformers\n",
        "!pip install t5\n",
        "!pip install sentencepiece\n",
        "#creating the folders \n",
        "!mkdir data/\n",
        "!mkdir data/AD_NMT-master\n",
        "!mkdir data/train/\n",
        "!mkdir data/test/\n",
        "!mkdir data/val/\n",
        "!mkdir data/model/\n",
        "!mkdir data/config/\n",
        "#fetching the pkl files\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW' -O data/AD_NMT-master/english-Arabic-both.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV' -O data/AD_NMT-master/LAV-MSA-2-both.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UjDX7cCG2S23SPfSHxSPdVayMTxB5Y16' -O data/AD_NMT-master/Magribi_MSA-both.pkl\n",
        "# !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1fEVj9jCxvcKn9zg8lO43i2sWZquegg5H' -O data/operative_config.gin\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UGKswXSqHSxWpx57cEDzvNeJaqbAuyt8' -O data/padic.xml"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: t5 in /usr/local/lib/python3.6/dist-packages (0.6.4)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.6/dist-packages (from t5) (2.3.0)\n",
            "Requirement already satisfied: tfds-nightly in /usr/local/lib/python3.6/dist-packages (from t5) (3.2.1.dev202008280105)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from t5) (1.18.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from t5) (1.0.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from t5) (1.4.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from t5) (0.8.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from t5) (0.22.2.post1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from t5) (0.1.91)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from t5) (1.6.0+cu101)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.6/dist-packages (from t5) (1.4.13)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.6/dist-packages (from t5) (2.8.0)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.6/dist-packages (from t5) (0.0.4)\n",
            "Requirement already satisfied: six>=1.14 in /usr/local/lib/python3.6/dist-packages (from t5) (1.15.0)\n",
            "Requirement already satisfied: transformers>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from t5) (3.0.2)\n",
            "Requirement already satisfied: mesh-tensorflow[transformer]>=0.1.13 in /usr/local/lib/python3.6/dist-packages (from t5) (0.1.16)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from t5) (3.2.5)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.6/dist-packages (from t5) (0.3.0)\n",
            "Requirement already satisfied: tensorflow<2.4,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-text->t5) (2.3.0)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (3.0.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (2.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (0.3.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (1.1.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (2.23.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (3.12.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (0.16.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (0.1.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (4.41.1)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (0.23.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (1.12.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (0.7)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (20.1.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->t5) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->t5) (2.8.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->t5) (0.16.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from sacrebleu->t5) (2.0.0)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5) (0.8.1rc1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5) (0.0.43)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5) (20.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5) (2019.12.20)\n",
            "Requirement already satisfied: tensorflow-datasets; extra == \"transformer\" in /usr/local/lib/python3.6/dist-packages (from mesh-tensorflow[transformer]>=0.1.13->t5) (2.1.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (2.10.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (3.3.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (1.31.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (0.35.1)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (2.3.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (0.3.3)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (0.2.0)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->tfds-nightly->t5) (3.1.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly->t5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly->t5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly->t5) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly->t5) (1.24.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tfds-nightly->t5) (49.6.0)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tfds-nightly->t5) (1.52.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.7.0->t5) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=2.7.0->t5) (2.4.7)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (0.4.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (1.17.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (3.2.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (0.2.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (1.7.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (0.4.8)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.91)\n",
            "mkdir: cannot create directory ‘data/’: File exists\n",
            "mkdir: cannot create directory ‘data/AD_NMT-master’: File exists\n",
            "mkdir: cannot create directory ‘data/train/’: File exists\n",
            "mkdir: cannot create directory ‘data/test/’: File exists\n",
            "mkdir: cannot create directory ‘data/val/’: File exists\n",
            "mkdir: cannot create directory ‘data/model/’: File exists\n",
            "mkdir: cannot create directory ‘data/config/’: File exists\n",
            "--2020-08-29 02:21:28--  https://docs.google.com/uc?export=download&id=1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.111.101, 108.177.111.138, 108.177.111.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.111.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-10-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/4ci9a7dmo6m4t96egeu5uei6o7qm0ken/1598667675000/16970776037313924126/*/1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-08-29 02:21:28--  https://doc-10-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/4ci9a7dmo6m4t96egeu5uei6o7qm0ken/1598667675000/16970776037313924126/*/1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW?e=download\n",
            "Resolving doc-10-2s-docs.googleusercontent.com (doc-10-2s-docs.googleusercontent.com)... 172.217.214.132, 2607:f8b0:4001:c05::84\n",
            "Connecting to doc-10-2s-docs.googleusercontent.com (doc-10-2s-docs.googleusercontent.com)|172.217.214.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 955428 (933K) [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/english-Arabic-both.pkl’\n",
            "\n",
            "data/AD_NMT-master/ 100%[===================>] 933.04K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2020-08-29 02:21:29 (109 MB/s) - ‘data/AD_NMT-master/english-Arabic-both.pkl’ saved [955428/955428]\n",
            "\n",
            "--2020-08-29 02:21:29--  https://docs.google.com/uc?export=download&id=1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.202.139, 74.125.202.138, 74.125.202.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.202.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-00-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/0mgclsn616qe09pjnujip9nh8bqpov3b/1598667675000/16970776037313924126/*/1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-08-29 02:21:30--  https://doc-00-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/0mgclsn616qe09pjnujip9nh8bqpov3b/1598667675000/16970776037313924126/*/1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV?e=download\n",
            "Resolving doc-00-2s-docs.googleusercontent.com (doc-00-2s-docs.googleusercontent.com)... 172.217.214.132, 2607:f8b0:4001:c05::84\n",
            "Connecting to doc-00-2s-docs.googleusercontent.com (doc-00-2s-docs.googleusercontent.com)|172.217.214.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/LAV-MSA-2-both.pkl’\n",
            "\n",
            "data/AD_NMT-master/     [ <=>                ]   2.33M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-08-29 02:21:30 (177 MB/s) - ‘data/AD_NMT-master/LAV-MSA-2-both.pkl’ saved [2447014]\n",
            "\n",
            "--2020-08-29 02:21:30--  https://docs.google.com/uc?export=download&id=1UjDX7cCG2S23SPfSHxSPdVayMTxB5Y16\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.111.113, 108.177.111.139, 108.177.111.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.111.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-04-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/gvkb5tqeo5rhti6k18sbtegnoionmh7e/1598667675000/16970776037313924126/*/1UjDX7cCG2S23SPfSHxSPdVayMTxB5Y16?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-08-29 02:21:31--  https://doc-04-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/gvkb5tqeo5rhti6k18sbtegnoionmh7e/1598667675000/16970776037313924126/*/1UjDX7cCG2S23SPfSHxSPdVayMTxB5Y16?e=download\n",
            "Resolving doc-04-2s-docs.googleusercontent.com (doc-04-2s-docs.googleusercontent.com)... 172.217.214.132, 2607:f8b0:4001:c05::84\n",
            "Connecting to doc-04-2s-docs.googleusercontent.com (doc-04-2s-docs.googleusercontent.com)|172.217.214.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/Magribi_MSA-both.pkl’\n",
            "\n",
            "data/AD_NMT-master/     [ <=>                ]   2.81M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-08-29 02:21:31 (215 MB/s) - ‘data/AD_NMT-master/Magribi_MSA-both.pkl’ saved [2944107]\n",
            "\n",
            "--2020-08-29 02:21:31--  https://docs.google.com/uc?export=download&id=1UGKswXSqHSxWpx57cEDzvNeJaqbAuyt8\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.111.100, 108.177.111.102, 108.177.111.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.111.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-04-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/3nna5dnjtjnvdod1dmtigupka67ngs05/1598667675000/16970776037313924126/*/1UGKswXSqHSxWpx57cEDzvNeJaqbAuyt8?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-08-29 02:21:32--  https://doc-04-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/3nna5dnjtjnvdod1dmtigupka67ngs05/1598667675000/16970776037313924126/*/1UGKswXSqHSxWpx57cEDzvNeJaqbAuyt8?e=download\n",
            "Resolving doc-04-2s-docs.googleusercontent.com (doc-04-2s-docs.googleusercontent.com)... 172.217.214.132, 2607:f8b0:4001:c05::84\n",
            "Connecting to doc-04-2s-docs.googleusercontent.com (doc-04-2s-docs.googleusercontent.com)|172.217.214.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/xml]\n",
            "Saving to: ‘data/padic.xml’\n",
            "\n",
            "data/padic.xml          [ <=>                ]   2.97M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2020-08-29 02:21:32 (169 MB/s) - ‘data/padic.xml’ saved [3114143]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-WzhRv4mIKNq",
        "colab": {}
      },
      "source": [
        "#James Chartouni\n",
        "#Joey Park\n",
        "#Raef Khan\n",
        "\n",
        "import torch\n",
        "from torch.optim import SGD\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os, io, glob\n",
        "import functools\n",
        "\n",
        "import sentencepiece as spm\n",
        "\n",
        "import transformers\n",
        "import t5\n",
        "from t5.data import preprocessors\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "irQQ9E_7IKOM"
      },
      "source": [
        "## Prepare Datasets\n",
        "\n",
        "We need to take our training and test sets from the pkl files and create new .txt files that are formatted so that the standard torchtext Dataset class can read them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fctRFLVvs3X2",
        "colab_type": "text"
      },
      "source": [
        "### PADIC Dataset Parsing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4_RfmVTUuze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "padic_tree = ET.parse('data/padic.xml')\n",
        "\n",
        "padic_alg_msa = []\n",
        "padic_ann_msa = []\n",
        "padic_syr_msa = []\n",
        "padic_pal_msa = []\n",
        "padic_mor_msa = [] \n",
        "\n",
        "for sentence in padic_tree.getroot():\n",
        "  padic_alg_msa.append([sentence.find('ALGIERS').text.strip(), sentence.find('MODERN-STANDARD-ARABIC').text.strip()])\n",
        "  padic_ann_msa.append([sentence.find('ANNABA').text.strip(), sentence.find('MODERN-STANDARD-ARABIC').text.strip()])\n",
        "  padic_syr_msa.append([sentence.find('SYRIAN').text.strip(), sentence.find('MODERN-STANDARD-ARABIC').text.strip()])\n",
        "  padic_pal_msa.append([sentence.find('PALESTINIAN').text.strip(), sentence.find('MODERN-STANDARD-ARABIC').text.strip()])\n",
        "  padic_mor_msa.append([sentence.find('MOROCCAN').text.strip(), sentence.find('MODERN-STANDARD-ARABIC').text.strip()])"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iL68WL8LEtTF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "46552e1f-a096-42f5-fe74-bce4eb74af43"
      },
      "source": [
        "print(padic_alg_msa[0])\n",
        "print(padic_ann_msa[0])\n",
        "print(padic_syr_msa[0])\n",
        "print(padic_pal_msa[0])\n",
        "print(padic_mor_msa[0])"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['EAdw AlnAs ytbAkAw bdyt nhdr mn qlby tqwl nhdy fAlnAs', \"tEAlt >SwAt AlnAs bAlbkA'،  bd>t >tHdv bAnfEAl w k>nny >hdy fy AlnAs\"]\n",
            "['EAdwA AlnAs ytbAkAw bdyt nhdr bg$ w qwl ElyA nhdy fy AlnAs', \"tEAlt >SwAt AlnAs bAlbkA'،  bd>t >tHdv bAnfEAl w k>nny >hdy fy AlnAs\"]\n",
            "['Ely Swt AlnAs bAlbky w bl~$t >Hky bESbyp w k>ny Em Ahdy bAlnAs', \"tEAlt >SwAt AlnAs bAlbkA'،  bd>t >tHdv bAnfEAl w k>nny >hdy fy AlnAs\"]\n",
            "['SArwA AlnAs ySyHwA bSwt EAly wbdyt AHky wAnA mnfEl wk>ny bhdy fy AlnAs', \"tEAlt >SwAt AlnAs bAlbkA'،  bd>t >tHdv bAnfEAl w k>nny >hdy fy AlnAs\"]\n",
            "['nAs bdAw tytbAkAw wbdyt tnhdr b nfEl bHAl <lY tnhdy AlnAs', \"tEAlt >SwAt AlnAs bAlbkA'،  bd>t >tHdv bAnfEAl w k>nny >hdy fy AlnAs\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSfrbD9pGrZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alg_msa_train, alg_msa_val = train_test_split(padic_alg_msa, test_size=.15)\n",
        "ann_msa_train, ann_msa_val = train_test_split(padic_ann_msa, test_size=.15)\n",
        "syr_msa_train, syr_msa_val = train_test_split(padic_syr_msa, test_size=.15)\n",
        "pal_msa_train, pal_msa_val = train_test_split(padic_pal_msa, test_size=.15)\n",
        "mor_msa_train, mor_msa_val = train_test_split(padic_mor_msa, test_size=.15)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5uDm27ZHOEi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "02ce28e1-35b0-4486-c196-db8ad1fa5071"
      },
      "source": [
        "#all the translations have equal amt. of examples\n",
        "print(len(alg_msa_train))\n",
        "print(len(alg_msa_val))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6131\n",
            "1082\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxCLgMZ2ns4b",
        "colab_type": "text"
      },
      "source": [
        "###Initial Loading from Pickle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aha0xureIKNw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cacbfdc7-03b1-43c1-d09d-ed69d7bf8b45"
      },
      "source": [
        "ls data/AD_NMT-master"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "english-Arabic-both.pkl  LAV-MSA-2-both.pkl  Magribi_MSA-both.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XWKnGRlLIKN9",
        "colab": {}
      },
      "source": [
        "file_path = 'data/AD_NMT-master/'\n",
        "\n",
        "with open(file_path + \"english-Arabic-both.pkl\", 'rb') as handle:\n",
        "    data_MSA_English_both = pickle.load(handle) \n",
        "\n",
        "with open(file_path + \"LAV-MSA-2-both.pkl\", 'rb') as handle:\n",
        "    data_LAV_MSA_both = pickle.load(handle) \n",
        "\n",
        "with open(file_path + \"Magribi_MSA-both.pkl\", 'rb') as handle:\n",
        "    data_Magribi_MSA_both = pickle.load(handle) \n",
        "    "
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ch3APJadIKOH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "5e46a84c-3e15-472c-a103-8dd63c74fa1e"
      },
      "source": [
        "#few dataset examples\n",
        "print(data_MSA_English_both[0:5])\n",
        "print(data_MSA_English_both[-5:])\n",
        "print(data_LAV_MSA_both[0:5])\n",
        "print(data_Magribi_MSA_both[0:5])"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Tom was also there', 'كان توم هنا ايضا'], ['That old woman lives by herself', 'تلك المراة العجوز تسكن بمفردها'], ['He went abroad for the purpose of studying English', 'سافر خارج البلد ليتعلم الانجليزية'], ['There is a fork missing', 'هناك شوكة ناقصة'], [\"I don't know this game\", 'لا اعرف هذه اللعبة']]\n",
            "[['Please send us more information', 'ارسل الينا المزيد من المعلومات اذا تكرمت'], ['I am an only child', 'انا طفل وحيد ابي و امي'], ['Make good use of your time', 'استفد من وقتك جيدا'], [\"Fighting won't settle anything\", 'لن يحل القتال اي شيء'], ['Practice makes perfect', 'الممارسة هي الطريق الى الاتقان']]\n",
            "[['لا انا بعرف وحدة راحت ع فرنسا و معا شنتا حطت فيها الفرش', 'لا اعرف واحدة ذهبت الى فرنسا و لها غرفة و ضعت فيها الافرشة'], ['روح بوشك و فتول عاليسار', 'اذهب تقدم و استدر يسارا'], ['لا لا لازم انه يكون عندك موضوع ما في اشي', ' لا لا يجب ان يكون لديك موضوع هذا ضروري'], ['اوعي تبعدي من هون بلاش تضيعي ', 'لا تبتعد عن هنا حتى لا تفقد الطريق '], ['قصدي صراحة يما انا كمان كرهته من يوم ما عملتيه زي ما بتعمله خالتي كرهته و صرت ما باطيقه بالمرة', 'اقصد صراحة يا امي انا ايضا كرهته من يوم حضرته مثلما تحضره خالتي كرهته و اصبحت لا اطيقه ابدا']]\n",
            "[['يا ربي متخليش حتى لبيوتا ديالهم يوصلو ل البارة', 'يارب لا تدع اهدافهم تصيب حتى العارضة'], ['يعطيك الصحة كريمة', 'يعطيك العافية كريمة'], [' لوكان جوزوزه ساعة و نص و يهنيونا ', 'لو انهم يبثونه ساعة و نصف و يريحوننا'], ['ولا عيط لك واحد و قالك راه فلان رايح يضربك غدوة بكف ', 'او انه قال لك بان فلان سيصفعك غدا'], ['عبرتي انا عقدتها مرة وحدة', 'احسنت عقدتها مرة واحدة']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lp7ncVY6wqET",
        "colab": {}
      },
      "source": [
        "#splits the train dataset into train and validation sets, define test set as datafile\n",
        "msa_en_train, msa_en_val = train_test_split(data_MSA_English_both, test_size=.2)\n",
        "\n",
        "lav_msa_train, lav_msa_val = train_test_split(data_LAV_MSA_both, test_size=.2)\n",
        "\n",
        "mag_msa_train, mag_msa_val = train_test_split(data_Magribi_MSA_both, test_size=.2)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9Ue2fSILwqEW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "7f823787-c98c-46fb-d0a0-822217ea239e"
      },
      "source": [
        "print(len(msa_en_train))\n",
        "print(len(msa_en_val))\n",
        "\n",
        "print(len(lav_msa_train))\n",
        "print(len(lav_msa_val))\n",
        "\n",
        "print(len(mag_msa_train))\n",
        "print(len(mag_msa_val))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8000\n",
            "2001\n",
            "12644\n",
            "3161\n",
            "15788\n",
            "3948\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W3bWU46TIKOb",
        "colab": {}
      },
      "source": [
        "file_path = 'data/'\n",
        "\n",
        "def list_to_csv(ds, src='msa', trg='en', datatype=''):\n",
        "    src_formatted = datatype + '_' + src + '_' + trg + '.' + 'csv'\n",
        "    \n",
        "    with open(file_path + datatype + \"/\" + src_formatted, 'wt') as csv:\n",
        "        for i, arr in enumerate(ds):\n",
        "            csv.write(arr[1] + ',' + arr[0] + '\\n')"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AqwDB9BuKEnV",
        "colab": {}
      },
      "source": [
        "list_to_csv(msa_en_train, 'msa', 'en', 'train')\n",
        "list_to_csv(msa_en_val, 'msa', 'en', 'val')\n",
        "\n",
        "list_to_csv(lav_msa_train, 'lav', 'msa', 'train')\n",
        "list_to_csv(lav_msa_val, 'lav', 'msa', 'val')\n",
        "\n",
        "list_to_csv(mag_msa_train, 'mag', 'msa', 'train')\n",
        "list_to_csv(mag_msa_val, 'mag', 'msa', 'val')"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGqh46GhVHfI",
        "colab_type": "text"
      },
      "source": [
        "## Training SentencePiece Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lcM0dLclt6J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#combine all the training lines of all three languages\n",
        "spm_input_ds = msa_en_train + mag_msa_train + lav_msa_train"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBZ5iR21etNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def list_to_input(ds):\n",
        "    src_formatted = 'spm_input' + '.' + 'txt'\n",
        "\n",
        "    with open(file_path + \"/\" + src_formatted, 'wt') as sentencelinefile:\n",
        "        for i, arr in enumerate(ds):\n",
        "            sentencelinefile.write(arr[0] + '\\n' + arr[1] + '\\n')"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxDKpM1H7vaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_to_input(spm_input_ds)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3YrvcR8anQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = 32128\n",
        "spm.SentencePieceTrainer.train('--input=data/spm_input.txt --model_prefix=data/model/spm --vocab_size=' + str(VOCAB_SIZE) + ' --unk_id=2 --bos_id=-1 --eos_id=1 --pad_id=0 --hard_vocab_limit=False')"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzB-_AeUUNLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath = 'data/model/spm.model'"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrHAuzNW1eIh",
        "colab_type": "text"
      },
      "source": [
        "##Tensor Processing + Add to TaskRegistry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oVVzdeAZd0x",
        "colab_type": "text"
      },
      "source": [
        "### English to Arabic Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWnZcYI_491p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "msa_en_split_csv_path = {\n",
        "    \"train\": \"data/train/train_msa_en.csv\",\n",
        "    \"validation\": \"data/val/val_msa_en.csv\"\n",
        "}\n",
        "msa_en_example_count = {\n",
        "    \"train\": len(msa_en_train),\n",
        "    \"validation\": len(msa_en_val)\n",
        "}"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2e3cM_wP_M8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "8979e519-169f-4dac-dd43-a9c56c34998e"
      },
      "source": [
        "def msa_en_translation_dataset_fn(split, shuffle_files=False):\n",
        "  ds = tf.data.TextLineDataset(msa_en_split_csv_path[split])\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"\",\"\"],\n",
        "                        field_delim=\",\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "  )\n",
        "  ds = ds.map(lambda *example: dict(zip([\"source\", \"target\"], example)) )\n",
        "  return ds\n",
        "\n",
        "for example in tfds.as_numpy(msa_en_translation_dataset_fn(\"train\").take(5)):\n",
        "    print(example)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'source': b'\\xd9\\x87\\xd8\\xb0\\xd8\\xa7 \\xd8\\xa7\\xd9\\x85\\xd8\\xb1 \\xd9\\x8a\\xd8\\xa7 \\xd8\\xaa\\xd9\\x88\\xd9\\x85', 'target': b\"That's an order Tom\"}\n",
            "{'source': b'\\xd8\\xa7\\xd9\\x86\\xd8\\xa7 \\xd8\\xac\\xd8\\xa7\\xd8\\xa6\\xd8\\xb9 \\xd9\\x84\\xd9\\x84\\xd8\\xba\\xd8\\xa7\\xd9\\x8a\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd9\\x86', 'target': b\"I'm very hungry now\"}\n",
            "{'source': b'\\xd8\\xa7\\xd8\\xb3\\xd8\\xaa\\xd9\\x85\\xd8\\xb1 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xb7\\xd8\\xb1 \\xd8\\xae\\xd9\\x85\\xd8\\xb3\\xd8\\xa9 \\xd8\\xa7\\xd9\\x8a\\xd8\\xa7\\xd9\\x85', 'target': b'The rain lasted five days'}\n",
            "{'source': b'\\xd9\\x84\\xd9\\x82\\xd8\\xaf \\xd8\\xb3\\xd9\\x85\\xd8\\xb9\\xd8\\xaa \\xd9\\x87\\xd8\\xb0\\xd9\\x87 \\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd8\\xba\\xd9\\x86\\xd9\\x8a\\xd8\\xa9 \\xd9\\x85\\xd9\\x86 \\xd9\\x82\\xd8\\xa8\\xd9\\x84', 'target': b\"I've heard this song before\"}\n",
            "{'source': b'\\xd8\\xaf\\xd8\\xb1\\xd8\\xa7\\xd8\\xac\\xd8\\xaa\\xd9\\x87 \\xd8\\xb2\\xd8\\xb1\\xd9\\x82\\xd8\\xa7\\xd8\\xa1', 'target': b'His bicycle is blue'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1GXED2IO3pE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#turn the ds of dictionaries and change the keys to inputs and targets that the model\n",
        "def msa_en_translation_preprocessor(ds):\n",
        "  def to_inputs_and_targets(ex):\n",
        "    return{\n",
        "        \"inputs\": tf.strings.join([\"translate MSA to English: \",ex[\"source\"]]),\n",
        "        \"targets\": ex[\"target\"]\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-W6j-SDXcNT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t5.data.TaskRegistry.remove(\"translation_msa_en\")\n",
        "t5.data.TaskRegistry.add(\n",
        "    #name of the Task\n",
        "    \"translation_msa_en\",\n",
        "    #Supply a function which returns a tf.data.Dataset\n",
        "    dataset_fn=msa_en_translation_dataset_fn,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[msa_en_translation_preprocessor],\n",
        "    # Lowercase targets before computing metrics.\n",
        "\n",
        "    #postprocess_fn=t5.data.postprocessors.lower_text, \n",
        "\n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "\n",
        "    metric_fns=[t5.evaluation.metrics.bleu],\n",
        "\n",
        "    # Not required, but helps for mixing and auto-caching.\n",
        "    num_input_examples=msa_en_example_count,\n",
        "    # output_features\n",
        "    output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(filepath)),\n",
        "    # specifying token processor\n",
        "    token_preprocessor=[\n",
        "      functools.partial(\n",
        "          preprocessors.select_random_chunk,\n",
        "          feature_key=\"targets\",\n",
        "          max_length=65536\n",
        "      ),\n",
        "      functools.partial(\n",
        "          preprocessors.reduce_concat_tokens,\n",
        "          feature_key=\"targets\",\n",
        "          batch_size=128\n",
        "      ),\n",
        "      preprocessors.split_tokens_to_inputs_length,\n",
        "      functools.partial(\n",
        "          preprocessors.denoise,\n",
        "          inputs_fn=preprocessors.noise_span_to_unique_sentinel,\n",
        "          targets_fn=preprocessors.nonnoise_span_to_unique_sentinel,\n",
        "          noise_density=0.15,\n",
        "          noise_mask_fn=preprocessors.iid_noise_mask,\n",
        "      )\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGr8wt7naDRL",
        "colab_type": "text"
      },
      "source": [
        "###Levantine to MSA Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e3A3wQdjagH6",
        "colab": {}
      },
      "source": [
        "lav_msa_split_csv_path = {\n",
        "    \"train\": \"data/train/train_lav_msa.csv\",\n",
        "    \"validation\": \"data/val/val_lav_msa.csv\"\n",
        "}\n",
        "lav_msa_example_count = {\n",
        "    \"train\": len(lav_msa_train),\n",
        "    \"validation\": len(lav_msa_val)\n",
        "}"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xCK_j-9BagH_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "403ccb44-5b53-4657-8849-86bc3af088f3"
      },
      "source": [
        "def lav_msa_translation_dataset_fn(split, shuffle_files=False):\n",
        "  ds = tf.data.TextLineDataset(lav_msa_split_csv_path[split])\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"\",\"\"],\n",
        "                        field_delim=\",\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "  )\n",
        "  ds = ds.map(lambda *example: dict(zip([\"source\", \"target\"], example)) )\n",
        "  return ds\n",
        "\n",
        "for example in tfds.as_numpy(lav_msa_translation_dataset_fn(\"train\").take(5)):\n",
        "    print(example)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'source': b'\\xd8\\xa8\\xd8\\xb5\\xd9\\x81\\xd9\\x87 \\xd8\\xb9\\xd8\\xa7\\xd9\\x85\\xd9\\x87 \\xd8\\xa7\\xd9\\x84\\xd8\\xb3\\xd8\\xad\\xd8\\xb1 \\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd8\\xb3\\xd9\\x88\\xd8\\xaf \\xd9\\x81\\xd9\\x89 \\xd9\\x87\\xd8\\xb0\\xd9\\x87 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd9\\x86\\xd8\\xa7\\xd8\\xb7\\xd9\\x82 \\xd9\\x8a\\xd8\\xb9\\xd8\\xaa\\xd9\\x85\\xd8\\xaf \\xd8\\xb9\\xd9\\x84\\xd9\\x89 \\xd8\\xa7\\xd9\\x84\\xd9\\x83\\xd8\\xab\\xd9\\x8a\\xd8\\xb1 \\xd9\\x85\\xd9\\x86 \\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd8\\xb4\\xd9\\x8a\\xd8\\xa7\\xd8\\xa1 ', 'target': b'\\xd8\\xa8\\xd8\\xb4\\xd9\\x83\\xd9\\x84 \\xd8\\xb9\\xd8\\xa7\\xd9\\x85 \\xd9\\x88\\xd9\\x83\\xd8\\xa7\\xd9\\x86 \\xd8\\xa7\\xd9\\x84\\xd8\\xb3\\xd8\\xad\\xd8\\xb1 \\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd8\\xb3\\xd9\\x88\\xd8\\xaf \\xd9\\x81\\xd9\\x8a \\xd8\\xaa\\xd9\\x84\\xd9\\x83 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd9\\x86\\xd8\\xa7\\xd8\\xb7\\xd9\\x82 \\xd9\\x8a\\xd8\\xb9\\xd8\\xaa\\xd9\\x85\\xd8\\xaf \\xd8\\xb9\\xd9\\x84\\xd9\\x89 \\xd9\\x83\\xd8\\xab\\xd9\\x8a\\xd8\\xb1 \\xd9\\x85\\xd9\\x86 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd9\\x85\\xd8\\xa7\\xd8\\xb1\\xd8\\xb3\\xd8\\xa7\\xd8\\xaa'}\n",
            "{'source': b'\\xd9\\x81\\xd9\\x87\\xd9\\x85\\xd8\\xaa \\xd9\\x83\\xd9\\x8a\\xd9\\x81', 'target': b'\\xd9\\x81\\xd9\\x87\\xd9\\x85\\xd8\\xaa \\xd9\\x83\\xd9\\x8a\\xd9\\x81'}\n",
            "{'source': b'\\xd9\\x86\\xd8\\xb9\\xd9\\x85 \\xd8\\xb0\\xd9\\x84\\xd9\\x83 \\xd8\\xac\\xd9\\x8a\\xd8\\xaf  \\xd8\\xa7\\xd9\\x86\\xd9\\x87 \\xd9\\x8a\\xd8\\xb4\\xd8\\xa8\\xd9\\x87 \\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xb7\\xd8\\xb1  \\xd8\\xa7\\xd9\\x84\\xd8\\xb0\\xd9\\x8a \\xd8\\xa7\\xd8\\xad\\xd8\\xb6\\xd8\\xb1\\xd9\\x87 \\xd8\\xa7\\xd8\\xa8\\xd9\\x8a \\xd9\\x85\\xd9\\x86 \\xd9\\x81\\xd8\\xb1\\xd9\\x86\\xd8\\xb3\\xd8\\xa7', 'target': b'\\xd8\\xa7\\xd9\\x8a \\xd9\\x87\\xd8\\xa7\\xd8\\xaf \\xd9\\x85\\xd9\\x86\\xd9\\x8a\\xd8\\xad \\xd8\\xa8\\xd8\\xaa\\xd8\\xb4\\xd8\\xa8\\xd9\\x87 \\xd8\\xb1\\xd9\\x8a\\xd8\\xad\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xb7\\xd8\\xb1 \\xd8\\xa7\\xd9\\x84\\xd9\\x84\\xd9\\x8a \\xd8\\xac\\xd8\\xa7\\xd8\\xa8\\xd9\\x88 \\xd8\\xa8\\xd8\\xa7\\xd8\\xa8\\xd8\\xa7 \\xd9\\x85\\xd9\\x86 \\xd9\\x81\\xd8\\xb1\\xd9\\x86\\xd8\\xb3\\xd8\\xa7'}\n",
            "{'source': b'\\xd8\\xb9\\xd8\\xa7\\xd8\\xaf\\xd9\\x8a  \\xd8\\xa7\\xd9\\x84\\xd8\\xab\\xd9\\x8a\\xd8\\xa7\\xd8\\xa8 \\xd8\\xa7\\xd9\\x88 \\xd8\\xb4\\xd9\\x8a\\xd8\\xa1 \\xd8\\xa7\\xd8\\xae\\xd8\\xb1', 'target': b'\\xd8\\xb9\\xd8\\xa7\\xd8\\xaf\\xd9\\x8a \\xd8\\xa7\\xd9\\x84\\xd8\\xab\\xd9\\x8a\\xd8\\xa7\\xd8\\xa8 \\xd8\\xa7\\xd9\\x88 \\xd8\\xb4\\xd9\\x8a'}\n",
            "{'source': b'\\xd9\\x84\\xd8\\xa7\\xd9\\x86 \\xd8\\xa7\\xd9\\x84\\xd8\\xa8\\xd9\\x8a\\xd9\\x88\\xd9\\x84\\xd9\\x88\\xd8\\xac\\xd9\\x8a\\xd8\\xa7 \\xd9\\x84\\xd9\\x85\\xd8\\xa7\\xd8\\xb0\\xd8\\xa7 \\xd8\\xa7\\xd9\\x84\\xd8\\xa8\\xd9\\x8a\\xd9\\x88\\xd9\\x84\\xd9\\x88\\xd8\\xac\\xd9\\x8a\\xd8\\xa7  \\xd8\\xa7\\xd8\\xb3\\xd8\\xaa\\xd8\\xb7\\xd9\\x8a\\xd8\\xb9 \\xd9\\x85\\xd8\\xab\\xd9\\x84\\xd8\\xa7 \\xd8\\xa7\\xd9\\x86 \\xd8\\xa7\\xd8\\xb9\\xd9\\x85\\xd9\\x84 \\xd9\\x81\\xd9\\x8a \\xd8\\xb5\\xd9\\x8a\\xd8\\xaf\\xd9\\x84\\xd9\\x8a\\xd8\\xa9 ', 'target': b'\\xd8\\xb9\\xd8\\xb4\\xd8\\xa7\\xd9\\x86 \\xd8\\xa7\\xd9\\x84\\xd8\\xa8\\xd9\\x8a\\xd9\\x88\\xd9\\x84\\xd9\\x88\\xd8\\xac\\xd9\\x8a\\xd8\\xa7 \\xd9\\x84\\xd9\\x8a\\xd8\\xb4 \\xd8\\xa7\\xd9\\x84\\xd8\\xa8\\xd9\\x8a\\xd9\\x88\\xd9\\x84\\xd9\\x88\\xd8\\xac\\xd9\\x8a\\xd8\\xa7 \\xd8\\xa8\\xd9\\x82\\xd8\\xaf\\xd8\\xb1 \\xd8\\xa7\\xd9\\x86\\xd9\\x8a \\xd8\\xa7\\xd8\\xb4\\xd8\\xaa\\xd8\\xba\\xd9\\x84 \\xd8\\xb9\\xd9\\x86\\xd8\\xaf \\xd8\\xb5\\xd9\\x8a\\xd8\\xaf\\xd9\\x84\\xd9\\x8a\\xd8\\xa9 \\xd9\\x85\\xd8\\xab\\xd9\\x84\\xd8\\xa7'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fW9eNYoGagID",
        "colab": {}
      },
      "source": [
        "#turn the ds of dictionaries and change the keys to inputs and targets that the model\n",
        "def lav_msa_translation_preprocessor(ds):\n",
        "  def to_inputs_and_targets(ex):\n",
        "    return{\n",
        "        \"inputs\": tf.strings.join([\"translate Levantine to MSA: \",ex[\"source\"]]),\n",
        "        \"targets\": ex[\"target\"]\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mDoUsP1AagIG",
        "colab": {}
      },
      "source": [
        "t5.data.TaskRegistry.remove(\"translation_lav_msa\")\n",
        "t5.data.TaskRegistry.add(\n",
        "    #name of the Task\n",
        "    \"translation_lav_msa\",\n",
        "    #Supply a function which returns a tf.data.Dataset\n",
        "    dataset_fn=lav_msa_translation_dataset_fn,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[lav_msa_translation_preprocessor],\n",
        "    # Lowercase targets before computing metrics.\n",
        "\n",
        "    #postprocess_fn = t5.data.postprocessors.lower_text, \n",
        "\n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "\n",
        "    metric_fns=[t5.evaluation.metrics.bleu],\n",
        "\n",
        "    # Not required, but helps for mixing and auto-caching.\n",
        "    num_input_examples=lav_msa_example_count,\n",
        "    # output_features\n",
        "    output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(filepath)),\n",
        "    # specifying token processor\n",
        "    token_preprocessor=[\n",
        "      functools.partial(\n",
        "          preprocessors.select_random_chunk,\n",
        "          feature_key=\"targets\",\n",
        "          max_length=65536\n",
        "      ),\n",
        "      functools.partial(\n",
        "          preprocessors.reduce_concat_tokens,\n",
        "          feature_key=\"targets\",\n",
        "          batch_size=128\n",
        "      ),\n",
        "      preprocessors.split_tokens_to_inputs_length,\n",
        "      functools.partial(\n",
        "          preprocessors.denoise,\n",
        "          inputs_fn=preprocessors.noise_span_to_unique_sentinel,\n",
        "          targets_fn=preprocessors.nonnoise_span_to_unique_sentinel,\n",
        "          noise_density=0.15,\n",
        "          noise_mask_fn=preprocessors.iid_noise_mask,\n",
        "      )\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wIK8EWyaLFR",
        "colab_type": "text"
      },
      "source": [
        "###Maghrib to MSA Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bVhYmQnQaj2K",
        "colab": {}
      },
      "source": [
        "mag_msa_split_csv_path = {\n",
        "    \"train\": \"data/train/train_mag_msa.csv\",\n",
        "    \"validation\": \"data/val/val_mag_msa.csv\"\n",
        "}\n",
        "mag_msa_example_count = {\n",
        "    \"train\": len(mag_msa_train),\n",
        "    \"validation\": len(mag_msa_val)\n",
        "}"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WBLF-J2Oaj2T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "c135c6f6-8d07-4a8e-dc33-a12a54988e48"
      },
      "source": [
        "def mag_msa_translation_dataset_fn(split, shuffle_files=False):\n",
        "  ds = tf.data.TextLineDataset(mag_msa_split_csv_path[split])\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"\",\"\"],\n",
        "                        field_delim=\",\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "  )\n",
        "  ds = ds.map(lambda *example: dict(zip([\"source\", \"target\"], example)) )\n",
        "  return ds\n",
        "\n",
        "for example in tfds.as_numpy(mag_msa_translation_dataset_fn(\"train\").take(5)):\n",
        "    print(example)\n",
        "    print(example['source'].decode())\n",
        "    print(example['target'].decode())\n",
        "    print(len(example['source']))\n",
        "    print(len(example['target']))"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'source': b'\\xd9\\x85\\xd9\\x8a\\xd9\\x85\\xd9\\x8a \\xd8\\xaf\\xd8\\xb9\\xd9\\x8a \\xd9\\x87\\xd8\\xa7\\xd8\\xaa\\xd9\\x81\\xd9\\x83 \\xd9\\x88 \\xd8\\xb4\\xd8\\xa7\\xd9\\x86\\xd9\\x87 \\xd9\\x84\\xd9\\x82\\xd8\\xaf \\xd8\\xac\\xd9\\x86 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xb3\\xd9\\x83\\xd9\\x8a\\xd9\\x86', 'target': b'\\xd9\\x85\\xd9\\x8a\\xd9\\x85\\xd9\\x8a \\xd8\\xae\\xd9\\x84\\xd9\\x8a \\xd8\\xa7\\xd9\\x84\\xd8\\xaa\\xd9\\x84\\xd9\\x8a\\xd9\\x81\\xd9\\x88\\xd9\\x86 \\xd9\\x88 \\xd8\\xae\\xd9\\x84\\xd9\\x8a\\xd9\\x87 \\xd8\\xb9\\xd9\\x84\\xd9\\x89 \\xd8\\xae\\xd8\\xa7\\xd8\\xb7\\xd8\\xb1\\xd9\\x88 \\xd8\\xb1\\xd8\\xa7\\xd9\\x87 \\xd9\\x85\\xd8\\xb3\\xd9\\x83\\xd9\\x8a\\xd9\\x86 \\xd8\\xad\\xd9\\x85\\xd9\\x82\\xd8\\xaa\\xd9\\x8a\\xd9\\x87'}\n",
            "ميمي دعي هاتفك و شانه لقد جن المسكين\n",
            "ميمي خلي التليفون و خليه على خاطرو راه مسكين حمقتيه\n",
            "65\n",
            "93\n",
            "{'source': b'\\xd9\\x83\\xd9\\x85\\xd8\\xa7 \\xd9\\x82\\xd9\\x84\\xd8\\xaa \\xd9\\x87\\xd8\\xb0\\xd8\\xa7 \\xd9\\x83\\xd8\\xa7\\xd9\\x81 \\xd8\\xac\\xd8\\xaf\\xd8\\xa7 \\xd9\\x84\\xd8\\xa7\\xd9\\x86\\xd9\\x83 \\xd8\\xb3\\xd8\\xaa\\xd8\\xb1\\xd8\\xa7\\xd9\\x81\\xd9\\x82\\xd9\\x8a\\xd9\\x86 \\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xb1\\xd9\\x88\\xd8\\xb3  \\xd9\\x88 \\xd9\\x81\\xd9\\x8a \\xd8\\xa7\\xd9\\x84\\xd9\\x84\\xd9\\x8a\\xd9\\x84 \\xd8\\xaa\\xd8\\xb1\\xd8\\xaa\\xd8\\xaf\\xd9\\x8a\\xd9\\x86  \\xd8\\xa7\\xd8\\xae\\xd8\\xb1 \\xd8\\xa7 \\xd9\\x88 \\xd8\\xa7\\xd9\\x86\\xd8\\xaa\\xd9\\x87\\xd9\\x89 \\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd9\\x85\\xd8\\xb1', 'target': b'\\xd9\\x83\\xd9\\x8a\\xd9\\x81 \\xd9\\x82\\xd9\\x84\\xd8\\xaa \\xd9\\x87\\xd8\\xaf\\xd8\\xb4\\xd9\\x8a \\xd9\\x83\\xd8\\xa7\\xd9\\x81\\xd9\\x8a \\xd8\\xad\\xd9\\x8a\\xd8\\xaa \\xd8\\xba\\xd8\\xa7\\xd8\\xaf\\xd9\\x8a \\xd8\\xaa\\xd9\\x85\\xd8\\xb4\\xd9\\x8a \\xd9\\x85\\xd8\\xb9 \\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xb1\\xd9\\x88\\xd8\\xb3 \\xd9\\x88 \\xd9\\x81 \\xd8\\xa7\\xd9\\x84\\xd9\\x84\\xd9\\x8a\\xd9\\x84 \\xd9\\x86\\xd8\\xaa\\xd9\\x8a \\xd9\\x84\\xd8\\xa7\\xd8\\xa8\\xd8\\xb3\\xd8\\xa9 \\xd9\\x88\\xd8\\xa7\\xd8\\xad\\xd8\\xaf \\xd8\\xa7\\xd8\\xae\\xd8\\xb1 \\xd9\\x88 \\xd8\\xb3\\xd8\\xa7\\xd9\\x84\\xd9\\x8a\\xd9\\x86\\xd8\\xa7'}\n",
            "كما قلت هذا كاف جدا لانك سترافقين العروس  و في الليل ترتدين  اخر ا و انتهى الامر\n",
            "كيف قلت هدشي كافي حيت غادي تمشي مع العروس و ف الليل نتي لابسة واحد اخر و سالينا\n",
            "142\n",
            "141\n",
            "{'source': b'\\xd8\\xa7\\xd9\\x86\\xd8\\xa7 \\xd8\\xa7\\xd8\\xad\\xd8\\xaf\\xd8\\xab\\xd9\\x83  \\xd8\\xa7\\xd8\\xb0\\xd9\\x87\\xd8\\xa8\\xd9\\x8a \\xd9\\x88 \\xd8\\xa7\\xd8\\xa8\\xd8\\xad\\xd8\\xab\\xd9\\x8a \\xd8\\xb9\\xd9\\x86 \\xd8\\xb9\\xd9\\x85\\xd9\\x84  \\xd8\\xa7\\xd8\\xb9\\xd9\\x85\\xd9\\x84\\xd9\\x8a', 'target': b'\\xd8\\xa7\\xd9\\x86\\xd8\\xa7 \\xd9\\x83\\xd9\\x86\\xd9\\x87\\xd8\\xb6\\xd8\\xb1 \\xd9\\x85\\xd8\\xb9\\xd9\\x83 \\xd8\\xb3\\xd9\\x8a\\xd8\\xb1\\xd9\\x8a \\xd9\\x88\\xd9\\x82\\xd9\\x84\\xd8\\xa8\\xd9\\x8a \\xd8\\xb9\\xd9\\x84\\xd9\\x89 \\xd8\\xae\\xd8\\xaf\\xd9\\x85\\xd8\\xa9'}\n",
            "انا احدثك  اذهبي و ابحثي عن عمل  اعملي\n",
            "انا كنهضر معك سيري وقلبي على خدمة\n",
            "67\n",
            "60\n",
            "{'source': b'\\xd8\\xa7\\xd8\\xaa\\xd9\\x85\\xd9\\x86\\xd9\\x89 \\xd9\\x84\\xd9\\x87\\xd8\\xa7 \\xd8\\xa7\\xd9\\x84\\xd8\\xaa\\xd9\\x88\\xd9\\x81\\xd9\\x8a\\xd9\\x82', 'target': b'\\xd8\\xa8\\xd8\\xa7\\xd9\\x84\\xd8\\xaa\\xd9\\x88\\xd9\\x81\\xd9\\x8a\\xd9\\x82'}\n",
            "اتمنى لها التوفيق\n",
            "بالتوفيق\n",
            "32\n",
            "16\n",
            "{'source': b'\\xd8\\xa7\\xd9\\x86\\xd8\\xa7 \\xd8\\xa7\\xd9\\x82\\xd9\\x88\\xd9\\x84 \\xd9\\x84\\xd9\\x87\\xd8\\xa7 \\xd8\\xb9\\xd8\\xa7\\xd8\\xa6\\xd8\\xb4\\xd8\\xa9 \\xd8\\xa7\\xd9\\x86\\xd8\\xa7 \\xd9\\x85\\xd8\\xb1\\xd9\\x8a\\xd8\\xb6\\xd8\\xa9 \\xd9\\x88 \\xd9\\x87\\xd9\\x8a \\xd8\\xaa\\xd9\\x82\\xd9\\x88\\xd9\\x84 \\xd8\\xa7\\xd8\\xb0\\xd9\\x87\\xd8\\xa8\\xd9\\x8a \\xd9\\x81\\xd8\\xad\\xd8\\xb3\\xd8\\xa8', 'target': b'\\xd8\\xa7\\xd9\\x86\\xd8\\xa7 \\xd8\\xaa\\xd9\\x86\\xd9\\x82\\xd9\\x88\\xd9\\x84 \\xd9\\x84\\xd9\\x8a\\xd9\\x87\\xd8\\xa7 \\xd8\\xb9\\xd8\\xa7\\xd8\\xa1\\xd8\\xb4\\xd8\\xa9 \\xd8\\xa7\\xd9\\x86\\xd8\\xa7 \\xd9\\x85\\xd8\\xb1\\xd9\\x8a\\xd8\\xb6\\xd8\\xa9 \\xd9\\x88 \\xd9\\x87\\xd9\\x8a \\xd8\\xaa\\xd8\\xaa\\xd9\\x82\\xd9\\x88\\xd9\\x84 \\xd9\\x84\\xd9\\x8a \\xd8\\xb5\\xd8\\xa7\\xd9\\x81\\xd9\\x8a \\xd8\\xb3\\xd9\\x8a\\xd8\\xb1\\xd9\\x8a'}\n",
            "انا اقول لها عائشة انا مريضة و هي تقول اذهبي فحسب\n",
            "انا تنقول ليها عاءشة انا مريضة و هي تتقول لي صافي سيري\n",
            "88\n",
            "97\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GgyqkGfYaj2Y",
        "colab": {}
      },
      "source": [
        "  #turn the ds of dictionaries and change the keys to inputs and targets that the model\n",
        "def mag_msa_translation_preprocessor(ds):\n",
        "    def to_inputs_and_targets(ex):\n",
        "      return{\n",
        "          \"inputs\": tf.strings.join([\"translate Maghrib to MSA: \",ex[\"source\"]]),\n",
        "          \"targets\": ex[\"target\"]\n",
        "      }\n",
        "    return ds.map(to_inputs_and_targets, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bmja1M2Eaj2f",
        "colab": {}
      },
      "source": [
        "t5.data.TaskRegistry.remove(\"translation_mag_msa\")\n",
        "t5.data.TaskRegistry.add(\n",
        "    #name of the Task\n",
        "    \"translation_mag_msa\",\n",
        "    #Supply a function which returns a tf.data.Dataset\n",
        "    dataset_fn=mag_msa_translation_dataset_fn,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[mag_msa_translation_preprocessor],\n",
        "    # Lowercase targets before computing metrics.\n",
        "\n",
        "    #postprocess_fn = t5.data.postprocessors.lower_text, \n",
        "    \n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    \n",
        "    metric_fns=[t5.evaluation.metrics.bleu],\n",
        "    \n",
        "    # Not required, but helps for mixing and auto-caching.\n",
        "    num_input_examples=mag_msa_example_count,\n",
        "    # output_features\n",
        "    output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(filepath)),\n",
        "    # specifying token processor\n",
        "    token_preprocessor=[\n",
        "      functools.partial(\n",
        "          preprocessors.select_random_chunk,\n",
        "          feature_key=\"targets\",\n",
        "          max_length=65536\n",
        "      ),\n",
        "      functools.partial(\n",
        "          preprocessors.reduce_concat_tokens,\n",
        "          feature_key=\"targets\",\n",
        "          batch_size=128\n",
        "      ),\n",
        "      preprocessors.split_tokens_to_inputs_length,\n",
        "      functools.partial(\n",
        "          preprocessors.denoise,\n",
        "          inputs_fn=preprocessors.noise_span_to_unique_sentinel,\n",
        "          targets_fn=preprocessors.nonnoise_span_to_unique_sentinel,\n",
        "          noise_density=0.15,\n",
        "          noise_mask_fn=preprocessors.iid_noise_mask,\n",
        "      )\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLoe7vbFV42o",
        "colab_type": "text"
      },
      "source": [
        "##Dataset Mixture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWMTtMsXWAua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t5.data.MixtureRegistry.remove(\"translation_msa\")\n",
        "t5.data.MixtureRegistry.add(\n",
        "    \"translation_msa\",\n",
        "    [\"translation_msa_en\", \"translation_lav_msa\", \"translation_mag_msa\"],\n",
        "     default_rate=1.0\n",
        ")"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc0bSgP1WenA",
        "colab_type": "text"
      },
      "source": [
        "##Pre-Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw146D5HkkPi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "588da9fd-b97e-4a80-dbe8-7680aff0f90c"
      },
      "source": [
        "#gotta get the base config and add the new tasks' task params\n",
        "!wget \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-config.json\" -O data/config/t5-base-config.json"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-29 02:21:35--  https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-config.json\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.240.142\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.240.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1199 (1.2K) [application/json]\n",
            "Saving to: ‘data/config/t5-base-config.json’\n",
            "\n",
            "\r          data/conf   0%[                    ]       0  --.-KB/s               \rdata/config/t5-base 100%[===================>]   1.17K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-08-29 02:21:35 (64.2 MB/s) - ‘data/config/t5-base-config.json’ saved [1199/1199]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uP2fwStXK5b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a7780e77-69d4-422e-afe2-1978c4ebd5e1"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "#Using the base config from Huggingface T5 Model\n",
        "config = transformers.T5Config.from_json_file(json_file=\"data/config/t5-base-config.json\")\n",
        "model = t5.models.HfPyTorchModel(config, \"/tmp/hft5/\", device)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Loading from /tmp/hft5/model-10000.checkpoint\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M23Y4l0tb4qM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "7cdddb0c-9972-4abf-8fcd-bd2cb84a7d08"
      },
      "source": [
        "ls /tmp/hft5"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "events.out.tfevents.1598661619.6f0b5dc6b701.111.0  model-2000.checkpoint\n",
            "events.out.tfevents.1598667702.6f0b5dc6b701.111.1  model-4000.checkpoint\n",
            "example_predictions.txt                            model-6000.checkpoint\n",
            "model-0.checkpoint                                 model-8000.checkpoint\n",
            "model-10000.checkpoint                             \u001b[0m\u001b[01;34mvalidation_eval\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VV2RSZGxQxYT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "f343a213-d897-4c7b-c026-2b7e5ea9807b"
      },
      "source": [
        " !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1pjeI4EBOjES-gZ-s4rdVlKCAqWYV5FU7' -O data/operative_config.gin"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-29 04:02:44--  https://docs.google.com/uc?export=download&id=1pjeI4EBOjES-gZ-s4rdVlKCAqWYV5FU7\n",
            "Resolving docs.google.com (docs.google.com)... 172.217.212.139, 172.217.212.113, 172.217.212.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.212.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0g-7c-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/hl75ak7eeggfbhshmat6ql76hgvth4kb/1598673750000/01542315482457206469/*/1pjeI4EBOjES-gZ-s4rdVlKCAqWYV5FU7?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-08-29 04:02:45--  https://doc-0g-7c-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/hl75ak7eeggfbhshmat6ql76hgvth4kb/1598673750000/01542315482457206469/*/1pjeI4EBOjES-gZ-s4rdVlKCAqWYV5FU7?e=download\n",
            "Resolving doc-0g-7c-docs.googleusercontent.com (doc-0g-7c-docs.googleusercontent.com)... 172.217.214.132, 2607:f8b0:4001:c05::84\n",
            "Connecting to doc-0g-7c-docs.googleusercontent.com (doc-0g-7c-docs.googleusercontent.com)|172.217.214.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10261 (10K) [application/octet-stream]\n",
            "Saving to: ‘data/operative_config.gin’\n",
            "\n",
            "data/operative_conf 100%[===================>]  10.02K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-08-29 04:02:45 (70.8 MB/s) - ‘data/operative_config.gin’ saved [10261/10261]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4ZqkAzufRI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " import gin\n",
        " with gin.unlock_config():\n",
        "   gin.parse_config_file(\"data/operative_config.gin\")"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6f5uUWXWUKw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "601e6646-22e8-4c19-fc59-979433b305fd"
      },
      "source": [
        "STEPS = 10000 #@param {type: \"integer\"}\n",
        "model.train(\n",
        "    mixture_or_task_name=\"translation_msa\",\n",
        "    steps=STEPS,\n",
        "    save_steps=STEPS/5,                                                   \n",
        "    sequence_length={\"inputs\": 32, \"targets\": 32},\n",
        "    split=\"train\",\n",
        "    batch_size=32,\n",
        "    optimizer=functools.partial(transformers.AdamW, lr=1e-4),\n",
        ")"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/t5/data/utils.py:273: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  return dataset.map(my_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
            "INFO:absl:Saving checkpoint for step 13045\n",
            "INFO:absl:Saving checkpoint for step 13245\n",
            "INFO:absl:Saving checkpoint for step 13445\n",
            "INFO:absl:Saving checkpoint for step 13645\n",
            "INFO:absl:Saving checkpoint for step 13845\n",
            "INFO:absl:Saving final checkpoint for step 14045\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Uv-8hgBo8Lb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        },
        "outputId": "42c6a2a8-9704-4ebb-c66d-d352dfe5e148"
      },
      "source": [
        "review_task = t5.data.TaskRegistry.get(\"translation_msa_en\")\n",
        "ds = review_task.get_dataset(split=\"validation\", sequence_length={\"inputs\": 128, \"targets\": 32})\n",
        "print(\"A few preprocessed validation examples...\")\n",
        "for ex in tfds.as_numpy(ds.take(5)):\n",
        "  print(ex['inputs'])"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/t5/data/utils.py:273: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  return dataset.map(my_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "A few preprocessed validation examples...\n",
            "[    5     2 17063  1416   487   118 18672 24734   380 24733    32 24732\n",
            "    51  3016 24731   170    61     5     2  5295    62   208  2013  2913\n",
            "   118   208 24730  1849  8727   171   208 24729   170  6403  1577   321\n",
            "   154  3465   208 24728  2191 24727  1491   260    54   411    61  4135\n",
            "  4608    62   116   281    36    82  4615  4514  2638   388   828   332\n",
            " 24726    82 24725    32   199  4195  8711   293 24724    39   864 11270\n",
            "    51   154  4514  2638   388 11287    39   781  1375   284    32 24723\n",
            "   209    72  8081 12240  1186 17178 12196 24722    32   969 11483   208\n",
            " 24721   199  2407 12723   118   751  3669   116    88  6004 24720  4334\n",
            "  1813   160   849 24719  1367   247 24718  1343 24717   208  2881   199\n",
            " 24716  2724 14601   118     1]\n",
            "[ 1037 24734   770   849    32   397    36    82 24733   247 10073  2638\n",
            "    32    36   309  2789   395 24732   293 24731 12753 24730   121   284\n",
            "  3799    32   184    72   840 24729   397    36    82   290 24728  4184\n",
            "   658    36    39   332   656     5  1161   116   281   624   781   116\n",
            "  9405   118    51 14593 24727  2019 24726   725    32    36 24725   284\n",
            "   597  2334  8711   116   332  1290   174    36    39    72  2324 19281\n",
            "   293    36    39  5412  1464   330    54  2543   577   864 14815   697\n",
            "  1034 13242  1472   302 24724  2984  1979   395  8927 24723   379    54\n",
            "  4318  3474   171     5     2  8914    62 24722    54   597    51   116\n",
            "    36    39  3985  4685 24721    32    36   216  1473  3542  2796    51\n",
            "     5 17240  2129    54   256   987  8122     1]\n",
            "[ 8764  1290   231 24734  6988   171   231  1412 24733    72  8237  4968\n",
            "  2583   469  5942    61  5569     1]\n",
            "[ 8568     5   432   332 22382   336   121  1558  3816  1463    32   199\n",
            "    72 11354 24734   432 24733 14588  1290   622    61  8987   160   242\n",
            "   254  2900  2115   174    36    39 24732   758  1043   528    39   830\n",
            " 24731  6753   160  1026 12674 13727  4206   728   309   116  2684   380\n",
            "   849   118   199    51   256  2250  2618   160   332  2180   174   290\n",
            " 24730    51 24729     2   336   121 24728   849    39    51   256  6354\n",
            "   118  1349 24727   170   223    72 13258  1814  1917  1349 24726  5237\n",
            "    39  2384 21268  1326 24725    54  1101   171   601 24724   528    39\n",
            "   830    88 17779     2 10798   150 24723  1393 24722    39  4968  2583\n",
            "  6938   380  5279     5     2  1028   292   830 24721    62  3807 18960\n",
            " 19415  5978   199 24720   121     1]\n",
            "[  414    61  1416 11450  1200   174  2737 24734   332 24733    36    39\n",
            " 10929  9478   260    54   758     5 24732  2593   223  1313    62   160\n",
            "   879  1436  1608  2554   622   699  1464   116 24731   284 17300   118\n",
            "   727  1192 24730     2  1551 16092 16394    39   302    61  5095 24729\n",
            "   379    54   656   512    62   174  2441   321 19611 24728   302   254\n",
            "  8086   302 24727   336   121 24726  2240   292 16929   209   154   930\n",
            "    32  6356   231  8921    62   658    36    39   332 24725   770    62\n",
            " 24724 16057   190   512    51 24723 15933 24722  8588  1488 24721   828\n",
            " 24720   121  1487 24719  9976   728 24718    54   849    51 24717    88\n",
            "  1730   330   242    54  2019   209 24716    32   411   553   116   379\n",
            "    51   725 24715     1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev_E4ZadXxmj",
        "colab_type": "text"
      },
      "source": [
        "##Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjZnTOIrYEh4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "26960658-a5df-477e-be67-bae18462c0f8"
      },
      "source": [
        "# Evaluate after fine-tuning\n",
        "model.eval(\n",
        "    mixture_or_task_name=\"translation_msa\",\n",
        "    checkpoint_steps= STEPS,\n",
        "    sequence_length={\"inputs\": 32, \"targets\": 32},\n",
        "    batch_size=32,\n",
        ")"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Loading from /tmp/hft5/model-14045.checkpoint\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-131-e53c8da7d50b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcheckpoint_steps\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m14045\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"inputs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"targets\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/t5/models/hf_model.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, mixture_or_task_name, sequence_length, batch_size, checkpoint_steps, summary_dir, split, **generate_kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcheckpoint_step\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcheckpoint_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0m_eval_current_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m   def predict(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/t5/models/hf_model.py\u001b[0m in \u001b[0;36m_eval_current_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    436\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m         \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_examples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_targets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'translation_msa_en'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF-3_4u1X-5r",
        "colab_type": "text"
      },
      "source": [
        "##Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL5yLXs4YJw7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "3f597e2c-6f3e-4f4d-90d0-3a2fe5f63824"
      },
      "source": [
        "inputs = [\n",
        "    \"translate Levantine to MSA: هلا كيفك\", #Hey how are you\n",
        "    \"translate MSA to English: لا اريد ان انام\", #I don't want to sleep\n",
        "]\n",
        "model.predict(\n",
        "    inputs,\n",
        "    sequence_length={\"inputs\": 32},\n",
        "    batch_size=2,\n",
        "    output_file=\"/tmp/hft5/example_predictions.txt\",\n",
        "    vocabulary=t5.data.SentencePieceVocabulary(filepath),\n",
        ")"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/t5/models/hf_model.py:549: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  num_parallel_calls=t5.data.preprocessors.num_parallel_calls()\n",
            "INFO:absl:translate Levantine to MSA: هلا كيفك\n",
            "  -> تيغني\n",
            "INFO:absl:translate MSA to English: لا اريد ان انام\n",
            "  -> تيغني وT\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}