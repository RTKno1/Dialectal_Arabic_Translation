{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:fastai] *",
      "language": "python",
      "name": "conda-env-fastai-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BQflUKhjIKNp"
      },
      "source": [
        "### Notes \n",
        "\n",
        "T5 Paper: https://arxiv.org/pdf/1910.10683.pdf\n",
        "\n",
        "T5 Tokenizer: https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_t5.py\n",
        "\n",
        "Important Tasks: https://docs.google.com/document/d/1weIZM6QTlnitpPQmpg-WeV2RW70TnYmDuogBQPr5mB0/edit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E93edKzBw_WE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "34d156dd-7db6-43b3-8c8b-13a62240c470"
      },
      "source": [
        "#installation step\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "#creating the folders \n",
        "!mkdir data/\n",
        "!mkdir data/AD_NMT-master\n",
        "!mkdir data/train\n",
        "!mkdir data/test\n",
        "!mkdir data/val\n",
        "#fetching the pkl files\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW' -O data/AD_NMT-master/english-Arabic-both.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1V8_tp8ZlWUYaX7QQL46t0uSRNrVehSf1' -O data/AD_NMT-master/english-Arabic-test.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1V7X0qtuDIyjTHY0wh-ZNoVwsiF4lId2e' -O data/AD_NMT-master/english-Arabic-train.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV' -O data/AD_NMT-master/LAV-MSA-2-both.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UpfCbkxhztof7dvNjeAs1bHjD4SER6h3' -O data/AD_NMT-master/LAV-MSA-2-test.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UlAZGtYsSfXzK7hrC_PbxQFqTSXD0DMw' -O data/AD_NMT-master/LAV-MSA-2-train.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UjDX7cCG2S23SPfSHxSPdVayMTxB5Y16' -O data/AD_NMT-master/Magribi_MSA-both.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UaVWIqRXo0rxuxDF4KArA4bEK1TaLX3l' -O data/AD_NMT-master/Magribi_MSA-test.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UYvlhdYAdfa4riP_4hn3-IEVd1ZUXVTQ' -O data/AD_NMT-master/Magribi_MSA-train.pkl"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\r\u001b[K     |▋                               | 10kB 17.1MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |█▊                              | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |██▎                             | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |███▌                            | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |████                            | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |████▋                           | 81kB 2.3MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 92kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 296kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 307kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 317kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 327kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 337kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 348kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 358kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 368kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 378kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 389kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 399kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 409kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 419kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 430kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 440kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 450kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 460kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 471kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 481kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 491kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 501kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 512kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 522kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 532kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 542kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 552kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 563kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 573kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 50.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.39)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 40.6MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/50/93509f906a40bffd7d175f97fd75ea328ad9bd91f48f59c4bd084c94a25e/sacremoses-0.0.41.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 36.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.39 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.39)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.41-cp36-none-any.whl size=893334 sha256=ee44e512a0fd398ee3e69656dd9128865e087e8d45ac222b7f0eb4d1d2e05e73\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/5a/d4/b020a81249de7dc63758a34222feaa668dbe8ebfe9170cc9b1\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.41 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.8.0\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.85)\n",
            "--2020-04-21 12:44:17--  https://docs.google.com/uc?export=download&id=1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW\n",
            "Resolving docs.google.com (docs.google.com)... 172.217.193.113, 172.217.193.100, 172.217.193.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.193.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-10-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/d5trsd0n6r58q15neln7p2vo80vlau07/1587473025000/16970776037313924126/*/1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-04-21 12:44:18--  https://doc-10-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/d5trsd0n6r58q15neln7p2vo80vlau07/1587473025000/16970776037313924126/*/1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW?e=download\n",
            "Resolving doc-10-2s-docs.googleusercontent.com (doc-10-2s-docs.googleusercontent.com)... 172.217.203.132, 2607:f8b0:400c:c07::84\n",
            "Connecting to doc-10-2s-docs.googleusercontent.com (doc-10-2s-docs.googleusercontent.com)|172.217.203.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 955428 (933K) [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/english-Arabic-both.pkl’\n",
            "\n",
            "data/AD_NMT-master/ 100%[===================>] 933.04K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2020-04-21 12:44:18 (140 MB/s) - ‘data/AD_NMT-master/english-Arabic-both.pkl’ saved [955428/955428]\n",
            "\n",
            "--2020-04-21 12:44:20--  https://docs.google.com/uc?export=download&id=1V8_tp8ZlWUYaX7QQL46t0uSRNrVehSf1\n",
            "Resolving docs.google.com (docs.google.com)... 172.217.204.101, 172.217.204.139, 172.217.204.102, ...\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.204.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-14-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/g3ekmhvpn9jrhbruql3t6cmhtaego2lc/1587473025000/16970776037313924126/*/1V8_tp8ZlWUYaX7QQL46t0uSRNrVehSf1?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-04-21 12:44:20--  https://doc-14-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/g3ekmhvpn9jrhbruql3t6cmhtaego2lc/1587473025000/16970776037313924126/*/1V8_tp8ZlWUYaX7QQL46t0uSRNrVehSf1?e=download\n",
            "Resolving doc-14-2s-docs.googleusercontent.com (doc-14-2s-docs.googleusercontent.com)... 172.217.203.132, 2607:f8b0:400c:c07::84\n",
            "Connecting to doc-14-2s-docs.googleusercontent.com (doc-14-2s-docs.googleusercontent.com)|172.217.203.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 95497 (93K) [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/english-Arabic-test.pkl’\n",
            "\n",
            "data/AD_NMT-master/ 100%[===================>]  93.26K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2020-04-21 12:44:20 (134 MB/s) - ‘data/AD_NMT-master/english-Arabic-test.pkl’ saved [95497/95497]\n",
            "\n",
            "--2020-04-21 12:44:21--  https://docs.google.com/uc?export=download&id=1V7X0qtuDIyjTHY0wh-ZNoVwsiF4lId2e\n",
            "Resolving docs.google.com (docs.google.com)... 173.194.216.101, 173.194.216.138, 173.194.216.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|173.194.216.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-10-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/gakas498c35iqc9km9osdrd0krahctar/1587473025000/16970776037313924126/*/1V7X0qtuDIyjTHY0wh-ZNoVwsiF4lId2e?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-04-21 12:44:22--  https://doc-10-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/gakas498c35iqc9km9osdrd0krahctar/1587473025000/16970776037313924126/*/1V7X0qtuDIyjTHY0wh-ZNoVwsiF4lId2e?e=download\n",
            "Resolving doc-10-2s-docs.googleusercontent.com (doc-10-2s-docs.googleusercontent.com)... 172.217.203.132, 2607:f8b0:400c:c07::84\n",
            "Connecting to doc-10-2s-docs.googleusercontent.com (doc-10-2s-docs.googleusercontent.com)|172.217.203.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 859172 (839K) [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/english-Arabic-train.pkl’\n",
            "\n",
            "data/AD_NMT-master/ 100%[===================>] 839.04K  --.-KB/s    in 0.006s  \n",
            "\n",
            "2020-04-21 12:44:22 (137 MB/s) - ‘data/AD_NMT-master/english-Arabic-train.pkl’ saved [859172/859172]\n",
            "\n",
            "--2020-04-21 12:44:24--  https://docs.google.com/uc?export=download&id=1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV\n",
            "Resolving docs.google.com (docs.google.com)... 173.194.216.100, 173.194.216.139, 173.194.216.102, ...\n",
            "Connecting to docs.google.com (docs.google.com)|173.194.216.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-00-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/asshkp5r4c1ere6ck255kabr7f2k2tja/1587473025000/16970776037313924126/*/1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-04-21 12:44:24--  https://doc-00-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/asshkp5r4c1ere6ck255kabr7f2k2tja/1587473025000/16970776037313924126/*/1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV?e=download\n",
            "Resolving doc-00-2s-docs.googleusercontent.com (doc-00-2s-docs.googleusercontent.com)... 172.217.203.132, 2607:f8b0:400c:c07::84\n",
            "Connecting to doc-00-2s-docs.googleusercontent.com (doc-00-2s-docs.googleusercontent.com)|172.217.203.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/LAV-MSA-2-both.pkl’\n",
            "\n",
            "data/AD_NMT-master/     [ <=>                ]   2.33M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-04-21 12:44:25 (172 MB/s) - ‘data/AD_NMT-master/LAV-MSA-2-both.pkl’ saved [2447014]\n",
            "\n",
            "--2020-04-21 12:44:26--  https://docs.google.com/uc?export=download&id=1UpfCbkxhztof7dvNjeAs1bHjD4SER6h3\n",
            "Resolving docs.google.com (docs.google.com)... 173.194.216.101, 173.194.216.138, 173.194.216.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|173.194.216.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0c-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/5hrbi7tqt11d35pmecdkmorqnublihib/1587473025000/16970776037313924126/*/1UpfCbkxhztof7dvNjeAs1bHjD4SER6h3?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-04-21 12:44:26--  https://doc-0c-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/5hrbi7tqt11d35pmecdkmorqnublihib/1587473025000/16970776037313924126/*/1UpfCbkxhztof7dvNjeAs1bHjD4SER6h3?e=download\n",
            "Resolving doc-0c-2s-docs.googleusercontent.com (doc-0c-2s-docs.googleusercontent.com)... 172.217.203.132, 2607:f8b0:400c:c07::84\n",
            "Connecting to doc-0c-2s-docs.googleusercontent.com (doc-0c-2s-docs.googleusercontent.com)|172.217.203.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 304332 (297K) [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/LAV-MSA-2-test.pkl’\n",
            "\n",
            "data/AD_NMT-master/ 100%[===================>] 297.20K  --.-KB/s    in 0.006s  \n",
            "\n",
            "2020-04-21 12:44:26 (48.7 MB/s) - ‘data/AD_NMT-master/LAV-MSA-2-test.pkl’ saved [304332/304332]\n",
            "\n",
            "--2020-04-21 12:44:28--  https://docs.google.com/uc?export=download&id=1UlAZGtYsSfXzK7hrC_PbxQFqTSXD0DMw\n",
            "Resolving docs.google.com (docs.google.com)... 172.217.203.102, 172.217.203.139, 172.217.203.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.203.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0c-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/fb79jri5mmqqgh0nq6a5giamhmlqcmkg/1587473025000/16970776037313924126/*/1UlAZGtYsSfXzK7hrC_PbxQFqTSXD0DMw?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-04-21 12:44:28--  https://doc-0c-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/fb79jri5mmqqgh0nq6a5giamhmlqcmkg/1587473025000/16970776037313924126/*/1UlAZGtYsSfXzK7hrC_PbxQFqTSXD0DMw?e=download\n",
            "Resolving doc-0c-2s-docs.googleusercontent.com (doc-0c-2s-docs.googleusercontent.com)... 172.217.203.132, 2607:f8b0:400c:c07::84\n",
            "Connecting to doc-0c-2s-docs.googleusercontent.com (doc-0c-2s-docs.googleusercontent.com)|172.217.203.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/LAV-MSA-2-train.pkl’\n",
            "\n",
            "data/AD_NMT-master/     [ <=>                ]   2.04M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2020-04-21 12:44:28 (108 MB/s) - ‘data/AD_NMT-master/LAV-MSA-2-train.pkl’ saved [2141923]\n",
            "\n",
            "--2020-04-21 12:44:29--  https://docs.google.com/uc?export=download&id=1UjDX7cCG2S23SPfSHxSPdVayMTxB5Y16\n",
            "Resolving docs.google.com (docs.google.com)... 172.217.193.113, 172.217.193.100, 172.217.193.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.193.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-04-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ultu0or8np2859aimh259h59md3eetfb/1587473025000/16970776037313924126/*/1UjDX7cCG2S23SPfSHxSPdVayMTxB5Y16?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-04-21 12:44:30--  https://doc-04-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ultu0or8np2859aimh259h59md3eetfb/1587473025000/16970776037313924126/*/1UjDX7cCG2S23SPfSHxSPdVayMTxB5Y16?e=download\n",
            "Resolving doc-04-2s-docs.googleusercontent.com (doc-04-2s-docs.googleusercontent.com)... 172.217.203.132, 2607:f8b0:400c:c07::84\n",
            "Connecting to doc-04-2s-docs.googleusercontent.com (doc-04-2s-docs.googleusercontent.com)|172.217.203.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/Magribi_MSA-both.pkl’\n",
            "\n",
            "data/AD_NMT-master/     [ <=>                ]   2.81M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-04-21 12:44:30 (189 MB/s) - ‘data/AD_NMT-master/Magribi_MSA-both.pkl’ saved [2944107]\n",
            "\n",
            "--2020-04-21 12:44:32--  https://docs.google.com/uc?export=download&id=1UaVWIqRXo0rxuxDF4KArA4bEK1TaLX3l\n",
            "Resolving docs.google.com (docs.google.com)... 172.217.204.101, 172.217.204.139, 172.217.204.102, ...\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.204.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0o-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ol4uo5mogmtfilll7t4ofi3ugjo6ih8h/1587473025000/16970776037313924126/*/1UaVWIqRXo0rxuxDF4KArA4bEK1TaLX3l?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-04-21 12:44:33--  https://doc-0o-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ol4uo5mogmtfilll7t4ofi3ugjo6ih8h/1587473025000/16970776037313924126/*/1UaVWIqRXo0rxuxDF4KArA4bEK1TaLX3l?e=download\n",
            "Resolving doc-0o-2s-docs.googleusercontent.com (doc-0o-2s-docs.googleusercontent.com)... 172.217.203.132, 2607:f8b0:400c:c07::84\n",
            "Connecting to doc-0o-2s-docs.googleusercontent.com (doc-0o-2s-docs.googleusercontent.com)|172.217.203.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 290840 (284K) [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/Magribi_MSA-test.pkl’\n",
            "\n",
            "data/AD_NMT-master/ 100%[===================>] 284.02K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2020-04-21 12:44:33 (90.3 MB/s) - ‘data/AD_NMT-master/Magribi_MSA-test.pkl’ saved [290840/290840]\n",
            "\n",
            "--2020-04-21 12:44:34--  https://docs.google.com/uc?export=download&id=1UYvlhdYAdfa4riP_4hn3-IEVd1ZUXVTQ\n",
            "Resolving docs.google.com (docs.google.com)... 173.194.216.100, 173.194.216.139, 173.194.216.102, ...\n",
            "Connecting to docs.google.com (docs.google.com)|173.194.216.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0s-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/2l80car55cr3c9csgbetfff5u28bta0g/1587473025000/16970776037313924126/*/1UYvlhdYAdfa4riP_4hn3-IEVd1ZUXVTQ?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-04-21 12:44:35--  https://doc-0s-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/2l80car55cr3c9csgbetfff5u28bta0g/1587473025000/16970776037313924126/*/1UYvlhdYAdfa4riP_4hn3-IEVd1ZUXVTQ?e=download\n",
            "Resolving doc-0s-2s-docs.googleusercontent.com (doc-0s-2s-docs.googleusercontent.com)... 172.217.203.132, 2607:f8b0:400c:c07::84\n",
            "Connecting to doc-0s-2s-docs.googleusercontent.com (doc-0s-2s-docs.googleusercontent.com)|172.217.203.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/Magribi_MSA-train.pkl’\n",
            "\n",
            "data/AD_NMT-master/     [ <=>                ]   2.53M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-04-21 12:44:35 (203 MB/s) - ‘data/AD_NMT-master/Magribi_MSA-train.pkl’ saved [2652508]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-WzhRv4mIKNq",
        "colab": {}
      },
      "source": [
        "#James Chartouni\n",
        "#Joey Park\n",
        "#Raef Khan\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os, io, glob\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchtext.datasets.translation import TranslationDataset\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import sentencepiece as spm\n",
        "\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config, T5PreTrainedModel\n",
        "\n",
        "\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.vocab import Vocab\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aha0xureIKNw",
        "outputId": "f4065623-7b04-4416-9b43-379783139ed5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "ls data/AD_NMT-master"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "english-Arabic-both.pkl   LAV-MSA-2-both.pkl   Magribi_MSA-both.pkl\n",
            "english-Arabic-test.pkl   LAV-MSA-2-test.pkl   Magribi_MSA-test.pkl\n",
            "english-Arabic-train.pkl  LAV-MSA-2-train.pkl  Magribi_MSA-train.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XWKnGRlLIKN9",
        "colab": {}
      },
      "source": [
        "file_path = 'data/AD_NMT-master/'\n",
        "\n",
        "with open(file_path + \"english-Arabic-train.pkl\", 'rb') as handle:\n",
        "    en_msa_pre_train_ds = pickle.load(handle) \n",
        "    \n",
        "with open(file_path + \"english-Arabic-test.pkl\", 'rb') as handle:\n",
        "    en_msa_test_ds = pickle.load(handle) \n",
        "    \n",
        "with open(file_path + \"LAV-MSA-2-train.pkl\", 'rb') as handle:\n",
        "    lav_msa_pre_train_ds = pickle.load(handle)\n",
        "    \n",
        "with open(file_path + \"LAV-MSA-2-test.pkl\", 'rb') as handle:\n",
        "    lav_msa_test_ds = pickle.load(handle)    \n",
        "    \n",
        "with open(file_path + \"LAV-MSA-2-both.pkl\", 'rb') as handle:\n",
        "    data_LAV_MSA = pickle.load(handle) \n",
        "\n",
        "with open(file_path + \"english-Arabic-both.pkl\", 'rb') as handle:\n",
        "    data_English_MSA = pickle.load(handle) \n",
        "    \n",
        "with open(file_path + \"Magribi_MSA-both.pkl\", 'rb') as handle:\n",
        "    data_Magribi_MSA = pickle.load(handle) \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ch3APJadIKOH",
        "outputId": "cd638d0f-0ee7-41c0-acee-26db2ce8f7ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(en_msa_pre_train_ds[0:5])\n",
        "print(en_msa_test_ds[0:5])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Tom was also there', 'كان توم هنا ايضا'], ['That old woman lives by herself', 'تلك المراة العجوز تسكن بمفردها'], ['He went abroad for the purpose of studying English', 'سافر خارج البلد ليتعلم الانجليزية'], ['There is a fork missing', 'هناك شوكة ناقصة'], [\"I don't know this game\", 'لا اعرف هذه اللعبة']]\n",
            "[[\"Where's your money?\", 'اين مالك؟'], ['Be prepared', 'كن مستعدا'], [\"I figured you'd be impressed\", 'توقعت انك ستنبهر'], ['May I come in?', 'هل بامكاني الدخول؟'], ['Read through the article', 'اقرا المقالة']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "irQQ9E_7IKOM"
      },
      "source": [
        "## Prepare Datasets\n",
        "\n",
        "example: https://iwslt2010.fbk.eu/node/32/\n",
        "\n",
        "We need to take our training and test sets from the pkl files and create new .txt files that are formatted so that the standard torchtext Dataset class can read them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "raw",
        "id": "ikpu9B5dIKON"
      },
      "source": [
        "Data format:\n",
        "each line consists of three fields divided by the character '\\'\n",
        "sentences consisting of words divided by single spaces\n",
        "format: <SENTENCE_ID>\\<PARAPHRASE_ID>\\<TEXT>\n",
        "Field_1: sentence ID\n",
        "Field_2: paraphrase ID\n",
        "Field_3: MT develop sentence / reference translation\n",
        "Text input example:\n",
        "DEV_001\\01\\This is the first develop sentence.\n",
        "DEV_002\\01\\This is the second develop sentence.\n",
        "Reference translation example:\n",
        "DEV_001\\01\\1st reference translation for 1st input\n",
        "DEV_001\\02\\2nd reference translation for 1st input\n",
        "...\n",
        "DEV_002\\01\\1st reference translation for 2nd input\n",
        "DEV_002\\02\\2nd reference translation for 2nd input\n",
        "...\n",
        "Languages:\n",
        "Arabic-English\n",
        "CSTAR03 testset: 506 sentences, 16 reference translations\n",
        "IWSLT04 testset: 500 sentences, 16 reference translations\n",
        "IWSLT05 testset: 506 sentences, 16 reference translations\n",
        "IWSLT07 testset: 489 sentences, 6 reference translations\n",
        "IWSLT08 testset: 507 sentences, 16 reference translations\n",
        "French-English\n",
        "CSTAR03 testset: 506 sentences, 16 reference translations\n",
        "IWSLT04 testset: 500 sentences, 16 reference translations\n",
        "IWSLT05 testset: 506 sentences, 16 reference translations\n",
        "Turkish-English\n",
        "CSTAR03 testset: 506 sentences, 16 reference translations\n",
        "IWSLT04 testset: 500 sentences, 16 reference translations\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wcnDJDKwqED",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3feb4faf-9564-447c-d50b-0a6e65fe7da9"
      },
      "source": [
        "ls data/AD_NMT-master/"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "english-Arabic-both.pkl   LAV-MSA-2-both.pkl   Magribi_MSA-both.pkl\n",
            "english-Arabic-test.pkl   LAV-MSA-2-test.pkl   Magribi_MSA-test.pkl\n",
            "english-Arabic-train.pkl  LAV-MSA-2-train.pkl  Magribi_MSA-train.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W3bWU46TIKOb",
        "colab": {}
      },
      "source": [
        "file_path = 'data/'\n",
        "\n",
        "def pytorch_format(ds, src='en', trg='msa',datatype=''):\n",
        "    src_formatted = datatype + '.' + src\n",
        "    trg_formatted = datatype + '.' + trg\n",
        "    \n",
        "    with open(file_path + datatype + \"/\" + src_formatted, 'wt') as srctxt, open(file_path + datatype + \"/\" + trg_formatted, 'wt') as trgtxt:\n",
        "        for i, arr in enumerate(ds):\n",
        "            srctxt.write(datatype.upper() + '_' + str(i).zfill( len(str(len(ds))) - len(str(i))) + '\\\\01\\\\' + arr[0] + '\\n')\n",
        "            trgtxt.write(datatype.upper() + '_' + str(i).zfill( len(str(len(ds))) - len(str(i))) + '\\\\01\\\\' + arr[1] + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lp7ncVY6wqET",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#splits the datatsets into train and validation sets \n",
        "en_msa_train_ds, en_msa_val_ds = train_test_split(en_msa_pre_train_ds, test_size=.2)\n",
        "lav_msa_train_ds, lav_msa_val_ds = train_test_split(lav_msa_pre_train_ds, test_size=.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ue2fSILwqEW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0a247f96-d810-4bdd-d986-61e4302b0d1a"
      },
      "source": [
        "print(len(en_msa_train_ds))\n",
        "print(len(en_msa_val_ds))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7200\n",
            "1800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AqwDB9BuKEnV",
        "colab": {}
      },
      "source": [
        "pytorch_format(en_msa_train_ds, 'en', 'msa', 'train')\n",
        "pytorch_format(en_msa_val_ds, 'en', 'msa', 'val')\n",
        "pytorch_format(en_msa_test_ds, 'en', 'msa', 'test')\n",
        "\n",
        "pytorch_format(lav_msa_train_ds, 'lav', 'msa', 'train')\n",
        "pytorch_format(lav_msa_val_ds, 'lav', 'msa', 'val')\n",
        "pytorch_format(lav_msa_test_ds, 'lav', 'msa', 'test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1W-AWwxwwqEo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cat data/train/train.en"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97BEXs7vwqEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cat data/train/train.msa"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lDPvs9zxIKO3"
      },
      "source": [
        "## Build Vocabulary "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ihevk2SWIKO5"
      },
      "source": [
        "Sentence Piece Google Colab\n",
        "https://github.com/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dIsjMpt0IKO6",
        "colab": {}
      },
      "source": [
        "len(data_English_MSA)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "I25H3IsyIKPB",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Create a text file with all the MSA vocab available for SentencePiece to create a library \n",
        "\"\"\"\n",
        "\n",
        "text_file_en = open(\"data/english_data.txt\", \"wt\")\n",
        "text_file_msa = open(\"data/arabic_data.txt\", \"wt\")\n",
        "\n",
        "MSA_text = \"\"\n",
        "EN_text = \"\"\n",
        "\n",
        "for line in data_English_MSA:\n",
        "        english = line[0]\n",
        "        english_words = english.split(\" \")\n",
        "        for count, word in enumerate(english_words):\n",
        "            text_file_en.write(word)\n",
        "        text_file_en.write(\"\\n\")\n",
        "        \n",
        "        arabic = line[1]\n",
        "        arabic_words = arabic.split(\" \")\n",
        "        for count, word in enumerate(arabic_words):\n",
        "            text_file_msa.write(word)\n",
        "        text_file_msa.write(\"\\n\")\n",
        "\n",
        "\n",
        "text_file_en.close()\n",
        "text_file_msa.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q0qG8N68IKPI",
        "outputId": "861879b0-e03e-4aac-feea-63967ed94bb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "spm.SentencePieceTrainer.train('--input=data/arabic_data.txt --model_prefix=data/msa --vocab_size=2000')\n",
        "spm.SentencePieceTrainer.train('--input=data/english_data.txt --model_prefix=data/en --vocab_size=2000')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y05LILRSIKPN",
        "outputId": "c23c6e88-ce7b-415e-a043-0445d9776844",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('data/en.model')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JyJm_doVIKPT",
        "colab": {}
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Egx5FaOsIKPW",
        "colab": {}
      },
      "source": [
        "print(sp.encode_as_pieces('This is a test'))\n",
        "print(sp.encode_as_ids('This is a test'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPy6YE06wqFA",
        "colab_type": "text"
      },
      "source": [
        "## Spacy Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SRIbnqMwqFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls data/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_U6-4yB1wqFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_file_en = open(\"data/en.vocab\", \"r\")\n",
        "text_file_ar = open(\"data/msa.vocab\", \"r\")\n",
        "\n",
        "en_vocab_list = []\n",
        "ar_vocab_list = []\n",
        "for line in text_file_en.readlines():\n",
        "    en_vocab_list.append(line.split(\"\\t\")[0])\n",
        "\n",
        "for line in text_file_ar.readlines():\n",
        "    # print(line)\n",
        "    # print(line.split(\"\\t\")[0])\n",
        "    # print(\"---------\")\n",
        "    ar_vocab_list.append(line.split(\"\\t\")[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IkkH0xQwqFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ar_vocab_list[0:200]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNXL6VRswqFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_file_en = open(\"data/en.vocab\", \"r\")\n",
        "text_file_ar = open(\"data/msa.vocab\", \"r\")\n",
        "\n",
        "line = text_file_en.readlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpMYD4zGwqFP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "en_vocab = Vocab(strings=en_vocab_list)\n",
        "spacy_en_tokenizer = Tokenizer(en_vocab)\n",
        "\n",
        "ar_vocab = Vocab(strings=ar_vocab_list)\n",
        "spacy_msa_tokenizer = Tokenizer(ar_vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0uDxsOI4IKPd"
      },
      "source": [
        "## TF Tokenizer\n",
        "\n",
        "https://huggingface.co/transformers/model_doc/t5.html#t5tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zDuf6tVBIKPe",
        "colab": {}
      },
      "source": [
        "ls data/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LH26oj-KIKPl",
        "colab": {}
      },
      "source": [
        "msa_tokenizer = T5Tokenizer(\"data/msa.model\")\n",
        "en_tokenizer = T5Tokenizer(\"data/en.model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "txeYu0e0IKPs",
        "colab": {}
      },
      "source": [
        "input_ids = msa_tokenizer.encode('هل بامكاني الدخول؟ </s>', return_tensors='pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D7IuJeL0IKPx",
        "colab": {}
      },
      "source": [
        "input_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi6rWTe0wqFc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#decode to make sure you can go back and forth between the encoding properly \n",
        "#@raef can you verify the Arabic?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHV0FsKEwqFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls data/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UuAey7jyIKP1"
      },
      "source": [
        "## Pytorch Data Set and Data Loader\n",
        "\n",
        "https://github.com/google-research/text-to-text-transfer-transformer\n",
        "\n",
        "pytorch dataset: https://pytorch.org/text/_modules/torchtext/datasets/translation.html\n",
        "\n",
        "pytorch dataset documentation: https://torchtext.readthedocs.io/en/latest/datasets.html#iwslt\n",
        "\n",
        "example dataset: https://iwslt2010.fbk.eu/node/32/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "god7BjRnwqFh",
        "colab_type": "text"
      },
      "source": [
        "Field API: https://pytorch.org/text/data.html#torchtext.data.Field\n",
        "Field Source: https://pytorch.org/text/_modules/torchtext/data/field.html#Field"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmvgEGoLwqFi",
        "colab_type": "text"
      },
      "source": [
        "https://pytorch.org/text/_modules/torchtext/data/utils.html#get_tokenizer\n",
        "    \n",
        "we need to pass our tokenizer as a function     "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fVIqommkIKQF",
        "colab": {}
      },
      "source": [
        "SRC = Field(tokenize = spacy_en_tokenizer,\n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower = False)\n",
        "\n",
        "TRG = Field(tokenize = spacy_msa_tokenizer,\n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiS3YbFGwqFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = TranslationDataset(path='data/train/train.', exts=('en', 'msa'), fields=(SRC, TRG))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEnHMRQxwqFo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SRC.build_vocab(dataset)\n",
        "TRG.build_vocab(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08ZvS7DYwqFq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ex = dataset[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAUEnQd6wqFu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0d2b1185-3e32-4eb4-a15c-0c776ca86924"
      },
      "source": [
        "print(ex.src) \n",
        "print(ex.trg)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN_000\\01\\They are proud of their daughter\n",
            "TRAIN_0000\\01\\هذه الرسالة منقولة يا ايمان\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nsz1PhXwqF2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2eb928dc-4764-4237-82d5-1b49b3e4a42c"
      },
      "source": [
        "ls data/train"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train.en  train.lav  train.msa\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Pps4KrlSIKQK",
        "colab": {}
      },
      "source": [
        "#train, validation and test are probably wrong.  \n",
        "#https://github.com/pytorch/text/blob/master/torchtext/datasets/translation.py\n",
        "\n",
        "'''\n",
        "Arguments:\n",
        "            exts: A tuple containing the extension to path for each language.\n",
        "            fields: A tuple containing the fields that will be used for data\n",
        "                in each language.\n",
        "            path (str): Common prefix of the splits' file paths, or None to use\n",
        "                the result of cls.download(root).\n",
        "            root: Root dataset storage directory. Default is '.data'.\n",
        "            train: The prefix of the train data. Default: 'train'.\n",
        "            validation: The prefix of the validation data. Default: 'val'.\n",
        "            test: The prefix of the test data. Default: 'test'.\n",
        "            Remaining keyword arguments: Passed to the splits method of\n",
        "                Dataset.\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "train_data, valid_data, test_data = dataset.splits(path= 'data/', train='train/train', validation='val/val', test='test/test', exts=('.en', '.msa'),\n",
        "                                                    fields = (SRC, TRG))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D86hmOgsIKQS",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE =32\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "15gKWRTZIKQW"
      },
      "source": [
        "## Model Training\n",
        "\n",
        "https://huggingface.co/transformers/model_doc/t5.html#training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BZ8YmzO5IKQW",
        "colab": {}
      },
      "source": [
        "configuration = T5Config()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4xt_iDQ4IKQa",
        "colab": {}
      },
      "source": [
        "model = T5PreTrainedModel.from_pretrained('t5-base')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsoUR7mABTBk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "87c85a6e-3fd8-47e9-fae4-1b5a515a2eb2"
      },
      "source": [
        "model\n",
        "model.init_weights()\n",
        "optimizer = optim.AdamW(model.parameters())\n",
        "pad_idx = TRG.vocab.stoi['<pad>']\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-96-01bbad503192>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpad_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTRG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<pad>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad)\u001b[0m\n\u001b[1;32m     43\u001b[0m         defaults = dict(lr=lr, betas=betas, eps=eps,\n\u001b[1;32m     44\u001b[0m                         weight_decay=weight_decay, amsgrad=amsgrad)\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"optimizer got an empty parameter list\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: optimizer got an empty parameter list"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_0cY8-SJIKQm",
        "colab": {}
      },
      "source": [
        "def train(model:nn.Module, \n",
        "          iterator:BucketIterator, \n",
        "          optimizer: optim.Optimizer,\n",
        "          criterion:nnCrossEntropyLoss, \n",
        "          clip: float):\n",
        "  model.train()\n",
        "  epoch_loss = 0\n",
        "  for i, batch in enumerate(train_iterator):\n",
        "    src = batch.src\n",
        "    trg = batch.trg\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = model(src, trg)\n",
        "    loss = criterion(output[1:].view(-1, output.shape[2]), trg[1:].view(-1))\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "  return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFYDdEkSIgs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model: nn.Module,\n",
        "             iterator: BucketIterator,\n",
        "             criterion: nn.Module):\n",
        "\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for _, batch in enumerate(iterator):\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "            output = output[1:].view(-1, output.shape[-1])\n",
        "            trg = trg[1:].view(-1)\n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G7NGlyJJaKv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gx-OUi3XJi0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_time(start_time: int,\n",
        "               end_time: int):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhWSfEFFJjoN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mPFePDoUIKQ1"
      },
      "source": [
        "## Model Inference\n",
        "\n",
        "https://github.com/huggingface/transformers/blob/master/examples/translation/t5/evaluate_wmt.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hMW3IMHqIKQ1",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_ZBVu6R9IKQ4",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}