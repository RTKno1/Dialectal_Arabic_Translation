{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_1.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BQflUKhjIKNp"
      },
      "source": [
        "### Notes \n",
        "\n",
        "T5 Paper: https://arxiv.org/pdf/1910.10683.pdf\n",
        "\n",
        "T5 Tokenizer: https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_t5.py\n",
        "\n",
        "Important Tasks: https://docs.google.com/document/d/1weIZM6QTlnitpPQmpg-WeV2RW70TnYmDuogBQPr5mB0/edit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLOmiOta6MJp",
        "colab_type": "code",
        "outputId": "e033a5a3-6666-4d4a-fe20-790368fe93c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#installation step\n",
        "!pip install transformers\n",
        "!pip install t5\n",
        "!pip install sentencepiece\n",
        "!pip install bpemb\n",
        "#creating the folders \n",
        "!mkdir data/\n",
        "!mkdir data/AD_NMT-master\n",
        "!mkdir data/train/\n",
        "!mkdir data/test/\n",
        "!mkdir data/val/\n",
        "#fetching the pkl files\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW' -O data/AD_NMT-master/english-Arabic-both.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1V8_tp8ZlWUYaX7QQL46t0uSRNrVehSf1' -O data/AD_NMT-master/english-Arabic-test.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1V7X0qtuDIyjTHY0wh-ZNoVwsiF4lId2e' -O data/AD_NMT-master/english-Arabic-train.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV' -O data/AD_NMT-master/LAV-MSA-2-both.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UpfCbkxhztof7dvNjeAs1bHjD4SER6h3' -O data/AD_NMT-master/LAV-MSA-2-test.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UlAZGtYsSfXzK7hrC_PbxQFqTSXD0DMw' -O data/AD_NMT-master/LAV-MSA-2-train.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UjDX7cCG2S23SPfSHxSPdVayMTxB5Y16' -O data/AD_NMT-master/Magribi_MSA-both.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UaVWIqRXo0rxuxDF4KArA4bEK1TaLX3l' -O data/AD_NMT-master/Magribi_MSA-test.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UYvlhdYAdfa4riP_4hn3-IEVd1ZUXVTQ' -O data/AD_NMT-master/Magribi_MSA-train.pkl"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/b5/ac41e3e95205ebf53439e4dd087c58e9fd371fd8e3724f2b9b4cdb8282e5/transformers-2.10.0-py3-none-any.whl (660kB)\n",
            "\r\u001b[K     |▌                               | 10kB 20.4MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 2.4MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 3.1MB/s eta 0:00:01\r\u001b[K     |██▌                             | 51kB 2.2MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 2.6MB/s eta 0:00:01\r\u001b[K     |███▌                            | 71kB 3.0MB/s eta 0:00:01\r\u001b[K     |████                            | 81kB 3.1MB/s eta 0:00:01\r\u001b[K     |████▌                           | 92kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 102kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 112kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████                          | 122kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 133kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████                         | 143kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 153kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████                        | 163kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 174kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████                       | 184kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 194kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████                      | 204kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 215kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████                     | 225kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 235kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████                    | 245kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 256kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 266kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 276kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 286kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 296kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 307kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 317kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 327kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 337kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 348kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 358kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 368kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 378kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 389kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 399kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 409kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 419kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 430kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 440kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 450kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 460kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 471kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 481kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 491kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 501kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 512kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 522kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 532kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 542kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 552kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 563kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 573kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 583kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 593kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 604kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 614kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 624kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 634kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 645kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 655kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 665kB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\r\u001b[K     |▍                               | 10kB 22.3MB/s eta 0:00:01\r\u001b[K     |▊                               | 20kB 30.7MB/s eta 0:00:01\r\u001b[K     |█▏                              | 30kB 37.8MB/s eta 0:00:01\r\u001b[K     |█▌                              | 40kB 42.6MB/s eta 0:00:01\r\u001b[K     |█▉                              | 51kB 44.5MB/s eta 0:00:01\r\u001b[K     |██▎                             | 61kB 47.6MB/s eta 0:00:01\r\u001b[K     |██▋                             | 71kB 48.8MB/s eta 0:00:01\r\u001b[K     |███                             | 81kB 49.9MB/s eta 0:00:01\r\u001b[K     |███▍                            | 92kB 51.6MB/s eta 0:00:01\r\u001b[K     |███▊                            | 102kB 53.1MB/s eta 0:00:01\r\u001b[K     |████                            | 112kB 53.1MB/s eta 0:00:01\r\u001b[K     |████▌                           | 122kB 53.1MB/s eta 0:00:01\r\u001b[K     |████▉                           | 133kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 143kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 153kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 163kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 174kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 184kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 194kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 204kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 215kB 53.1MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 225kB 53.1MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 235kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████                       | 245kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 256kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 266kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 276kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 286kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 296kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 307kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 317kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 327kB 53.1MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 337kB 53.1MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 348kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 358kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 368kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 378kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 389kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 399kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 409kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 419kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 430kB 53.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 440kB 53.1MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 450kB 53.1MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 460kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 471kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 481kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 491kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 501kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 512kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 522kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 532kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 542kB 53.1MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 552kB 53.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 563kB 53.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 573kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 583kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 593kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 604kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 614kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 624kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 634kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 645kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 655kB 53.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 665kB 53.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 675kB 53.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 686kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 696kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 706kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 716kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 727kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 737kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 747kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 757kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 768kB 53.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 778kB 53.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 788kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 798kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 808kB 53.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 819kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 829kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 839kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 849kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 860kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 870kB 53.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 880kB 53.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 890kB 53.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 57.0MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 53.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=25a46fe5c687c08198bdb2a500001ca747119c104dec5f3e4a4725e5f0bf2aed\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.10.0\n",
            "Collecting t5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/55/cf4b9ad68873d28224ac9ca78bd30332b53b1f9f6c564ce8d3cc5358a0a8/t5-0.6.0-py3-none-any.whl (149kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: gin-config in /usr/local/lib/python3.6/dist-packages (from t5) (0.3.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from t5) (1.5.0+cu101)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.6/dist-packages (from t5) (2.8.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from t5) (0.1.91)\n",
            "Collecting tfds-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/ef/3ba3228cd48db13ff785e1af14d7eebfdf6091903cad094527ef5ddcf2ec/tfds_nightly-3.1.0.dev202005310105-py3-none-any.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 11.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from t5) (0.22.2.post1)\n",
            "Collecting tensorflow-text\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/78/e7/d260e51d44bea241e8eee39d0266df9b33a3d6219ded118a1c81a872e848/tensorflow_text-2.2.0-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 30.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from t5) (0.9.0)\n",
            "Collecting mesh-tensorflow[transformer]>=0.1.13\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/32/ceaf1549586ccfeaf7840a166876be358bd78ab2fdd152b17c5600ed7888/mesh_tensorflow-0.1.13-py3-none-any.whl (292kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 42.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from t5) (1.4.1)\n",
            "Collecting six>=1.14\n",
            "  Downloading https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from t5) (1.0.3)\n",
            "Requirement already satisfied: transformers>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from t5) (2.10.0)\n",
            "Collecting rouge-score\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/6d/2b9a64cba1e4e6ecd4effbf6834b2592b54dc813654f84029758e5daeeb5/rouge_score-0.0.3-py3-none-any.whl\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from t5) (3.2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from t5) (1.18.4)\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/4b/6c7a0b26a48d88f56573d11aa5058808fe0d36ba40951287894f943556b5/sacrebleu-1.4.10-py3-none-any.whl (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->t5) (0.16.0)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.6/dist-packages (from babel->t5) (2018.9)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (4.41.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (3.10.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (1.12.1)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (19.3.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (2.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (2.23.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (1.1.0)\n",
            "Collecting tensorflow-metadata<0.16,>=0.15\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/0c/afb81ea6998f6e26521671585d1cd9d3f7945a8b9834764e91757453dc25/tensorflow_metadata-0.15.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (0.3.1.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->t5) (0.15.1)\n",
            "Requirement already satisfied: tensorflow<2.3,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-text->t5) (2.2.0)\n",
            "Requirement already satisfied: tensorflow-datasets; extra == \"transformer\" in /usr/local/lib/python3.6/dist-packages (from mesh-tensorflow[transformer]>=0.1.13->t5) (2.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->t5) (2.8.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5) (0.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5) (0.0.43)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5) (3.0.12)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/53/84/7b3146ec6378d28abc73ab484f09f47dfa008ad6f03f33d90a369f880e25/portalocker-1.7.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tfds-nightly->t5) (46.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly->t5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly->t5) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly->t5) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly->t5) (1.24.3)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata<0.16,>=0.15->tfds-nightly->t5) (1.51.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (0.34.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (0.2.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (0.3.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (3.2.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (2.2.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (1.6.3)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (2.10.0)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (2.2.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (1.29.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.7.0->t5) (7.1.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (3.2.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (1.7.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (1.6.0.post3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (1.6.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (3.1.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (0.2.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3,>=2.2.0->tensorflow-text->t5) (0.4.8)\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.12.0, but you'll have six 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: six, tensorflow-metadata, tfds-nightly, tensorflow-text, mesh-tensorflow, rouge-score, portalocker, sacrebleu, t5\n",
            "  Found existing installation: six 1.12.0\n",
            "    Uninstalling six-1.12.0:\n",
            "      Successfully uninstalled six-1.12.0\n",
            "  Found existing installation: tensorflow-metadata 0.22.0\n",
            "    Uninstalling tensorflow-metadata-0.22.0:\n",
            "      Successfully uninstalled tensorflow-metadata-0.22.0\n",
            "Successfully installed mesh-tensorflow-0.1.13 portalocker-1.7.0 rouge-score-0.0.3 sacrebleu-1.4.10 six-1.15.0 t5-0.6.0 tensorflow-metadata-0.15.2 tensorflow-text-2.2.0 tfds-nightly-3.1.0.dev202005310105\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "six"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.91)\n",
            "Collecting bpemb\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/70/468a9652095b370f797ed37ff77e742b11565c6fd79eaeca5f2e50b164a7/bpemb-0.3.0-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from bpemb) (2.23.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from bpemb) (3.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from bpemb) (4.41.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from bpemb) (0.1.91)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from bpemb) (1.18.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (1.24.3)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim->bpemb) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->bpemb) (2.0.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim->bpemb) (1.4.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->bpemb) (1.13.13)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->bpemb) (2.49.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->bpemb) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->bpemb) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.13 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->bpemb) (1.16.13)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->smart-open>=1.2.1->gensim->bpemb) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->smart-open>=1.2.1->gensim->bpemb) (0.15.2)\n",
            "Installing collected packages: bpemb\n",
            "Successfully installed bpemb-0.3.0\n",
            "--2020-05-31 14:17:39--  https://docs.google.com/uc?export=download&id=1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.134.138, 74.125.134.101, 74.125.134.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.134.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-10-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/v02mate06dpvgphn1b4t49d4gdq9j98l/1590934650000/16970776037313924126/*/1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-05-31 14:17:40--  https://doc-10-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/v02mate06dpvgphn1b4t49d4gdq9j98l/1590934650000/16970776037313924126/*/1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW?e=download\n",
            "Resolving doc-10-2s-docs.googleusercontent.com (doc-10-2s-docs.googleusercontent.com)... 173.194.217.132, 2607:f8b0:400c:c13::84\n",
            "Connecting to doc-10-2s-docs.googleusercontent.com (doc-10-2s-docs.googleusercontent.com)|173.194.217.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 955428 (933K) [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/english-Arabic-both.pkl’\n",
            "\n",
            "data/AD_NMT-master/ 100%[===================>] 933.04K  --.-KB/s    in 0.006s  \n",
            "\n",
            "2020-05-31 14:17:40 (146 MB/s) - ‘data/AD_NMT-master/english-Arabic-both.pkl’ saved [955428/955428]\n",
            "\n",
            "--2020-05-31 14:17:41--  https://docs.google.com/uc?export=download&id=1V8_tp8ZlWUYaX7QQL46t0uSRNrVehSf1\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.134.100, 74.125.134.139, 74.125.134.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.134.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-14-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/tnm1je56sn9t9bkr02ml9nocjiiej2ep/1590934650000/16970776037313924126/*/1V8_tp8ZlWUYaX7QQL46t0uSRNrVehSf1?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-05-31 14:17:41--  https://doc-14-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/tnm1je56sn9t9bkr02ml9nocjiiej2ep/1590934650000/16970776037313924126/*/1V8_tp8ZlWUYaX7QQL46t0uSRNrVehSf1?e=download\n",
            "Resolving doc-14-2s-docs.googleusercontent.com (doc-14-2s-docs.googleusercontent.com)... 173.194.217.132, 2607:f8b0:400c:c13::84\n",
            "Connecting to doc-14-2s-docs.googleusercontent.com (doc-14-2s-docs.googleusercontent.com)|173.194.217.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 95497 (93K) [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/english-Arabic-test.pkl’\n",
            "\n",
            "data/AD_NMT-master/ 100%[===================>]  93.26K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2020-05-31 14:17:41 (111 MB/s) - ‘data/AD_NMT-master/english-Arabic-test.pkl’ saved [95497/95497]\n",
            "\n",
            "--2020-05-31 14:17:42--  https://docs.google.com/uc?export=download&id=1V7X0qtuDIyjTHY0wh-ZNoVwsiF4lId2e\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.134.101, 74.125.134.138, 74.125.134.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.134.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-10-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/5ru2tm370fgjnnkqif84o567lp8br5g9/1590934650000/16970776037313924126/*/1V7X0qtuDIyjTHY0wh-ZNoVwsiF4lId2e?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-05-31 14:17:43--  https://doc-10-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/5ru2tm370fgjnnkqif84o567lp8br5g9/1590934650000/16970776037313924126/*/1V7X0qtuDIyjTHY0wh-ZNoVwsiF4lId2e?e=download\n",
            "Resolving doc-10-2s-docs.googleusercontent.com (doc-10-2s-docs.googleusercontent.com)... 173.194.217.132, 2607:f8b0:400c:c13::84\n",
            "Connecting to doc-10-2s-docs.googleusercontent.com (doc-10-2s-docs.googleusercontent.com)|173.194.217.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 859172 (839K) [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/english-Arabic-train.pkl’\n",
            "\n",
            "data/AD_NMT-master/ 100%[===================>] 839.04K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2020-05-31 14:17:43 (109 MB/s) - ‘data/AD_NMT-master/english-Arabic-train.pkl’ saved [859172/859172]\n",
            "\n",
            "--2020-05-31 14:17:44--  https://docs.google.com/uc?export=download&id=1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.134.101, 74.125.134.102, 74.125.134.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.134.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-00-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/t6t8t7jhtud98ipi769jtbkea1lm7ap4/1590934650000/16970776037313924126/*/1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-05-31 14:17:44--  https://doc-00-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/t6t8t7jhtud98ipi769jtbkea1lm7ap4/1590934650000/16970776037313924126/*/1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV?e=download\n",
            "Resolving doc-00-2s-docs.googleusercontent.com (doc-00-2s-docs.googleusercontent.com)... 173.194.217.132, 2607:f8b0:400c:c13::84\n",
            "Connecting to doc-00-2s-docs.googleusercontent.com (doc-00-2s-docs.googleusercontent.com)|173.194.217.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/LAV-MSA-2-both.pkl’\n",
            "\n",
            "data/AD_NMT-master/     [ <=>                ]   2.33M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-05-31 14:17:45 (231 MB/s) - ‘data/AD_NMT-master/LAV-MSA-2-both.pkl’ saved [2447014]\n",
            "\n",
            "--2020-05-31 14:17:46--  https://docs.google.com/uc?export=download&id=1UpfCbkxhztof7dvNjeAs1bHjD4SER6h3\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.134.101, 74.125.134.138, 74.125.134.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.134.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0c-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/nqp4vco531ofls5c73g7tn18jo4nuafb/1590934650000/16970776037313924126/*/1UpfCbkxhztof7dvNjeAs1bHjD4SER6h3?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-05-31 14:17:46--  https://doc-0c-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/nqp4vco531ofls5c73g7tn18jo4nuafb/1590934650000/16970776037313924126/*/1UpfCbkxhztof7dvNjeAs1bHjD4SER6h3?e=download\n",
            "Resolving doc-0c-2s-docs.googleusercontent.com (doc-0c-2s-docs.googleusercontent.com)... 173.194.217.132, 2607:f8b0:400c:c13::84\n",
            "Connecting to doc-0c-2s-docs.googleusercontent.com (doc-0c-2s-docs.googleusercontent.com)|173.194.217.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 304332 (297K) [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/LAV-MSA-2-test.pkl’\n",
            "\n",
            "data/AD_NMT-master/ 100%[===================>] 297.20K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2020-05-31 14:17:46 (135 MB/s) - ‘data/AD_NMT-master/LAV-MSA-2-test.pkl’ saved [304332/304332]\n",
            "\n",
            "--2020-05-31 14:17:47--  https://docs.google.com/uc?export=download&id=1UlAZGtYsSfXzK7hrC_PbxQFqTSXD0DMw\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.134.113, 74.125.134.101, 74.125.134.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.134.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0c-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/sho8umd15508u2pa5g15a8o0g0fedvsk/1590934650000/16970776037313924126/*/1UlAZGtYsSfXzK7hrC_PbxQFqTSXD0DMw?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-05-31 14:17:48--  https://doc-0c-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/sho8umd15508u2pa5g15a8o0g0fedvsk/1590934650000/16970776037313924126/*/1UlAZGtYsSfXzK7hrC_PbxQFqTSXD0DMw?e=download\n",
            "Resolving doc-0c-2s-docs.googleusercontent.com (doc-0c-2s-docs.googleusercontent.com)... 173.194.217.132, 2607:f8b0:400c:c13::84\n",
            "Connecting to doc-0c-2s-docs.googleusercontent.com (doc-0c-2s-docs.googleusercontent.com)|173.194.217.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/LAV-MSA-2-train.pkl’\n",
            "\n",
            "data/AD_NMT-master/     [ <=>                ]   2.04M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-05-31 14:17:48 (204 MB/s) - ‘data/AD_NMT-master/LAV-MSA-2-train.pkl’ saved [2141923]\n",
            "\n",
            "--2020-05-31 14:17:49--  https://docs.google.com/uc?export=download&id=1UjDX7cCG2S23SPfSHxSPdVayMTxB5Y16\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.134.100, 74.125.134.139, 74.125.134.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.134.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-04-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/8flbbi3i4fhb4ov99qaknuqhjplfsbj2/1590934650000/16970776037313924126/*/1UjDX7cCG2S23SPfSHxSPdVayMTxB5Y16?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-05-31 14:17:49--  https://doc-04-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/8flbbi3i4fhb4ov99qaknuqhjplfsbj2/1590934650000/16970776037313924126/*/1UjDX7cCG2S23SPfSHxSPdVayMTxB5Y16?e=download\n",
            "Resolving doc-04-2s-docs.googleusercontent.com (doc-04-2s-docs.googleusercontent.com)... 173.194.217.132, 2607:f8b0:400c:c13::84\n",
            "Connecting to doc-04-2s-docs.googleusercontent.com (doc-04-2s-docs.googleusercontent.com)|173.194.217.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/Magribi_MSA-both.pkl’\n",
            "\n",
            "data/AD_NMT-master/     [ <=>                ]   2.81M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-05-31 14:17:50 (216 MB/s) - ‘data/AD_NMT-master/Magribi_MSA-both.pkl’ saved [2944107]\n",
            "\n",
            "--2020-05-31 14:17:51--  https://docs.google.com/uc?export=download&id=1UaVWIqRXo0rxuxDF4KArA4bEK1TaLX3l\n",
            "Resolving docs.google.com (docs.google.com)... 173.194.214.102, 173.194.214.101, 173.194.214.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|173.194.214.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0o-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/hmhs37ebr3fjfcdfejnc3rpn621pck2q/1590934650000/16970776037313924126/*/1UaVWIqRXo0rxuxDF4KArA4bEK1TaLX3l?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-05-31 14:17:51--  https://doc-0o-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/hmhs37ebr3fjfcdfejnc3rpn621pck2q/1590934650000/16970776037313924126/*/1UaVWIqRXo0rxuxDF4KArA4bEK1TaLX3l?e=download\n",
            "Resolving doc-0o-2s-docs.googleusercontent.com (doc-0o-2s-docs.googleusercontent.com)... 173.194.217.132, 2607:f8b0:400c:c13::84\n",
            "Connecting to doc-0o-2s-docs.googleusercontent.com (doc-0o-2s-docs.googleusercontent.com)|173.194.217.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 290840 (284K) [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/Magribi_MSA-test.pkl’\n",
            "\n",
            "data/AD_NMT-master/ 100%[===================>] 284.02K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2020-05-31 14:17:51 (161 MB/s) - ‘data/AD_NMT-master/Magribi_MSA-test.pkl’ saved [290840/290840]\n",
            "\n",
            "--2020-05-31 14:17:52--  https://docs.google.com/uc?export=download&id=1UYvlhdYAdfa4riP_4hn3-IEVd1ZUXVTQ\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.134.138, 74.125.134.101, 74.125.134.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.134.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0s-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/hbbqq7lfk2bh65qvi7924sge2ap9gjeg/1590934650000/16970776037313924126/*/1UYvlhdYAdfa4riP_4hn3-IEVd1ZUXVTQ?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-05-31 14:17:53--  https://doc-0s-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/hbbqq7lfk2bh65qvi7924sge2ap9gjeg/1590934650000/16970776037313924126/*/1UYvlhdYAdfa4riP_4hn3-IEVd1ZUXVTQ?e=download\n",
            "Resolving doc-0s-2s-docs.googleusercontent.com (doc-0s-2s-docs.googleusercontent.com)... 173.194.217.132, 2607:f8b0:400c:c13::84\n",
            "Connecting to doc-0s-2s-docs.googleusercontent.com (doc-0s-2s-docs.googleusercontent.com)|173.194.217.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/Magribi_MSA-train.pkl’\n",
            "\n",
            "data/AD_NMT-master/     [ <=>                ]   2.53M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-05-31 14:17:53 (227 MB/s) - ‘data/AD_NMT-master/Magribi_MSA-train.pkl’ saved [2652508]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-WzhRv4mIKNq",
        "colab": {}
      },
      "source": [
        "#James Chartouni\n",
        "#Joey Park\n",
        "#Raef Khan\n",
        "\n",
        "import torch\n",
        "from torch.optim import SGD\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os, io, glob\n",
        "import functools\n",
        "\n",
        "import sentencepiece as spm\n",
        "from bpemb import BPEmb\n",
        "\n",
        "import transformers\n",
        "import t5\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxCLgMZ2ns4b",
        "colab_type": "text"
      },
      "source": [
        "##Initial Loading from Pickle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T67vZ0Nv4-uw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = 5000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aha0xureIKNw",
        "outputId": "7ae2f9d4-1e14-498e-cd9c-09d35a574bb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "ls data/AD_NMT-master"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "english-Arabic-both.pkl   LAV-MSA-2-both.pkl   Magribi_MSA-both.pkl\n",
            "english-Arabic-test.pkl   LAV-MSA-2-test.pkl   Magribi_MSA-test.pkl\n",
            "english-Arabic-train.pkl  LAV-MSA-2-train.pkl  Magribi_MSA-train.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XWKnGRlLIKN9",
        "colab": {}
      },
      "source": [
        "file_path = 'data/AD_NMT-master/'\n",
        "\n",
        "with open(file_path + \"english-Arabic-train.pkl\", 'rb') as handle:\n",
        "    data_MSA_English_trainval = pickle.load(handle)\n",
        "\n",
        "with open(file_path + \"english-Arabic-test.pkl\", 'rb') as handle:\n",
        "    data_MSA_English_test = pickle.load(handle)\n",
        "\n",
        "with open(file_path + \"english-Arabic-both.pkl\", 'rb') as handle:\n",
        "    data_MSA_English_both = pickle.load(handle) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with open(file_path + \"LAV-MSA-2-train.pkl\", 'rb') as handle:\n",
        "    data_LAV_MSA_trainval = pickle.load(handle) \n",
        "\n",
        "with open(file_path + \"LAV-MSA-2-test.pkl\", 'rb') as handle:\n",
        "    data_LAV_MSA_test = pickle.load(handle) \n",
        "\n",
        "with open(file_path + \"LAV-MSA-2-both.pkl\", 'rb') as handle:\n",
        "    data_LAV_MSA_both = pickle.load(handle) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with open(file_path + \"Magribi_MSA-train.pkl\", 'rb') as handle:\n",
        "    data_Magribi_MSA_trainval = pickle.load(handle) \n",
        "    \n",
        "with open(file_path + \"Magribi_MSA-test.pkl\", 'rb') as handle:\n",
        "    data_Magribi_MSA_test = pickle.load(handle) \n",
        "\n",
        "with open(file_path + \"Magribi_MSA-both.pkl\", 'rb') as handle:\n",
        "    data_Magribi_MSA_both = pickle.load(handle) \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ch3APJadIKOH",
        "outputId": "a1f670d1-62fe-4d02-a58e-8b8a2cf4742d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "#few dataset examples\n",
        "print(data_MSA_English_both[0:5])\n",
        "print(data_LAV_MSA_both[0:5])\n",
        "\n",
        "#print length of train + val dataset, print length of both (train + val) + test\n",
        "print(len(data_MSA_English_trainval))\n",
        "print(len(data_MSA_English_both))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Tom was also there', 'كان توم هنا ايضا'], ['That old woman lives by herself', 'تلك المراة العجوز تسكن بمفردها'], ['He went abroad for the purpose of studying English', 'سافر خارج البلد ليتعلم الانجليزية'], ['There is a fork missing', 'هناك شوكة ناقصة'], [\"I don't know this game\", 'لا اعرف هذه اللعبة']]\n",
            "[['لا انا بعرف وحدة راحت ع فرنسا و معا شنتا حطت فيها الفرش', 'لا اعرف واحدة ذهبت الى فرنسا و لها غرفة و ضعت فيها الافرشة'], ['روح بوشك و فتول عاليسار', 'اذهب تقدم و استدر يسارا'], ['لا لا لازم انه يكون عندك موضوع ما في اشي', ' لا لا يجب ان يكون لديك موضوع هذا ضروري'], ['اوعي تبعدي من هون بلاش تضيعي ', 'لا تبتعد عن هنا حتى لا تفقد الطريق '], ['قصدي صراحة يما انا كمان كرهته من يوم ما عملتيه زي ما بتعمله خالتي كرهته و صرت ما باطيقه بالمرة', 'اقصد صراحة يا امي انا ايضا كرهته من يوم حضرته مثلما تحضره خالتي كرهته و اصبحت لا اطيقه ابدا']]\n",
            "9000\n",
            "10001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "irQQ9E_7IKOM"
      },
      "source": [
        "## Prepare Datasets\n",
        "\n",
        "example: https://iwslt2010.fbk.eu/node/32/\n",
        "\n",
        "We need to take our training and test sets from the pkl files and create new .txt files that are formatted so that the standard torchtext Dataset class can read them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "raw",
        "id": "ikpu9B5dIKON"
      },
      "source": [
        "Data format:\n",
        "each line consists of three fields divided by the character '\\'\n",
        "sentences consisting of words divided by single spaces\n",
        "format: <SENTENCE_ID>\\<PARAPHRASE_ID>\\<TEXT>\n",
        "Field_1: sentence ID\n",
        "Field_2: paraphrase ID\n",
        "Field_3: MT develop sentence / reference translation\n",
        "Text input example:\n",
        "DEV_001\\01\\This is the first develop sentence.\n",
        "DEV_002\\01\\This is the second develop sentence.\n",
        "Reference translation example:\n",
        "DEV_001\\01\\1st reference translation for 1st input\n",
        "DEV_001\\02\\2nd reference translation for 1st input\n",
        "...\n",
        "DEV_002\\01\\1st reference translation for 2nd input\n",
        "DEV_002\\02\\2nd reference translation for 2nd input\n",
        "...\n",
        "Languages:\n",
        "Arabic-English\n",
        "CSTAR03 testset: 506 sentences, 16 reference translations\n",
        "IWSLT04 testset: 500 sentences, 16 reference translations\n",
        "IWSLT05 testset: 506 sentences, 16 reference translations\n",
        "IWSLT07 testset: 489 sentences, 6 reference translations\n",
        "IWSLT08 testset: 507 sentences, 16 reference translations\n",
        "French-English\n",
        "CSTAR03 testset: 506 sentences, 16 reference translations\n",
        "IWSLT04 testset: 500 sentences, 16 reference translations\n",
        "IWSLT05 testset: 506 sentences, 16 reference translations\n",
        "Turkish-English\n",
        "CSTAR03 testset: 506 sentences, 16 reference translations\n",
        "IWSLT04 testset: 500 sentences, 16 reference translations\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7wcnDJDKwqED",
        "outputId": "d51c3663-bfae-4580-e81a-5ad8d7c612b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "ls data/AD_NMT-master/"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "english-Arabic-both.pkl   LAV-MSA-2-both.pkl   Magribi_MSA-both.pkl\n",
            "english-Arabic-test.pkl   LAV-MSA-2-test.pkl   Magribi_MSA-test.pkl\n",
            "english-Arabic-train.pkl  LAV-MSA-2-train.pkl  Magribi_MSA-train.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lp7ncVY6wqET",
        "colab": {}
      },
      "source": [
        "#splits the train dataset into train and validation sets, define test set as datafile\n",
        "msa_en_train, msa_en_val = train_test_split(data_MSA_English_trainval, test_size=.2)\n",
        "msa_en_test = data_MSA_English_test\n",
        "\n",
        "lav_msa_train, lav_msa_val = train_test_split(data_LAV_MSA_trainval, test_size=.2)\n",
        "lav_msa_test = data_LAV_MSA_test\n",
        "\n",
        "mag_msa_train, mag_msa_val = train_test_split(data_Magribi_MSA_trainval, test_size=.2)\n",
        "mag_msa_test = data_Magribi_MSA_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9Ue2fSILwqEW",
        "outputId": "e808690d-d797-4895-bbdc-263367034fd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "print(len(msa_en_train))\n",
        "print(len(msa_en_val))\n",
        "\n",
        "print(len(lav_msa_train))\n",
        "print(len(lav_msa_val))\n",
        "\n",
        "print(len(mag_msa_train))\n",
        "print(len(mag_msa_val))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7200\n",
            "1800\n",
            "11044\n",
            "2761\n",
            "14188\n",
            "3548\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W3bWU46TIKOb",
        "colab": {}
      },
      "source": [
        "file_path = 'data/'\n",
        "\n",
        "def list_to_csv(ds, src='msa', trg='en', datatype=''):\n",
        "    src_formatted = datatype + '_' + src + '_' + trg + '.' + 'csv'\n",
        "    \n",
        "    with open(file_path + datatype + \"/\" + src_formatted, 'wt') as csv:\n",
        "        for i, arr in enumerate(ds):\n",
        "            csv.write(arr[1] + ',' + arr[0] + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AqwDB9BuKEnV",
        "colab": {}
      },
      "source": [
        "list_to_csv(msa_en_train, 'msa', 'en', 'train')\n",
        "list_to_csv(msa_en_val, 'msa', 'en', 'val')\n",
        "list_to_csv(msa_en_test, 'msa', 'en', 'test')\n",
        "\n",
        "list_to_csv(lav_msa_train, 'lav', 'msa', 'train')\n",
        "list_to_csv(lav_msa_val, 'lav', 'msa', 'val')\n",
        "list_to_csv(lav_msa_test, 'lav', 'msa', 'test')\n",
        "\n",
        "list_to_csv(mag_msa_train, 'mag', 'msa', 'train')\n",
        "list_to_csv(mag_msa_val, 'mag', 'msa', 'val')\n",
        "list_to_csv(mag_msa_test, 'mag', 'msa', 'test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AgOklFxmWEr",
        "colab_type": "code",
        "outputId": "c6f9ce19-38ef-477e-de6e-f1d56058e09d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "multibpemb = BPEmb(lang=\"multi\", vs=1000000, dim=300)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/multi/multi.wiki.bpe.vs1000000.model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20636145/20636145 [00:01<00:00, 13831409.82B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/multi/multi.wiki.bpe.vs1000000.d300.w2v.bin.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1123372891/1123372891 [00:47<00:00, 23755278.48B/s]\n",
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrHAuzNW1eIh",
        "colab_type": "text"
      },
      "source": [
        "##Tensor Processing + Add to TaskRegistry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oVVzdeAZd0x",
        "colab_type": "text"
      },
      "source": [
        "### English to Arabic Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWnZcYI_491p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "msa_en_split_csv_path = {\n",
        "    \"train\": \"data/train/train_msa_en.csv\",\n",
        "    \"validation\": \"data/val/val_msa_en.csv\"\n",
        "}\n",
        "msa_en_example_count = {\n",
        "    \"train\": 7200,\n",
        "    \"validation\": 1800\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2e3cM_wP_M8",
        "colab_type": "code",
        "outputId": "eb865b70-ee3c-4353-b552-f8e2a259a8ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "def msa_en_translation_dataset_fn(split, shuffle_files=False):\n",
        "  ds = tf.data.TextLineDataset(msa_en_split_csv_path[split])\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"\",\"\"],\n",
        "                        field_delim=\",\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "  )\n",
        "  ds = ds.map(lambda *example: dict(zip([\"source\", \"target\"], example)) )\n",
        "  return ds\n",
        "\n",
        "for example in tfds.as_numpy(msa_en_translation_dataset_fn(\"train\").take(5)):\n",
        "    print(example)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'source': b'\\xd8\\xa7\\xd8\\xb1\\xd9\\x8a\\xd8\\xaf \\xd8\\xa7\\xd9\\x86 \\xd9\\x8a\\xd9\\x81\\xd9\\x87\\xd9\\x85 \\xd8\\xaa\\xd9\\x88\\xd9\\x85', 'target': b'I want Tom to understand'}\n",
            "{'source': b'\\xd9\\x88\\xd9\\x82\\xd8\\xb9\\xd8\\xaa \\xd9\\x85\\xd8\\xb1\\xd8\\xa7\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xb1\\xd8\\xa4\\xd9\\x8a\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xae\\xd9\\x84\\xd9\\x81\\xd9\\x8a\\xd8\\xa9 ', 'target': b'The rearview mirror fell off'}\n",
            "{'source': b'\\xd9\\x84\\xd9\\x8a\\xd8\\xb3 \\xd8\\xb9\\xd9\\x84\\xd9\\x8a\\xd9\\x83 \\xd8\\xa7\\xd9\\x86 \\xd8\\xaa\\xd8\\xaf\\xd8\\xb1\\xd8\\xb3', 'target': b\"You don't have to study\"}\n",
            "{'source': b'\\xd8\\xa7\\xd8\\xb1\\xd8\\xaa\\xd8\\xac\\xd9\\x81\\xd8\\xaa \\xd8\\xae\\xd9\\x88\\xd9\\x81\\xd8\\xa7', 'target': b'She trembled with fear'}\n",
            "{'source': b'\\xd9\\x87\\xd9\\x84 \\xd8\\xaa\\xd8\\xb9\\xd8\\xa7\\xd8\\xb1\\xd8\\xb6 \\xd8\\xa7\\xd9\\x84\\xd8\\xaa\\xd8\\xaf\\xd8\\xae\\xd9\\x8a\\xd9\\x86\\xd8\\x9f', 'target': b'Do you object to smoking?'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1GXED2IO3pE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#turn the ds of dictionaries and change the keys to inputs and targets that the model\n",
        "def msa_en_translation_preprocessor(ds):\n",
        "  def to_inputs_and_targets(ex):\n",
        "    return{\n",
        "        \"inputs\": tf.strings.join([\"Translate MSA to English: \",ex[\"source\"]]),\n",
        "        \"targets\": ex[\"target\"]\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag6soPRLhcwY",
        "colab_type": "code",
        "outputId": "1dd87fa8-2939-4d72-e3a6-083473cae67d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "filepath = str(multibpemb.model_file)\n",
        "print(filepath)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/.cache/bpemb/multi/multi.wiki.bpe.vs1000000.model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-W6j-SDXcNT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t5.data.TaskRegistry.add()\n",
        "t5.data.TaskRegistry.remove(\"msa_en_translation\")\n",
        "t5.data.TaskRegistry.add(\n",
        "    #name of the Task\n",
        "    \"msa_en_translation\",\n",
        "    #Supply a function which returns a tf.data.Dataset\n",
        "    dataset_fn=msa_en_translation_dataset_fn,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[msa_en_translation_preprocessor],\n",
        "    # Lowercase targets before computing metrics.\n",
        "    postprocess_fn = t5.data.postprocessors.lower_text, \n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy],\n",
        "    # Not required, but helps for mixing and auto-caching.\n",
        "    num_input_examples=msa_en_example_count\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGr8wt7naDRL",
        "colab_type": "text"
      },
      "source": [
        "###Levantine to MSA Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e3A3wQdjagH6",
        "colab": {}
      },
      "source": [
        "lav_msa_split_csv_path = {\n",
        "    \"train\": \"data/train/train_lav_msa.csv\",\n",
        "    \"validation\": \"data/val/val_lav_msa.csv\"\n",
        "}\n",
        "lav_msa_example_count = {\n",
        "    \"train\": 11044,\n",
        "    \"validation\": 2761\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c0757203-2740-44ea-9736-8b34c647cbb1",
        "id": "xCK_j-9BagH_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "def lav_msa_translation_dataset_fn(split, shuffle_files=False):\n",
        "  ds = tf.data.TextLineDataset(lav_msa_split_csv_path[split])\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"\",\"\"],\n",
        "                        field_delim=\",\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "  )\n",
        "  ds = ds.map(lambda *example: dict(zip([\"source\", \"target\"], example)) )\n",
        "  return ds\n",
        "\n",
        "for example in tfds.as_numpy(lav_msa_translation_dataset_fn(\"train\").take(5)):\n",
        "    print(example)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'source': b'\\xd9\\x88 \\xd9\\x87\\xd9\\x84 \\xd8\\xb3\\xd8\\xa7\\xd9\\x83\\xd9\\x84\\xd9\\x87\\xd8\\xa7 \\xd9\\x85\\xd8\\xb1\\xd8\\xa9 \\xd9\\x88\\xd8\\xa7\\xd8\\xad\\xd8\\xaf\\xd8\\xa9', 'target': b'\\xd9\\x88 \\xd9\\x87\\xd9\\x88 \\xd8\\xad\\xd8\\xa7\\xd9\\x83\\xd9\\x84\\xd9\\x87\\xd8\\xa7 \\xd9\\x85\\xd8\\xb1\\xd8\\xa9 \\xd9\\x88\\xd8\\xad\\xd8\\xaf\\xd8\\xa9'}\n",
            "{'source': b'\\xd9\\x86\\xd8\\xb9\\xd9\\x85 \\xd8\\xa7\\xd8\\xaf\\xd8\\xb1\\xd8\\xb3', 'target': b'\\xd8\\xa7\\xd9\\x87 \\xd8\\xa8\\xd8\\xa7\\xd8\\xaf\\xd8\\xb1\\xd8\\xb3'}\n",
            "{'source': b'\\xd8\\xa7\\xd8\\xac\\xd9\\x84 \\xd9\\x88 \\xd9\\x87\\xd9\\x88 \\xd9\\x83\\xd8\\xb0\\xd9\\x84\\xd9\\x83  \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd9\\x87\\xd9\\x85 \\xd8\\xa7\\xd8\\xb9\\xd8\\xa7\\xd9\\x86\\xd9\\x83\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd9\\x84\\xd9\\x87', 'target': b'\\xd8\\xa7\\xd9\\x87 \\xd9\\x88 \\xd9\\x87\\xd9\\x88 \\xd9\\x83\\xd8\\xb0\\xd9\\x84\\xd9\\x83 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd9\\x87\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd9\\x84\\xd9\\x87 \\xd9\\x8a\\xd8\\xb9\\xd9\\x8a\\xd9\\x86\\xd9\\x83\\xd9\\x85'}\n",
            "{'source': b'\\xd8\\xb9\\xd9\\x84\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd9\\x86\\xd9\\x81\\xd8\\xb3 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xb1\\xd8\\xb6\\xd9\\x8a \\xd9\\x85\\xd9\\x82\\xd9\\x8a\\xd8\\xa7\\xd8\\xb3 \\xd9\\x85\\xd8\\xaf\\xd8\\xaa\\xd9\\x87 \\xd8\\xb9\\xd8\\xa7\\xd9\\x85\\xd8\\xa7\\xd9\\x86', 'target': b'\\xd8\\xb9\\xd9\\x84\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd9\\x86\\xd9\\x81\\xd8\\xb3 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xb1\\xd8\\xb6\\xd9\\x8a \\xd9\\x85\\xd9\\x82\\xd9\\x8a\\xd8\\xa7\\xd8\\xb3 \\xd9\\x85\\xd8\\xaf\\xd8\\xaa\\xd9\\x88 \\xd8\\xb3\\xd9\\x86\\xd8\\xaa\\xd9\\x8a\\xd9\\x86'}\n",
            "{'source': b' \\xd9\\x8a\\xd8\\xb2\\xd8\\xb1\\xd8\\xb9 \\xd8\\xb5\\xd9\\x81\\xd8\\xb5\\xd8\\xa7\\xd9\\x81\\xd9\\x87 \\xd9\\x83\\xd8\\xa8\\xd9\\x8a\\xd8\\xb1\\xd8\\xa9', 'target': b'\\xd8\\xa8\\xd9\\x8a\\xd8\\xb2\\xd8\\xb1\\xd8\\xb9 \\xd8\\xb5\\xd9\\x81\\xd8\\xb5\\xd8\\xa7\\xd9\\x81\\xd9\\x87 \\xd9\\x83\\xd8\\xa8\\xd9\\x8a\\xd8\\xb1\\xd8\\xa9 '}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fW9eNYoGagID",
        "colab": {}
      },
      "source": [
        "#turn the ds of dictionaries and change the keys to inputs and targets that the model\n",
        "def lav_msa_translation_preprocessor(ds):\n",
        "  def to_inputs_and_targets(ex):\n",
        "    return{\n",
        "        \"inputs\": tf.strings.join([\"Translate Levantine to MSA: \",ex[\"source\"]]),\n",
        "        \"targets\": ex[\"target\"]\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mDoUsP1AagIG",
        "colab": {}
      },
      "source": [
        "t5.data.TaskRegistry.remove(\"lav_msa_translation\")\n",
        "t5.data.TaskRegistry.add(\n",
        "    #name of the Task\n",
        "    \"lav_msa_translation\",\n",
        "    #Supply a function which returns a tf.data.Dataset\n",
        "    dataset_fn=lav_msa_translation_dataset_fn,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[lav_msa_translation_preprocessor],\n",
        "    # Lowercase targets before computing metrics.\n",
        "    postprocess_fn = t5.data.postprocessors.lower_text, \n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy],\n",
        "    # Not required, but helps for mixing and auto-caching.\n",
        "    num_input_examples=lav_msa_example_count\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wIK8EWyaLFR",
        "colab_type": "text"
      },
      "source": [
        "###Maghrib to MSA Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bVhYmQnQaj2K",
        "colab": {}
      },
      "source": [
        "mag_msa_split_csv_path = {\n",
        "    \"train\": \"data/train/train_mag_msa.csv\",\n",
        "    \"validation\": \"data/val/val_mag_msa.csv\"\n",
        "}\n",
        "mag_msa_example_count = {\n",
        "    \"train\": 14188,\n",
        "    \"validation\": 3548\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1aa0e840-9ac3-4081-b871-853c3a5cec2b",
        "id": "WBLF-J2Oaj2T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "def mag_msa_translation_dataset_fn(split, shuffle_files=False):\n",
        "  ds = tf.data.TextLineDataset(mag_msa_split_csv_path[split])\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"\",\"\"],\n",
        "                        field_delim=\",\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "  )\n",
        "  ds = ds.map(lambda *example: dict(zip([\"source\", \"target\"], example)) )\n",
        "  return ds\n",
        "\n",
        "for example in tfds.as_numpy(mag_msa_translation_dataset_fn(\"train\").take(5)):\n",
        "    print(example)\n",
        "    print(example['source'].decode())\n",
        "    print(example['target'].decode())\n",
        "    print(len(example['source']))\n",
        "    print(len(example['target']))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'source': b'\\xd9\\x88\\xd8\\xa8\\xd8\\xb9\\xd8\\xb6 \\xd8\\xa7\\xd9\\x84\\xd9\\x86\\xd8\\xa7\\xd8\\xb3 \\xd9\\x8a\\xd8\\xba\\xd8\\xb6\\xd8\\xa8\\xd9\\x88\\xd9\\x86 \\xd8\\xb9\\xd9\\x84\\xd9\\x89 \\xd8\\xa8\\xd8\\xb9\\xd8\\xb6\\xd9\\x87\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd8\\xa8\\xd8\\xb9\\xd8\\xb6 \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd8\\xb9\\xd8\\xb2\\xd8\\xa7\\xd8\\xa1 \\xd9\\x8a\\xd8\\xaa\\xd8\\xb1\\xd9\\x83\\xd9\\x88\\xd9\\x86 \\xd8\\xa8\\xd8\\xb9\\xd8\\xb6\\xd9\\x87\\xd9\\x85', 'target': b'\\xd9\\x81\\xd9\\x85\\xd8\\xa7 \\xd9\\x86\\xd8\\xa7\\xd8\\xb3 \\xd9\\x8a\\xd8\\xaa\\xd8\\xba\\xd8\\xb4\\xd8\\xb4\\xd9\\x88\\xd8\\xa7 \\xd8\\xb9\\xd9\\x84\\xd9\\x89 \\xd8\\xa8\\xd8\\xb9\\xd8\\xb6\\xd9\\x87\\xd9\\x85 \\xd9\\x88\\xd8\\xa7\\xd8\\xad\\xd8\\xa8\\xd8\\xa7\\xd8\\xa8 \\xd9\\x8a\\xd8\\xa8\\xd8\\xb9\\xd8\\xaf\\xd9\\x88\\xd8\\xa7 \\xd8\\xb9\\xd9\\x84\\xd9\\x89 \\xd8\\xa8\\xd8\\xb9\\xd8\\xb6\\xd9\\x87\\xd9\\x85'}\n",
            "وبعض الناس يغضبون على بعضهم البعض والاعزاء يتركون بعضهم\n",
            "فما ناس يتغششوا على بعضهم واحباب يبعدوا على بعضهم\n",
            "102\n",
            "90\n",
            "{'source': b'\\xd9\\x85\\xd8\\xb3\\xd8\\xa7\\xd8\\xa1 \\xd8\\xa7\\xd9\\x84\\xd8\\xae\\xd9\\x8a\\xd8\\xb1  \\xd9\\x84\\xd8\\xa7\\xd8\\xa8\\xd8\\xa7\\xd8\\xb3', 'target': b'\\xd9\\x85\\xd8\\xb3\\xd8\\xa7 \\xd8\\xa7\\xd9\\x84\\xd8\\xae\\xd9\\x8a\\xd8\\xb1 \\xd8\\xb3\\xd8\\xa7\\xda\\xa2\\xd8\\xa7'}\n",
            "مساء الخير  لاباس\n",
            "مسا الخير ساڢا\n",
            "31\n",
            "26\n",
            "{'source': b'\\xd9\\x82\\xd8\\xa7\\xd9\\x84 \\xd9\\x84\\xd9\\x8a \\xd8\\xa7\\xd9\\x84\\xd8\\xb7\\xd8\\xa8\\xd9\\x8a\\xd8\\xa8  \\xd9\\x8a\\xd8\\xac\\xd8\\xa8 \\xd8\\xa7\\xd9\\x86 \\xd8\\xaa\\xd8\\xac\\xd8\\xb1\\xd9\\x8a \\xd8\\xb9\\xd9\\x85\\xd9\\x84\\xd9\\x8a\\xd8\\xa9 \\xd8\\xac\\xd8\\xb1\\xd8\\xa7\\xd8\\xad\\xd9\\x8a\\xd8\\xa9 \\xd8\\xb9\\xd9\\x84\\xd9\\x89 \\xd9\\x85\\xd8\\xb3\\xd8\\xaa\\xd9\\x88\\xd9\\x89 \\xd8\\xa7\\xd9\\x84\\xd9\\x82\\xd9\\x84\\xd8\\xa8', 'target': b'\\xda\\xad\\xd8\\xa7\\xd9\\x84 \\xd9\\x84\\xd9\\x8a \\xd8\\xa7\\xd9\\x84\\xd8\\xb7\\xd8\\xa8\\xd9\\x8a\\xd8\\xa8 \\xd8\\xae\\xd8\\xa7\\xd8\\xb5\\xd9\\x83 \\xd8\\xb9\\xd9\\x85\\xd9\\x84\\xd9\\x8a\\xd8\\xa9 \\xd8\\xaf\\xd9\\x8a\\xd8\\xa7\\xd9\\x84 \\xd8\\xa7\\xd9\\x84\\xd9\\x82\\xd9\\x84\\xd8\\xa8'}\n",
            "قال لي الطبيب  يجب ان تجري عملية جراحية على مستوى القلب\n",
            "ڭال لي الطبيب خاصك عملية ديال القلب\n",
            "99\n",
            "64\n",
            "{'source': b'\\xd9\\x85\\xd9\\x8a\\xd9\\x85\\xd9\\x8a \\xd8\\xaf\\xd8\\xb9\\xd9\\x8a \\xd9\\x87\\xd8\\xa7\\xd8\\xaa\\xd9\\x81\\xd9\\x83 \\xd9\\x88 \\xd8\\xb4\\xd8\\xa7\\xd9\\x86\\xd9\\x87 \\xd9\\x84\\xd9\\x82\\xd8\\xaf \\xd8\\xac\\xd9\\x86 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xb3\\xd9\\x83\\xd9\\x8a\\xd9\\x86', 'target': b'\\xd9\\x85\\xd9\\x8a\\xd9\\x85\\xd9\\x8a \\xd8\\xae\\xd9\\x84\\xd9\\x8a \\xd8\\xa7\\xd9\\x84\\xd8\\xaa\\xd9\\x84\\xd9\\x8a\\xd9\\x81\\xd9\\x88\\xd9\\x86 \\xd9\\x88 \\xd8\\xae\\xd9\\x84\\xd9\\x8a\\xd9\\x87 \\xd8\\xb9\\xd9\\x84\\xd9\\x89 \\xd8\\xae\\xd8\\xa7\\xd8\\xb7\\xd8\\xb1\\xd9\\x88 \\xd8\\xb1\\xd8\\xa7\\xd9\\x87 \\xd9\\x85\\xd8\\xb3\\xd9\\x83\\xd9\\x8a\\xd9\\x86 \\xd8\\xad\\xd9\\x85\\xd9\\x82\\xd8\\xaa\\xd9\\x8a\\xd9\\x87'}\n",
            "ميمي دعي هاتفك و شانه لقد جن المسكين\n",
            "ميمي خلي التليفون و خليه على خاطرو راه مسكين حمقتيه\n",
            "65\n",
            "93\n",
            "{'source': b'\\xd8\\xa7\\xd9\\x84\\xd8\\xb3\\xd9\\x84\\xd8\\xa7\\xd9\\x85 \\xd8\\xb9\\xd9\\x84\\xd9\\x8a\\xd9\\x83\\xd9\\x85', 'target': b'\\xd8\\xa7\\xd9\\x84\\xd8\\xb3\\xd9\\x84\\xd8\\xa7\\xd9\\x85 \\xd8\\xb9\\xd9\\x84\\xd9\\x8a\\xd9\\x83\\xd9\\x85'}\n",
            "السلام عليكم\n",
            "السلام عليكم\n",
            "23\n",
            "23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GgyqkGfYaj2Y",
        "colab": {}
      },
      "source": [
        "#turn the ds of dictionaries and change the keys to inputs and targets that the model\n",
        "def mag_msa_translation_preprocessor(ds):\n",
        "  def to_inputs_and_targets(ex):\n",
        "    return{\n",
        "        \"inputs\": tf.strings.join([\"Translate Maghrib to Arabic: \",ex[\"source\"]]),\n",
        "        \"targets\": ex[\"target\"]\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bmja1M2Eaj2f",
        "colab": {}
      },
      "source": [
        "t5.data.TaskRegistry.remove(\"mag_msa_translation\")\n",
        "t5.data.TaskRegistry.add(\n",
        "    #name of the Task\n",
        "    \"mag_msa_translation\",\n",
        "    #Supply a function which returns a tf.data.Dataset\n",
        "    dataset_fn=mag_msa_translation_dataset_fn,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[mag_msa_translation_preprocessor],\n",
        "    # Lowercase targets before computing metrics.\n",
        "    postprocess_fn = t5.data.postprocessors.lower_text, \n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy],\n",
        "    # Not required, but helps for mixing and auto-caching.\n",
        "    num_input_examples=mag_msa_example_count\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLoe7vbFV42o",
        "colab_type": "text"
      },
      "source": [
        "##Dataset Mixture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWMTtMsXWAua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t5.data.MixtureRegistry.remove(\"ar_translation\")\n",
        "t5.data.MixtureRegistry.add(\n",
        "    \"ar_translation\",\n",
        "    [\"msa_en_translation\", \"lav_msa_translation\", \"mag_msa_translation\"],\n",
        "     default_rate=1.0\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc0bSgP1WenA",
        "colab_type": "text"
      },
      "source": [
        "##Fine Tune"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M23Y4l0tb4qM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls /tmp/hft5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uP2fwStXK5b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "#Using the Huggingface T5 Model\n",
        "model = t5.models.HfPyTorchModel(\"t5-base\", \"/tmp/hft5/\", device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6f5uUWXWUKw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "STEPS = 1000 #@param {type: \"integer\"}\n",
        "model.train(\n",
        "    mixture_or_task_name=\"ar_translation\",\n",
        "    steps=STEPS,\n",
        "    save_steps=100,\n",
        "    sequence_length={\"inputs\": 128, \"targets\": 128},\n",
        "    split=\"train\",\n",
        "    batch_size=32, #32\n",
        "    optimizer=functools.partial(transformers.AdamW, lr=1e-4),\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev_E4ZadXxmj",
        "colab_type": "text"
      },
      "source": [
        "##Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjZnTOIrYEh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate after fine-tuning\n",
        "model.eval(\n",
        "    \"ar_translation\",\n",
        "    checkpoint_steps=\"all\",\n",
        "    sequence_length={\"inputs\": 64, \"targets\": 64},\n",
        "    batch_size=128,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF-3_4u1X-5r",
        "colab_type": "text"
      },
      "source": [
        "##Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL5yLXs4YJw7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = [\n",
        "    \"translation: This is a totally valid sentence.\",\n",
        "    \"translation: A doggy detail was walking famously.\",\n",
        "]\n",
        "model.predict(\n",
        "    inputs,\n",
        "    sequence_length={\"inputs\": 32},\n",
        "    batch_size=2,\n",
        "    output_file=\"/tmp/hft5/example_predictions.txt\",\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}