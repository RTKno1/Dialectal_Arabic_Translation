{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BQflUKhjIKNp"
   },
   "source": [
    "### Notes \n",
    "\n",
    "T5 Paper: https://arxiv.org/pdf/1910.10683.pdf\n",
    "\n",
    "T5 Tokenizer: https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_t5.py\n",
    "\n",
    "Important Tasks: https://docs.google.com/document/d/1weIZM6QTlnitpPQmpg-WeV2RW70TnYmDuogBQPr5mB0/edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "XLOmiOta6MJp",
    "outputId": "ff45d283-9268-43e2-8f22-6f20312d5c4c"
   },
   "outputs": [],
   "source": [
    "#installation step\n",
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install utils\n",
    "#creating the folders \n",
    "!mkdir data/\n",
    "!mkdir data/AD_NMT-master\n",
    "!mkdir data/train/\n",
    "!mkdir data/test/\n",
    "!mkdir data/val/\n",
    "!mkdir data/vocab/\n",
    "!mkdir data/model/\n",
    "#fetching the pkl files\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW' -O data/AD_NMT-master/english-Arabic-both.pkl\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1V8_tp8ZlWUYaX7QQL46t0uSRNrVehSf1' -O data/AD_NMT-master/english-Arabic-test.pkl\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1V7X0qtuDIyjTHY0wh-ZNoVwsiF4lId2e' -O data/AD_NMT-master/english-Arabic-train.pkl\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV' -O data/AD_NMT-master/LAV-MSA-2-both.pkl\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UpfCbkxhztof7dvNjeAs1bHjD4SER6h3' -O data/AD_NMT-master/LAV-MSA-2-test.pkl\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UlAZGtYsSfXzK7hrC_PbxQFqTSXD0DMw' -O data/AD_NMT-master/LAV-MSA-2-train.pkl\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UjDX7cCG2S23SPfSHxSPdVayMTxB5Y16' -O data/AD_NMT-master/Magribi_MSA-both.pkl\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UaVWIqRXo0rxuxDF4KArA4bEK1TaLX3l' -O data/AD_NMT-master/Magribi_MSA-test.pkl\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UYvlhdYAdfa4riP_4hn3-IEVd1ZUXVTQ' -O data/AD_NMT-master/Magribi_MSA-train.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-WzhRv4mIKNq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#James Chartouni\n",
    "#Joey Park\n",
    "#Raef Khan\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os, io, glob, time\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtext.datasets.translation import TranslationDataset\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config, T5Model\n",
    "\n",
    "\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.vocab import Vocab\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import *\n",
    "from custom_field import *\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T67vZ0Nv4-uw"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "aha0xureIKNw",
    "outputId": "5d7fd491-112e-4d7e-a35d-0bef23512fdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAV-MSA-2-both.pkl        Magribi_MSA-train.pkl     english-arabic-test.pkl\r\n",
      "LAV-MSA-2-test.pkl        README.md                 lav_formatted_.txt\r\n",
      "LAV-MSA-2-train.pkl       en_formatted_.txt         mag_formatted_.txt\r\n",
      "Magribi_MSA-both.pkl      english-Arabic-both.pkl   msa_formatted_.txt\r\n",
      "Magribi_MSA-test.pkl      english-Arabic-train.pkl\r\n"
     ]
    }
   ],
   "source": [
    "ls data/AD_NMT-master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XWKnGRlLIKN9"
   },
   "outputs": [],
   "source": [
    "file_path = 'data/AD_NMT-master/'\n",
    "\n",
    "with open(file_path + \"english-Arabic-train.pkl\", 'rb') as handle:\n",
    "    data_English_MSA_trainval = pickle.load(handle)\n",
    "\n",
    "with open(file_path + \"english-Arabic-test.pkl\", 'rb') as handle:\n",
    "    data_English_MSA_test = pickle.load(handle)\n",
    "\n",
    "with open(file_path + \"english-Arabic-both.pkl\", 'rb') as handle:\n",
    "    data_English_MSA_both = pickle.load(handle) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(file_path + \"LAV-MSA-2-train.pkl\", 'rb') as handle:\n",
    "    data_LAV_MSA_trainval = pickle.load(handle) \n",
    "\n",
    "with open(file_path + \"LAV-MSA-2-test.pkl\", 'rb') as handle:\n",
    "    data_LAV_MSA_test = pickle.load(handle) \n",
    "\n",
    "with open(file_path + \"LAV-MSA-2-both.pkl\", 'rb') as handle:\n",
    "    data_LAV_MSA_both = pickle.load(handle) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(file_path + \"Magribi_MSA-train.pkl\", 'rb') as handle:\n",
    "    data_Magribi_MSA_trainval = pickle.load(handle) \n",
    "    \n",
    "with open(file_path + \"Magribi_MSA-test.pkl\", 'rb') as handle:\n",
    "    data_Magribi_MSA_test = pickle.load(handle) \n",
    "\n",
    "with open(file_path + \"Magribi_MSA-both.pkl\", 'rb') as handle:\n",
    "    data_Magribi_MSA_both = pickle.load(handle) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "Ch3APJadIKOH",
    "outputId": "560319c8-3dc1-4003-fc3b-d2324e794050"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Tom was also there', 'كان توم هنا ايضا'], ['That old woman lives by herself', 'تلك المراة العجوز تسكن بمفردها'], ['He went abroad for the purpose of studying English', 'سافر خارج البلد ليتعلم الانجليزية'], ['There is a fork missing', 'هناك شوكة ناقصة'], [\"I don't know this game\", 'لا اعرف هذه اللعبة']]\n",
      "[['لا انا بعرف وحدة راحت ع فرنسا و معا شنتا حطت فيها الفرش', 'لا اعرف واحدة ذهبت الى فرنسا و لها غرفة و ضعت فيها الافرشة'], ['روح بوشك و فتول عاليسار', 'اذهب تقدم و استدر يسارا'], ['لا لا لازم انه يكون عندك موضوع ما في اشي', ' لا لا يجب ان يكون لديك موضوع هذا ضروري'], ['اوعي تبعدي من هون بلاش تضيعي ', 'لا تبتعد عن هنا حتى لا تفقد الطريق '], ['قصدي صراحة يما انا كمان كرهته من يوم ما عملتيه زي ما بتعمله خالتي كرهته و صرت ما باطيقه بالمرة', 'اقصد صراحة يا امي انا ايضا كرهته من يوم حضرته مثلما تحضره خالتي كرهته و اصبحت لا اطيقه ابدا']]\n",
      "9000\n",
      "10001\n"
     ]
    }
   ],
   "source": [
    "print(data_English_MSA_both[0:5])\n",
    "print(data_LAV_MSA_both[0:5])\n",
    "print(len(data_English_MSA_trainval))\n",
    "print(len(data_English_MSA_both))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "irQQ9E_7IKOM"
   },
   "source": [
    "## Prepare Datasets\n",
    "\n",
    "example: https://iwslt2010.fbk.eu/node/32/\n",
    "\n",
    "We need to take our training and test sets from the pkl files and create new .txt files that are formatted so that the standard torchtext Dataset class can read them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "raw",
    "id": "ikpu9B5dIKON"
   },
   "source": [
    "Data format:\n",
    "each line consists of three fields divided by the character '\\'\n",
    "sentences consisting of words divided by single spaces\n",
    "format: <SENTENCE_ID>\\<PARAPHRASE_ID>\\<TEXT>\n",
    "Field_1: sentence ID\n",
    "Field_2: paraphrase ID\n",
    "Field_3: MT develop sentence / reference translation\n",
    "Text input example:\n",
    "DEV_001\\01\\This is the first develop sentence.\n",
    "DEV_002\\01\\This is the second develop sentence.\n",
    "Reference translation example:\n",
    "DEV_001\\01\\1st reference translation for 1st input\n",
    "DEV_001\\02\\2nd reference translation for 1st input\n",
    "...\n",
    "DEV_002\\01\\1st reference translation for 2nd input\n",
    "DEV_002\\02\\2nd reference translation for 2nd input\n",
    "...\n",
    "Languages:\n",
    "Arabic-English\n",
    "CSTAR03 testset: 506 sentences, 16 reference translations\n",
    "IWSLT04 testset: 500 sentences, 16 reference translations\n",
    "IWSLT05 testset: 506 sentences, 16 reference translations\n",
    "IWSLT07 testset: 489 sentences, 6 reference translations\n",
    "IWSLT08 testset: 507 sentences, 16 reference translations\n",
    "French-English\n",
    "CSTAR03 testset: 506 sentences, 16 reference translations\n",
    "IWSLT04 testset: 500 sentences, 16 reference translations\n",
    "IWSLT05 testset: 506 sentences, 16 reference translations\n",
    "Turkish-English\n",
    "CSTAR03 testset: 506 sentences, 16 reference translations\n",
    "IWSLT04 testset: 500 sentences, 16 reference translations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "7wcnDJDKwqED",
    "outputId": "319a799d-eb12-498a-d919-5354624dad11"
   },
   "outputs": [],
   "source": [
    "ls data/AD_NMT-master/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lp7ncVY6wqET"
   },
   "outputs": [],
   "source": [
    "#splits the train dataset into train and validation sets, define test set as datafile\n",
    "eng_msa_train, eng_msa_val = train_test_split(data_English_MSA_trainval, test_size=.2, random_state=22)\n",
    "eng_msa_test = data_English_MSA_test\n",
    "\n",
    "lav_msa_train, lav_msa_val = train_test_split(data_LAV_MSA_trainval, test_size=.2, random_state=22)\n",
    "lav_msa_test = data_LAV_MSA_test\n",
    "\n",
    "mag_msa_train, mag_msa_val = train_test_split(data_Magribi_MSA_trainval, test_size=.2, random_state=22)\n",
    "mag_msa_test = data_Magribi_MSA_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "9Ue2fSILwqEW",
    "outputId": "04d6b2f9-f91f-4731-a589-d6d83d02b52a"
   },
   "outputs": [],
   "source": [
    "print(len(eng_msa_train))\n",
    "print(len(eng_msa_val))\n",
    "\n",
    "print(len(lav_msa_train))\n",
    "print(len(lav_msa_val))\n",
    "\n",
    "print(len(mag_msa_train))\n",
    "print(len(mag_msa_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W3bWU46TIKOb"
   },
   "outputs": [],
   "source": [
    "file_path = 'data/'\n",
    "\n",
    "def pytorch_format(ds, src='en', trg='msa', datatype=''):\n",
    "    src_formatted = datatype + '_' + src + '_' + trg + '.' + src\n",
    "    trg_formatted = datatype + '_' + src + '_' + trg + '.' + trg\n",
    "    \n",
    "    with open(file_path + datatype + \"/\" + src_formatted, 'wt') as srctxt, open(file_path + datatype + \"/\" + trg_formatted, 'wt') as trgtxt:\n",
    "        for i, arr in enumerate(ds):\n",
    "            srctxt.write(datatype.upper() + '_' + str(i).zfill( len(str(len(ds))) - len(str(i))) + '\\\\01\\\\' + arr[0] + '\\n')\n",
    "            trgtxt.write(datatype.upper() + '_' + str(i).zfill( len(str(len(ds))) - len(str(i))) + '\\\\01\\\\' + arr[1] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AqwDB9BuKEnV"
   },
   "outputs": [],
   "source": [
    "#splits each language pair file into datasets of single language, to be merged again by the pytorch dataset class later\n",
    "\n",
    "pytorch_format(eng_msa_train, 'eng', 'msa', 'train')\n",
    "pytorch_format(eng_msa_val, 'eng', 'msa', 'val')\n",
    "pytorch_format(eng_msa_test, 'eng', 'msa', 'test')\n",
    "\n",
    "pytorch_format(lav_msa_train, 'lav', 'msa', 'train')\n",
    "pytorch_format(lav_msa_val, 'lav', 'msa', 'val')\n",
    "pytorch_format(lav_msa_test, 'lav', 'msa', 'test')\n",
    "\n",
    "pytorch_format(mag_msa_train, 'mag', 'msa', 'train')\n",
    "pytorch_format(mag_msa_val, 'mag', 'msa', 'val')\n",
    "pytorch_format(mag_msa_test, 'mag', 'msa', 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lDPvs9zxIKO3"
   },
   "source": [
    "## Build Vocabulary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ihevk2SWIKO5"
   },
   "source": [
    "Sentence Piece Google Colab\n",
    "https://github.com/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I25H3IsyIKPB"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Testing:\n",
    "Create a text file with all the vocab available from all sources for each language for SentencePiece to create a library \n",
    "\n",
    "TODO: implement bpemb model for MSA vocab/train SPM on larger datasets for dialects\n",
    "\"\"\"\n",
    "\n",
    "en_vocab = open(\"data/vocab/eng_vocab.txt\", \"wt\")\n",
    "msa_vocab = open(\"data/vocab/msa_vocab.txt\", \"wt\")\n",
    "lav_vocab = open(\"data/vocab/lav_vocab.txt\", \"wt\")\n",
    "mag_vocab = open(\"data/vocab/mag_vocab.txt\", \"wt\")\n",
    "\n",
    "MSA_text = \"\"\n",
    "EN_text = \"\"\n",
    "\n",
    "def create_vocab(file='', src='en_vocab', tgt='msa_vocab'):\n",
    "  for line in file:\n",
    "        src_sent = line[0]\n",
    "        src_words = src_sent.split(\" \")\n",
    "        for count, word in enumerate(src_words):\n",
    "            src.write(word)\n",
    "        src.write(\"\\n\")\n",
    "        \n",
    "        tgt_sent = line[1]\n",
    "        tgt_words = tgt_sent.split(\" \")\n",
    "        for count, word in enumerate(tgt_words):\n",
    "            tgt.write(word)\n",
    "        tgt.write(\"\\n\")\n",
    "\n",
    "create_vocab(data_English_MSA_both, en_vocab, msa_vocab)\n",
    "create_vocab(data_LAV_MSA_both, lav_vocab, msa_vocab)\n",
    "create_vocab(data_Magribi_MSA_both, mag_vocab, msa_vocab)\n",
    "\n",
    "en_vocab.close()\n",
    "msa_vocab.close()\n",
    "lav_vocab.close()\n",
    "mag_vocab.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Q0qG8N68IKPI",
    "outputId": "f850842c-8c28-445a-88c7-9e62fcece427"
   },
   "outputs": [],
   "source": [
    "#spm.SentencePieceTrainer.train('--input=data/vocab/eng_vocab.txt,data/vocab/msa_vocab.txt,data/vocab/lav_vocab.txt,data/vocab/mag_vocab.txt --model_prefix=data/model/spm --vocab_size=' + str(VOCAB_SIZE))\n",
    "#spm.SentencePieceTrainer.train('--input=data/vocab/eng_vocab.txt --model_prefix=data/model/eng --vocab_size=' + str(VOCAB_SIZE))\n",
    "#spm.SentencePieceTrainer.train('--input=data/vocab/msa_vocab.txt --model_prefix=data/model/msa --vocab_size=' + str(VOCAB_SIZE))\n",
    "#spm.SentencePieceTrainer.train('--input=data/vocab/lav_vocab.txt --model_prefix=data/model/lav --vocab_size=' + str(VOCAB_SIZE))\n",
    "#spm.SentencePieceTrainer.train('--input=data/vocab/mag_vocab.txt --model_prefix=data/model/mag --vocab_size=' + str(VOCAB_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=data/vocab/eng_vocab.txt,data/vocab/msa_vocab.txt,data/vocab/lav_vocab.txt,data/vocab/mag_vocab.txt --model_prefix=data/model/spm --vocab_size=' + str(VOCAB_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "BOuH1-Tl4-vt",
    "outputId": "8a77b489-02fd-4835-e6e6-ab5b1ec11191"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng.model  lav.model  mag.model  msa.model  spm.model\r\n",
      "eng.vocab  lav.vocab  mag.vocab  msa.vocab  spm.vocab\r\n"
     ]
    }
   ],
   "source": [
    "ls data/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "y05LILRSIKPN",
    "outputId": "f152e018-c63e-4383-b82f-95d58f5c352e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('data/model/spm.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Egx5FaOsIKPW",
    "outputId": "4c001726-3189-49e9-ea35-70917a5a2a68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁This', '▁', 'is', '▁', 'a', '▁', 't', 'est']\n",
      "[1256, 3, 123, 3, 64, 3, 49, 2328]\n"
     ]
    }
   ],
   "source": [
    "print(sp.encode_as_pieces('This is a test'))\n",
    "print(sp.encode_as_ids('This is a test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁هناك', '▁شو', 'ك', 'ة', '▁', 'نا', 'قصة']\n",
      "[818, 931, 11, 12, 3, 23, 1167]\n"
     ]
    }
   ],
   "source": [
    "print(sp.encode_as_pieces('هناك شوكة ناقصة'))\n",
    "print(sp.encode_as_ids('هناك شوكة ناقصة'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁This', '▁', 'is', '▁', 'a', '▁', 't', 'est']\n",
      "[1256, 3, 123, 3, 64, 3, 49, 2328]\n"
     ]
    }
   ],
   "source": [
    "print(sp.encode_as_pieces('This is a test'))\n",
    "print(sp.encode_as_ids('This is a test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vPy6YE06wqFA"
   },
   "source": [
    "## Spacy Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_U6-4yB1wqFG"
   },
   "outputs": [],
   "source": [
    "eng_vocab_data = open(\"data/model/eng.vocab\", \"r\")\n",
    "msa_vocab_data = open(\"data/model/msa.vocab\", \"r\")\n",
    "lav_vocab_data = open(\"data/model/lav.vocab\", \"r\")\n",
    "mag_vocab_data = open(\"data/model/mag.vocab\", \"r\")\n",
    "spm_vocab_data = open(\"data/model/spm.vocab\", \"r\")\n",
    "\n",
    "eng_vocab_list = []\n",
    "msa_vocab_list = []\n",
    "lav_vocab_list = []\n",
    "mag_vocab_list = []\n",
    "spm_vocab_list = []\n",
    "\n",
    "for line in eng_vocab_data.readlines():\n",
    "    eng_vocab_list.append(line.split(\"\\t\")[0])\n",
    "\n",
    "for line in msa_vocab_data.readlines():\n",
    "    msa_vocab_list.append(line.split(\"\\t\")[0])\n",
    "\n",
    "for line in lav_vocab_data.readlines():\n",
    "    lav_vocab_list.append(line.split(\"\\t\")[0])\n",
    "\n",
    "for line in mag_vocab_data.readlines():\n",
    "    mag_vocab_list.append(line.split(\"\\t\")[0])\n",
    "    \n",
    "for line in spm_vocab_data.readlines():\n",
    "    spm_vocab_list.append(line.split(\"\\t\")[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9IkkH0xQwqFK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " '<s>',\n",
       " '</s>',\n",
       " '▁',\n",
       " 'ا',\n",
       " 'و',\n",
       " 'ي',\n",
       " 'ت',\n",
       " 'ال',\n",
       " 'ب',\n",
       " 'م',\n",
       " 'ك',\n",
       " 'ة',\n",
       " 'ه',\n",
       " 'ن',\n",
       " 'ل',\n",
       " 'في',\n",
       " 'ها',\n",
       " 'من',\n",
       " 'ما',\n",
       " 'ش',\n",
       " 'ان',\n",
       " 'ف',\n",
       " 'نا',\n",
       " '؟',\n",
       " 'ين',\n",
       " 'وا',\n",
       " 'ون',\n",
       " 'لي',\n",
       " 'لا',\n",
       " 'س',\n",
       " 'ع',\n",
       " 'على',\n",
       " \"'\",\n",
       " 'ني',\n",
       " 's',\n",
       " 'انا',\n",
       " 'هم',\n",
       " '▁و',\n",
       " 'ى',\n",
       " 'هو',\n",
       " 'ح',\n",
       " 'د',\n",
       " 'انت',\n",
       " 'ق',\n",
       " 'يا',\n",
       " 'ات',\n",
       " '▁لا',\n",
       " 'فيال',\n",
       " 't',\n",
       " 'هذا',\n",
       " '▁انا',\n",
       " 'الم',\n",
       " '?',\n",
       " '▁نعم',\n",
       " 'مع',\n",
       " 'ر',\n",
       " 'ز',\n",
       " 'عن',\n",
       " 'بال',\n",
       " 'سي',\n",
       " '▁I',\n",
       " 'هي',\n",
       " '▁ا',\n",
       " 'a',\n",
       " 'كان',\n",
       " 'ج',\n",
       " 'خ',\n",
       " 'ص',\n",
       " 'انه',\n",
       " 'كل',\n",
       " 'منال',\n",
       " 'تي',\n",
       " 'غ',\n",
       " 'لل',\n",
       " 'لك',\n",
       " 'ط',\n",
       " 'ية',\n",
       " 'حتى',\n",
       " 'to',\n",
       " 'سا',\n",
       " 'بس',\n",
       " 'او',\n",
       " 'ذلك',\n",
       " 'بي',\n",
       " 'ض',\n",
       " 'اني',\n",
       " 'm',\n",
       " 'شي',\n",
       " 'اليوم',\n",
       " 'اذا',\n",
       " 'ار',\n",
       " 'الى',\n",
       " 'واحد',\n",
       " 'ست',\n",
       " 'لو',\n",
       " 'هناك',\n",
       " 'بت',\n",
       " 'اء',\n",
       " 'كنت',\n",
       " 'كم',\n",
       " 'وال',\n",
       " 'غير',\n",
       " 'ور',\n",
       " 'ولا',\n",
       " 'اللي',\n",
       " 'مت',\n",
       " '▁اه',\n",
       " 'لما',\n",
       " 'the',\n",
       " 'you',\n",
       " 'بعد',\n",
       " 'كي',\n",
       " 'الله',\n",
       " 'هنا',\n",
       " 'هل',\n",
       " '▁هل',\n",
       " 'عندما',\n",
       " 'يت',\n",
       " 'اد',\n",
       " 'الان',\n",
       " '▁ايه',\n",
       " 'd',\n",
       " 'is',\n",
       " 'جدا',\n",
       " 'ing',\n",
       " 'كانت',\n",
       " 'مش',\n",
       " 'لكن',\n",
       " 'كيف',\n",
       " 'ام',\n",
       " 'علىال',\n",
       " 'r',\n",
       " 'فى',\n",
       " 'ذ',\n",
       " 'الا',\n",
       " 'تو',\n",
       " 'سليمة',\n",
       " 'انها',\n",
       " 'me',\n",
       " 'كتير',\n",
       " 'الي',\n",
       " 'لات',\n",
       " 'ماذا',\n",
       " 'ed',\n",
       " 'فيها',\n",
       " 'مو',\n",
       " 'تها',\n",
       " 'n',\n",
       " '▁هذا',\n",
       " 'قال',\n",
       " 'اي',\n",
       " 'it',\n",
       " 'ير',\n",
       " 'in',\n",
       " 'ئ',\n",
       " '▁لكن',\n",
       " 'ته',\n",
       " 'شيء',\n",
       " 'e',\n",
       " 'توم',\n",
       " 'وش',\n",
       " 'له',\n",
       " 'فقط',\n",
       " 'فيه',\n",
       " 'ائ',\n",
       " 'ولكن',\n",
       " 'جد',\n",
       " 'يكون',\n",
       " 'اب',\n",
       " 'عمل',\n",
       " 'بر',\n",
       " 'تك',\n",
       " 're',\n",
       " 'قبل',\n",
       " 'لها',\n",
       " 'هيك',\n",
       " 'c',\n",
       " 'تر',\n",
       " '▁ما',\n",
       " 'وقت',\n",
       " '▁اي',\n",
       " 'اس',\n",
       " 'عند',\n",
       " 'دي',\n",
       " 'هذه',\n",
       " 'اخر',\n",
       " 'الذي',\n",
       " 'اش',\n",
       " '▁قالتلي',\n",
       " 'وم',\n",
       " 'انك',\n",
       " 'دو',\n",
       " '▁انت',\n",
       " 'درس',\n",
       " 'ول',\n",
       " 'ايضا',\n",
       " 'f',\n",
       " 'ث',\n",
       " 'مات']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm_vocab_list[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MpMYD4zGwqFP"
   },
   "outputs": [],
   "source": [
    "en_vocab = Vocab(strings=eng_vocab_list)\n",
    "spacy_en_tokenizer = Tokenizer(en_vocab)\n",
    "\n",
    "msa_vocab = Vocab(strings=msa_vocab_list)\n",
    "spacy_msa_tokenizer = Tokenizer(msa_vocab)\n",
    "\n",
    "lav_vocab = Vocab(strings=lav_vocab_list)\n",
    "spacy_lav_tokenizer = Tokenizer(lav_vocab)\n",
    "\n",
    "mag_vocab = Vocab(strings=mag_vocab_list)\n",
    "spacy_mag_tokenizer = Tokenizer(mag_vocab)\n",
    "\n",
    "spm_vocab = Vocab(strings=spm_vocab_list)\n",
    "spacy_spm_tokenizer = Tokenizer(spm_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "james is testing "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_spm_tokenizer(\"james is testing \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callable(spacy_spm_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0uDxsOI4IKPd"
   },
   "source": [
    "## TF Tokenizer\n",
    "\n",
    "https://huggingface.co/transformers/model_doc/t5.html#t5tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zDuf6tVBIKPe"
   },
   "outputs": [],
   "source": [
    "ls data/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LH26oj-KIKPl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer('data/model/spm.model')\n",
    "print(callable(T5Tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "txeYu0e0IKPs"
   },
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode('تعلمت كيفية ركوب الدراجة عندما كان عمري ست سنوات ', return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D7IuJeL0IKPx"
   },
   "outputs": [],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fi6rWTe0wqFc"
   },
   "outputs": [],
   "source": [
    "#decode to make sure you can go back and forth between the encoding properly \n",
    "#@raef can you verify the Arabic?\n",
    "output = tokenizer.decode(input_ids[0])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z2TpLlDln28C"
   },
   "outputs": [],
   "source": [
    "#from transformers import T5WithLMHeadModel\n",
    "#model = T5WithLMHeadModel.from_pretrained('t5-small')\n",
    "input_ids = tokenizer.encode('translate English to Arabic: She hates green peppers', return_tensor='pt')\n",
    "lm_labels = tokenizer.encode('انها لا تحب الفلفل الاخضر', return_tensor='pt')\n",
    "\n",
    "print(input_ids)\n",
    "print(lm_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UuAey7jyIKP1"
   },
   "source": [
    "## Pytorch Data Set and Data Loader\n",
    "\n",
    "https://github.com/google-research/text-to-text-transfer-transformer\n",
    "\n",
    "pytorch dataset: https://pytorch.org/text/_modules/torchtext/datasets/translation.html\n",
    "\n",
    "pytorch dataset documentation: https://torchtext.readthedocs.io/en/latest/datasets.html#iwslt\n",
    "\n",
    "example dataset: https://iwslt2010.fbk.eu/node/32/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "god7BjRnwqFh"
   },
   "source": [
    "Field API: https://pytorch.org/text/data.html#torchtext.data.Field\n",
    "Field Source: https://pytorch.org/text/_modules/torchtext/data/field.html#Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TmvgEGoLwqFi"
   },
   "source": [
    "https://pytorch.org/text/_modules/torchtext/data/utils.html#get_tokenizer\n",
    "    \n",
    "we need to pass our tokenizer as a function     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fVIqommkIKQF"
   },
   "outputs": [],
   "source": [
    "SRC = Field(tokenize = spacy_spm_tokenizer,\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = False)\n",
    "\n",
    "TRG = Field(tokenize = spacy_spm_tokenizer,\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = False)\n",
    "\n",
    "SRC = Field(tokenize =  T5Tokenizer,\n",
    "             init_token = '<sos>',\n",
    "             eos_token = '<eos>',\n",
    "             lower = False)\n",
    "\n",
    "TRG = Field(tokenize =  T5Tokenizer,\n",
    "             init_token = '<sos>',\n",
    "             eos_token = '<eos>',\n",
    "             lower = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used spacy spm tokenizer\n",
      "used spacy spm tokenizer\n"
     ]
    }
   ],
   "source": [
    "SRC = Custom_Field(tokenize = \"spacy\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = False)\n",
    "\n",
    "TRG = Custom_Field(tokenize = \"spacy\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SRC.use_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = SRC.tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n",
      "james is a dog\n"
     ]
    }
   ],
   "source": [
    "output = tok(\"james is a dog\")\n",
    "print(type(output))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UiS3YbFGwqFl"
   },
   "outputs": [],
   "source": [
    "eng_msa_train_dataset = TranslationDataset(path='data/train/train_eng_msa.', exts=('eng', 'msa'), fields=(SRC, TRG))\n",
    "lav_msa_train_dataset = TranslationDataset(path='data/train/train_lav_msa.', exts=('lav', 'msa'), fields=(SRC, TRG))\n",
    "mag_msa_train_dataset = TranslationDataset(path='data/train/train_mag_msa.', exts=('mag', 'msa'), fields=(SRC, TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(eng_msa_train_dataset, lav_msa_train_dataset, mag_msa_train_dataset)\n",
    "TRG.build_vocab(eng_msa_train_dataset, lav_msa_train_dataset, mag_msa_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.vocab.Vocab at 0x1a3eaf7ed0>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SRC.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dEnHMRQxwqFo"
   },
   "outputs": [],
   "source": [
    "\"\"\"SRC.build_vocab(eng_msa_train_dataset)\n",
    "TRG.build_vocab(eng_msa_train_dataset)\n",
    "SRC.build_vocab(lav_msa_train_dataset)\n",
    "TRG.build_vocab(lav_msa_train_dataset)\n",
    "SRC.build_vocab(mag_msa_train_dataset)\n",
    "TRG.build_vocab(mag_msa_train_dataset)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "08ZvS7DYwqFq"
   },
   "outputs": [],
   "source": [
    "ex = eng_msa_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "SAUEnQd6wqFu",
    "outputId": "44c82d4b-e548-477a-9514-e4b42dfbe186"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN_000\\01\\I don't remember what happened anymore\n",
      "TRAIN_000\\01\\لم اعد اتذكر ما حصل\n"
     ]
    }
   ],
   "source": [
    "print(ex.src) \n",
    "print(ex.trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "2nsz1PhXwqF2",
    "outputId": "d2f61c41-2c0a-463c-a6e0-f569aeaf5353"
   },
   "outputs": [],
   "source": [
    "ls data/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pps4KrlSIKQK"
   },
   "outputs": [],
   "source": [
    "#train, validation and test are probably wrong.  \n",
    "#https://github.com/pytorch/text/blob/master/torchtext/datasets/translation.py\n",
    "\n",
    "'''\n",
    "Arguments:\n",
    "            exts: A tuple containing the extension to path for each language.\n",
    "            fields: A tuple containing the fields that will be used for data\n",
    "                in each language.\n",
    "            path (str): Common prefix of the splits' file paths, or None to use\n",
    "                the result of cls.download(root).\n",
    "            root: Root dataset storage directory. Default is '.data'.\n",
    "            train: The prefix of the train data. Default: 'train'.\n",
    "            validation: The prefix of the validation data. Default: 'val'.\n",
    "            test: The prefix of the test data. Default: 'test'.\n",
    "            Remaining keyword arguments: Passed to the splits method of\n",
    "                Dataset.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "train_data, valid_data, test_data = eng_msa_train_dataset.splits(path= 'data/', train='train/train_eng_msa', validation='val/val_eng_msa', test='test/test_eng_msa', exts=('.eng', '.msa'),\n",
    "                                                    fields = (SRC, TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'fromCSV',\n",
       " 'fromJSON',\n",
       " 'fromdict',\n",
       " 'fromlist',\n",
       " 'fromtree',\n",
       " 'src',\n",
       " 'trg']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__del__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__name__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__next__',\n",
       " '__qualname__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'close',\n",
       " 'gi_code',\n",
       " 'gi_frame',\n",
       " 'gi_running',\n",
       " 'gi_yieldfrom',\n",
       " 'send',\n",
       " 'throw']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(train_data.src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Dataset.__getattr__ at 0x1a3eef30d0>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D86hmOgsIKQS"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE =32\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "15gKWRTZIKQW"
   },
   "source": [
    "## Model Training\n",
    "\n",
    "https://huggingface.co/transformers/model_doc/t5.html#training\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "u47qb7PG6WTM",
    "outputId": "6beede7b-5059-4777-9b3e-7a60e7eb6a22",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "         2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 1, 0, 0, 1, 0,\n",
      "         0, 0, 0, 0, 0, 3, 0, 0],\n",
      "        [0, 0, 0, 3, 3, 3, 0, 1, 3, 0, 0, 3, 0, 0, 0, 1, 1, 0, 3, 1, 3, 3, 1, 3,\n",
      "         3, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 3, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 3, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "         1, 3, 0, 0, 0, 1, 3, 0],\n",
      "        [3, 1, 0, 1, 1, 1, 3, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 3, 0, 1, 1, 3],\n",
      "        [1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 1, 0, 1, 1, 1],\n",
      "        [1, 1, 0, 1, 1, 1, 1, 1, 1, 3, 3, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 1, 3, 1, 1, 1],\n",
      "        [1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 0, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 3, 1, 1, 1, 1, 1]])\n",
      "tensor([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "         2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 3, 3, 0, 0, 3, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 3, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 3, 0],\n",
      "        [0, 3, 0, 3, 3, 1, 3, 1, 0, 0, 0, 3, 0, 0, 3, 1, 1, 0, 1, 1, 3, 1, 1, 3,\n",
      "         3, 3, 0, 3, 0, 0, 1, 0],\n",
      "        [3, 1, 0, 1, 1, 1, 1, 1, 3, 3, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 1, 0, 3, 1, 3],\n",
      "        [1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 1, 0, 1, 1, 1],\n",
      "        [1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 3, 1, 3, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 1, 3, 1, 1, 1],\n",
      "        [1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 3, 1, 1, 1, 1, 1]])\n",
      "tensor([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "         2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 3, 0, 0, 0, 3, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 3, 0, 0, 1, 0, 3, 0, 0, 1, 0, 0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 3, 3, 0, 0],\n",
      "        [0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 3, 1, 3, 0, 3, 0, 0, 3,\n",
      "         0, 0, 3, 0, 1, 1, 0, 0],\n",
      "        [0, 1, 1, 0, 0, 1, 3, 1, 3, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 3, 1, 0, 0, 1,\n",
      "         0, 3, 1, 0, 1, 1, 0, 3],\n",
      "        [3, 1, 1, 0, 0, 1, 1, 1, 1, 3, 1, 0, 3, 0, 0, 1, 1, 1, 1, 1, 1, 3, 3, 1,\n",
      "         0, 1, 1, 3, 1, 1, 3, 1],\n",
      "        [1, 1, 1, 3, 0, 1, 1, 1, 1, 1, 1, 0, 1, 3, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         3, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "         2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 3, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 3, 0, 3, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 3, 3, 1, 3, 0, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 3,\n",
      "         0, 0, 1, 0, 1, 0, 0, 0],\n",
      "        [3, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 3, 3, 3, 0, 0, 0, 0, 1,\n",
      "         0, 3, 1, 0, 1, 3, 3, 0],\n",
      "        [1, 1, 3, 0, 0, 1, 1, 1, 1, 0, 1, 0, 3, 0, 0, 1, 1, 1, 1, 0, 3, 0, 3, 1,\n",
      "         0, 1, 1, 0, 1, 1, 1, 3],\n",
      "        [1, 1, 1, 3, 3, 1, 1, 1, 1, 0, 1, 0, 1, 3, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
      "         0, 1, 1, 3, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 3, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
      "         3, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 3, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "         2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 3],\n",
      "        [3, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 3, 0, 1],\n",
      "        [1, 0, 3, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "         0, 3, 0, 3, 0, 1, 0, 1],\n",
      "        [1, 3, 1, 0, 0, 1, 0, 0, 1, 3, 0, 0, 0, 0, 3, 1, 0, 0, 1, 3, 3, 0, 0, 3,\n",
      "         3, 1, 3, 1, 0, 1, 0, 1],\n",
      "        [1, 1, 1, 0, 3, 1, 0, 0, 1, 1, 3, 0, 3, 0, 1, 1, 3, 0, 1, 1, 1, 3, 3, 1,\n",
      "         1, 1, 1, 1, 3, 1, 0, 1],\n",
      "        [1, 1, 1, 3, 1, 1, 3, 3, 1, 1, 1, 3, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 3, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "         2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3,\n",
      "         0, 0, 0, 0, 0, 3, 0, 3],\n",
      "        [3, 3, 1, 0, 0, 0, 3, 0, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1,\n",
      "         3, 3, 0, 0, 0, 1, 0, 1],\n",
      "        [1, 1, 1, 3, 0, 3, 1, 0, 1, 1, 1, 0, 0, 0, 3, 3, 0, 3, 3, 0, 1, 0, 0, 1,\n",
      "         1, 1, 0, 3, 0, 1, 3, 1],\n",
      "        [1, 1, 1, 1, 3, 1, 1, 3, 1, 1, 1, 3, 0, 0, 1, 1, 3, 1, 1, 3, 1, 3, 3, 1,\n",
      "         1, 1, 3, 1, 0, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 3, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "         2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 3, 0, 0, 0],\n",
      "        [3, 0, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         3, 0, 0, 0, 1, 0, 0, 0],\n",
      "        [1, 3, 1, 0, 3, 0, 0, 1, 3, 3, 0, 3, 0, 3, 0, 3, 3, 3, 0, 3, 0, 0, 3, 0,\n",
      "         1, 3, 0, 3, 1, 0, 3, 0],\n",
      "        [1, 1, 1, 3, 1, 0, 0, 1, 1, 1, 0, 1, 3, 1, 0, 1, 1, 1, 0, 1, 3, 3, 1, 0,\n",
      "         1, 1, 3, 1, 1, 0, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 3, 1, 1, 1, 3, 1, 1, 1, 1, 0,\n",
      "         1, 1, 1, 1, 1, 3, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 3, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 3]])\n",
      "tensor([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "         2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 3, 0, 3, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 3, 0, 0, 0],\n",
      "        [3, 3, 1, 3, 1, 0, 0, 1, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0,\n",
      "         0, 0, 3, 3, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 1, 3, 3, 3, 1, 0, 1, 0, 3, 0, 3, 0, 3, 1, 0, 1, 0,\n",
      "         3, 0, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 3, 1, 0, 1, 0, 1, 0, 1, 1, 3, 1, 0,\n",
      "         1, 3, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 0, 1, 3, 1, 1, 1, 1, 0,\n",
      "         1, 1, 1, 1, 1, 3, 3, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 3]])\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for x in train_iterator:\n",
    "    print(x.src)\n",
    "    print(x.trg)\n",
    "    #print(spacy_en_tokenizer.decode(x.src))\n",
    "    count += 1\n",
    "    \n",
    "    if count > 3:\n",
    "        break\n",
    "    \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kmd4r1ce4-wq"
   },
   "outputs": [],
   "source": [
    "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qBOsxexa4-wu"
   },
   "outputs": [],
   "source": [
    "def train(model: nn.Module,\n",
    "          iterator: BucketIterator,\n",
    "          optimizer: optim.Optimizer,\n",
    "          criterion: nn.Module,\n",
    "          clip: float):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for _, batch in enumerate(iterator):\n",
    "\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2OvKSZ214-wy"
   },
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module,\n",
    "             iterator: BucketIterator,\n",
    "             criterion: nn.Module):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for _, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing?\n",
    "\n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6oKXrTpU4-w1"
   },
   "outputs": [],
   "source": [
    "#declare model\n",
    "config = T5Config(vocab_size=VOCAB_SIZE)\n",
    "model = T5Model(config)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Config {\n",
       "  \"_num_labels\": 2,\n",
       "  \"architectures\": null,\n",
       "  \"bad_words_ids\": null,\n",
       "  \"bos_token_id\": null,\n",
       "  \"d_ff\": 2048,\n",
       "  \"d_kv\": 64,\n",
       "  \"d_model\": 512,\n",
       "  \"decoder_start_token_id\": null,\n",
       "  \"do_sample\": false,\n",
       "  \"dropout_rate\": 0.1,\n",
       "  \"early_stopping\": false,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"finetuning_task\": null,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\"\n",
       "  },\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"is_decoder\": false,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1\n",
       "  },\n",
       "  \"layer_norm_epsilon\": 1e-06,\n",
       "  \"length_penalty\": 1.0,\n",
       "  \"max_length\": 20,\n",
       "  \"min_length\": 0,\n",
       "  \"model_type\": \"t5\",\n",
       "  \"n_positions\": 512,\n",
       "  \"no_repeat_ngram_size\": 0,\n",
       "  \"num_beams\": 1,\n",
       "  \"num_heads\": 8,\n",
       "  \"num_layers\": 6,\n",
       "  \"num_return_sequences\": 1,\n",
       "  \"output_attentions\": false,\n",
       "  \"output_hidden_states\": false,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"prefix\": null,\n",
       "  \"pruned_heads\": {},\n",
       "  \"relative_attention_num_buckets\": 32,\n",
       "  \"repetition_penalty\": 1.0,\n",
       "  \"task_specific_params\": null,\n",
       "  \"temperature\": 1.0,\n",
       "  \"top_k\": 50,\n",
       "  \"top_p\": 1.0,\n",
       "  \"torchscript\": false,\n",
       "  \"use_bfloat16\": false,\n",
       "  \"vocab_size\": 5000\n",
       "}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "9HwOuA3y4-w5",
    "outputId": "20590d68-bfdd-4f9c-b2b6-15abd5da19de"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (18) must match the size of tensor b (17) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-814d03c8fef9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-48bbe32f17dc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/fastai/lib/python3.7/site-packages/transformers/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, inputs_embeds, decoder_inputs_embeds, head_mask)\u001b[0m\n\u001b[1;32m    833\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoder_outputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             encoder_outputs = self.encoder(\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m             )\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/fastai/lib/python3.7/site-packages/transformers/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask)\u001b[0m\n\u001b[1;32m    666\u001b[0m                 \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_extended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m                 \u001b[0mencoder_decoder_position_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_decoder_position_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m                 \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m             )\n\u001b[1;32m    670\u001b[0m             \u001b[0;31m# layer_outputs is a tuple with:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/fastai/lib/python3.7/site-packages/transformers/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, head_mask)\u001b[0m\n\u001b[1;32m    424\u001b[0m     ):\n\u001b[1;32m    425\u001b[0m         self_attention_outputs = self.layer[0](\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_bias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         )\n\u001b[1;32m    428\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/fastai/lib/python3.7/site-packages/transformers/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, head_mask)\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0mnorm_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         attention_output = self.SelfAttention(\n\u001b[0;32m--> 377\u001b[0;31m             \u001b[0mnorm_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_bias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m         )\n\u001b[1;32m    379\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/fastai/lib/python3.7/site-packages/transformers/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, mask, kv, position_bias, cache, head_mask)\u001b[0m\n\u001b[1;32m    344\u001b[0m                 \u001b[0mposition_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposition_bias\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmask\u001b[0m  \u001b[0;31m# (bs, n_heads, qlen, klen)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mposition_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, qlen, klen)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, qlen, klen)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (18) must match the size of tensor b (17) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B0iYXG2c4-w9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d27X3PDM4-w_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RsKKhepA4-xC"
   },
   "outputs": [],
   "source": [
    "def trainIters(model, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    optimizer = AdamW.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX) #CHECK IF THE BEST LOSS FUNCTION\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        \n",
    "\n",
    "        loss = train(decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6s5rRYCg4-xI"
   },
   "outputs": [],
   "source": [
    "model = \n",
    "n_iteres = 10\n",
    "trainIters(model, n_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1A10gV1R4-xM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y6hpf6xY4-xR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mPFePDoUIKQ1"
   },
   "source": [
    "## Model Inference\n",
    "\n",
    "https://github.com/huggingface/transformers/blob/master/examples/translation/t5/evaluate_wmt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GXw9evdHyYhz"
   },
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i : i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4xt_iDQ4IKQa"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def generate_translations(lns, output_file_path, model_size, batch_size, device):\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_size)\n",
    "    model.to(device)\n",
    "    #returned to using T5 tokenizer\n",
    "    tokenizer = tokenizer\n",
    "\n",
    "    task_specific_params = model.config.task_specific_params\n",
    "\n",
    "    with Path(output_file_path).open(\"w\") as output_file:\n",
    "        for batch in tqdm(list(chunks(lns, batch_size))):\n",
    "        #modify to have prefix 'translate English to Arabic: ...'\n",
    "            batch = [ text for text in batch]\n",
    "            \n",
    "            dct = tokenizer.batch_encode_plus(batch, max_length=512, return_tensors=\"pt\", pad_to_max_length=True)\n",
    "\n",
    "            input_ids = dct[\"input_ids\"].to(device)\n",
    "            attention_mask = dct[\"attention_mask\"].to(device)\n",
    "\n",
    "            translations = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            dec = [\n",
    "                tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in translations\n",
    "            ]\n",
    "\n",
    "            for hypothesis in dec:\n",
    "                output_file.write(hypothesis + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8T4t7_HXy44y"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def calculate_bleu_score(output_lns, refs_lns, score_path):\n",
    "    bleu = corpus_bleu(output_lns, [refs_lns])\n",
    "    result = \"BLEU score: {}\".format(bleu.score)\n",
    "    with Path(score_path).open(\"w\") as score_file:\n",
    "        score_file.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hrpvKdBwNUYG"
   },
   "outputs": [],
   "source": [
    "!mkdir data/output/\n",
    "!touch data/output/translated.msa\n",
    "!touch data/output/score_eng_msa.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GslDGSi2zADM"
   },
   "outputs": [],
   "source": [
    "#got rid of the argparse and just put it in as params\n",
    "def run_generate(model_size='t5-base', input_path='data/train/train_eng_msa.eng', output_path='data/output/translated.msa', ref_path='data/train/train_eng_msa.eng', score_path='data/output/score_eng_msa.txt', batch_size=BATCH_SIZE):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "\n",
    "    # Read input lines into python\n",
    "    with open(input_path, \"r\") as input_file:\n",
    "        input_lns = [x.strip() for x in input_file.readlines()]\n",
    "\n",
    "    generate_translations(input_lns, output_path, model_size, batch_size, device)\n",
    "\n",
    "    # Read generated lines into python\n",
    "    with open(output_path, \"r\") as output_file:\n",
    "        output_lns = [x.strip() for x in output_file.readlines()]\n",
    "\n",
    "    # Read reference lines into python\n",
    "    with open(ref_path, \"r\") as reference_file:\n",
    "        refs_lns = [x.strip() for x in reference_file.readlines()]\n",
    "\n",
    "    calculate_bleu_score(output_lns, refs_lns, args.score_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_sKwIFVT0itp"
   },
   "outputs": [],
   "source": [
    "run_generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SpfOCjlt4-x7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "name": "Model_1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:fastai] *",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
