{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_1.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BQflUKhjIKNp"
      },
      "source": [
        "### Notes \n",
        "\n",
        "T5 Paper: https://arxiv.org/pdf/1910.10683.pdf\n",
        "\n",
        "T5 Tokenizer: https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_t5.py\n",
        "\n",
        "Important Tasks: https://docs.google.com/document/d/1weIZM6QTlnitpPQmpg-WeV2RW70TnYmDuogBQPr5mB0/edit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLOmiOta6MJp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "878ec2d0-d60a-4788-8e79-3247e5ec8896"
      },
      "source": [
        "#installation step\n",
        "!pip install transformers\n",
        "!pip install t5\n",
        "!pip install sentencepiece\n",
        "#creating the folders \n",
        "!mkdir data/\n",
        "!mkdir data/AD_NMT-master\n",
        "!mkdir data/train/\n",
        "!mkdir data/test/\n",
        "!mkdir data/val/\n",
        "!mkdir data/model/\n",
        "!mkdir data/config/\n",
        "#fetching the pkl files\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW' -O data/AD_NMT-master/english-Arabic-both.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV' -O data/AD_NMT-master/LAV-MSA-2-both.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UjDX7cCG2S23SPfSHxSPdVayMTxB5Y16' -O data/AD_NMT-master/Magribi_MSA-both.pkl\n",
        "# !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1fEVj9jCxvcKn9zg8lO43i2sWZquegg5H' -O data/operative_config.gin\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UGKswXSqHSxWpx57cEDzvNeJaqbAuyt8' -O data/padic.xml"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: t5 in /usr/local/lib/python3.6/dist-packages (0.6.4)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.6/dist-packages (from t5) (2.3.0)\n",
            "Requirement already satisfied: tfds-nightly in /usr/local/lib/python3.6/dist-packages (from t5) (3.2.1.dev202008280105)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from t5) (1.18.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from t5) (1.0.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from t5) (1.4.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from t5) (0.8.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from t5) (0.22.2.post1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from t5) (0.1.91)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from t5) (1.6.0+cu101)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.6/dist-packages (from t5) (1.4.13)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.6/dist-packages (from t5) (2.8.0)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.6/dist-packages (from t5) (0.0.4)\n",
            "Requirement already satisfied: six>=1.14 in /usr/local/lib/python3.6/dist-packages (from t5) (1.15.0)\n",
            "Requirement already satisfied: transformers>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from t5) (3.0.2)\n",
            "Requirement already satisfied: mesh-tensorflow[transformer]>=0.1.13 in /usr/local/lib/python3.6/dist-packages (from t5) (0.1.16)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from t5) (3.2.5)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.6/dist-packages (from t5) (0.3.0)\n",
            "Requirement already satisfied: tensorflow<2.4,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-text->t5) (2.3.0)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (3.0.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (2.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (0.3.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (1.1.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (2.23.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (3.12.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (0.16.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (0.1.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (4.41.1)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (0.23.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (1.12.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (0.7)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5) (20.1.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->t5) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->t5) (2.8.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->t5) (0.16.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from sacrebleu->t5) (2.0.0)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5) (0.8.1rc1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5) (0.0.43)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5) (20.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5) (2019.12.20)\n",
            "Requirement already satisfied: tensorflow-datasets; extra == \"transformer\" in /usr/local/lib/python3.6/dist-packages (from mesh-tensorflow[transformer]>=0.1.13->t5) (2.1.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (2.10.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (3.3.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (1.31.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (0.35.1)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (2.3.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (0.3.3)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (0.2.0)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->tfds-nightly->t5) (3.1.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly->t5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly->t5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly->t5) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly->t5) (1.24.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tfds-nightly->t5) (49.6.0)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tfds-nightly->t5) (1.52.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.7.0->t5) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=2.7.0->t5) (2.4.7)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (0.4.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (1.17.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (3.2.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (0.2.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (1.7.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->t5) (0.4.8)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.91)\n",
            "mkdir: cannot create directory ‘data/’: File exists\n",
            "mkdir: cannot create directory ‘data/AD_NMT-master’: File exists\n",
            "mkdir: cannot create directory ‘data/train/’: File exists\n",
            "mkdir: cannot create directory ‘data/test/’: File exists\n",
            "mkdir: cannot create directory ‘data/val/’: File exists\n",
            "mkdir: cannot create directory ‘data/model/’: File exists\n",
            "mkdir: cannot create directory ‘data/config/’: File exists\n",
            "--2020-08-29 02:21:28--  https://docs.google.com/uc?export=download&id=1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.111.101, 108.177.111.138, 108.177.111.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.111.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-10-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/4ci9a7dmo6m4t96egeu5uei6o7qm0ken/1598667675000/16970776037313924126/*/1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-08-29 02:21:28--  https://doc-10-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/4ci9a7dmo6m4t96egeu5uei6o7qm0ken/1598667675000/16970776037313924126/*/1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW?e=download\n",
            "Resolving doc-10-2s-docs.googleusercontent.com (doc-10-2s-docs.googleusercontent.com)... 172.217.214.132, 2607:f8b0:4001:c05::84\n",
            "Connecting to doc-10-2s-docs.googleusercontent.com (doc-10-2s-docs.googleusercontent.com)|172.217.214.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 955428 (933K) [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/english-Arabic-both.pkl’\n",
            "\n",
            "data/AD_NMT-master/ 100%[===================>] 933.04K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2020-08-29 02:21:29 (109 MB/s) - ‘data/AD_NMT-master/english-Arabic-both.pkl’ saved [955428/955428]\n",
            "\n",
            "--2020-08-29 02:21:29--  https://docs.google.com/uc?export=download&id=1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.202.139, 74.125.202.138, 74.125.202.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.202.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-00-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/0mgclsn616qe09pjnujip9nh8bqpov3b/1598667675000/16970776037313924126/*/1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-08-29 02:21:30--  https://doc-00-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/0mgclsn616qe09pjnujip9nh8bqpov3b/1598667675000/16970776037313924126/*/1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV?e=download\n",
            "Resolving doc-00-2s-docs.googleusercontent.com (doc-00-2s-docs.googleusercontent.com)... 172.217.214.132, 2607:f8b0:4001:c05::84\n",
            "Connecting to doc-00-2s-docs.googleusercontent.com (doc-00-2s-docs.googleusercontent.com)|172.217.214.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/LAV-MSA-2-both.pkl’\n",
            "\n",
            "data/AD_NMT-master/     [ <=>                ]   2.33M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-08-29 02:21:30 (177 MB/s) - ‘data/AD_NMT-master/LAV-MSA-2-both.pkl’ saved [2447014]\n",
            "\n",
            "--2020-08-29 02:21:30--  https://docs.google.com/uc?export=download&id=1UjDX7cCG2S23SPfSHxSPdVayMTxB5Y16\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.111.113, 108.177.111.139, 108.177.111.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.111.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-04-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/gvkb5tqeo5rhti6k18sbtegnoionmh7e/1598667675000/16970776037313924126/*/1UjDX7cCG2S23SPfSHxSPdVayMTxB5Y16?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-08-29 02:21:31--  https://doc-04-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/gvkb5tqeo5rhti6k18sbtegnoionmh7e/1598667675000/16970776037313924126/*/1UjDX7cCG2S23SPfSHxSPdVayMTxB5Y16?e=download\n",
            "Resolving doc-04-2s-docs.googleusercontent.com (doc-04-2s-docs.googleusercontent.com)... 172.217.214.132, 2607:f8b0:4001:c05::84\n",
            "Connecting to doc-04-2s-docs.googleusercontent.com (doc-04-2s-docs.googleusercontent.com)|172.217.214.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/Magribi_MSA-both.pkl’\n",
            "\n",
            "data/AD_NMT-master/     [ <=>                ]   2.81M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-08-29 02:21:31 (215 MB/s) - ‘data/AD_NMT-master/Magribi_MSA-both.pkl’ saved [2944107]\n",
            "\n",
            "--2020-08-29 02:21:31--  https://docs.google.com/uc?export=download&id=1UGKswXSqHSxWpx57cEDzvNeJaqbAuyt8\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.111.100, 108.177.111.102, 108.177.111.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.111.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-04-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/3nna5dnjtjnvdod1dmtigupka67ngs05/1598667675000/16970776037313924126/*/1UGKswXSqHSxWpx57cEDzvNeJaqbAuyt8?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-08-29 02:21:32--  https://doc-04-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/3nna5dnjtjnvdod1dmtigupka67ngs05/1598667675000/16970776037313924126/*/1UGKswXSqHSxWpx57cEDzvNeJaqbAuyt8?e=download\n",
            "Resolving doc-04-2s-docs.googleusercontent.com (doc-04-2s-docs.googleusercontent.com)... 172.217.214.132, 2607:f8b0:4001:c05::84\n",
            "Connecting to doc-04-2s-docs.googleusercontent.com (doc-04-2s-docs.googleusercontent.com)|172.217.214.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/xml]\n",
            "Saving to: ‘data/padic.xml’\n",
            "\n",
            "data/padic.xml          [ <=>                ]   2.97M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2020-08-29 02:21:32 (169 MB/s) - ‘data/padic.xml’ saved [3114143]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-WzhRv4mIKNq",
        "colab": {}
      },
      "source": [
        "#James Chartouni\n",
        "#Joey Park\n",
        "#Raef Khan\n",
        "\n",
        "import torch\n",
        "from torch.optim import SGD\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os, io, glob\n",
        "import functools\n",
        "\n",
        "import sentencepiece as spm\n",
        "\n",
        "import transformers\n",
        "import t5\n",
        "from t5.data import preprocessors\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "irQQ9E_7IKOM"
      },
      "source": [
        "## Prepare Datasets\n",
        "\n",
        "We need to take our training and test sets from the pkl files and create new .txt files that are formatted so that the standard torchtext Dataset class can read them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fctRFLVvs3X2",
        "colab_type": "text"
      },
      "source": [
        "### PADIC Dataset Parsing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4_RfmVTUuze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "padic_tree = ET.parse('data/padic.xml')\n",
        "\n",
        "padic_alg_msa = []\n",
        "padic_ann_msa = []\n",
        "padic_syr_msa = []\n",
        "padic_pal_msa = []\n",
        "padic_mor_msa = [] \n",
        "\n",
        "for sentence in padic_tree.getroot():\n",
        "  padic_alg_msa.append([sentence.find('ALGIERS').text.strip(), sentence.find('MODERN-STANDARD-ARABIC').text.strip()])\n",
        "  padic_ann_msa.append([sentence.find('ANNABA').text.strip(), sentence.find('MODERN-STANDARD-ARABIC').text.strip()])\n",
        "  padic_syr_msa.append([sentence.find('SYRIAN').text.strip(), sentence.find('MODERN-STANDARD-ARABIC').text.strip()])\n",
        "  padic_pal_msa.append([sentence.find('PALESTINIAN').text.strip(), sentence.find('MODERN-STANDARD-ARABIC').text.strip()])\n",
        "  padic_mor_msa.append([sentence.find('MOROCCAN').text.strip(), sentence.find('MODERN-STANDARD-ARABIC').text.strip()])"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iL68WL8LEtTF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "46552e1f-a096-42f5-fe74-bce4eb74af43"
      },
      "source": [
        "print(padic_alg_msa[0])\n",
        "print(padic_ann_msa[0])\n",
        "print(padic_syr_msa[0])\n",
        "print(padic_pal_msa[0])\n",
        "print(padic_mor_msa[0])"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['EAdw AlnAs ytbAkAw bdyt nhdr mn qlby tqwl nhdy fAlnAs', \"tEAlt >SwAt AlnAs bAlbkA'،  bd>t >tHdv bAnfEAl w k>nny >hdy fy AlnAs\"]\n",
            "['EAdwA AlnAs ytbAkAw bdyt nhdr bg$ w qwl ElyA nhdy fy AlnAs', \"tEAlt >SwAt AlnAs bAlbkA'،  bd>t >tHdv bAnfEAl w k>nny >hdy fy AlnAs\"]\n",
            "['Ely Swt AlnAs bAlbky w bl~$t >Hky bESbyp w k>ny Em Ahdy bAlnAs', \"tEAlt >SwAt AlnAs bAlbkA'،  bd>t >tHdv bAnfEAl w k>nny >hdy fy AlnAs\"]\n",
            "['SArwA AlnAs ySyHwA bSwt EAly wbdyt AHky wAnA mnfEl wk>ny bhdy fy AlnAs', \"tEAlt >SwAt AlnAs bAlbkA'،  bd>t >tHdv bAnfEAl w k>nny >hdy fy AlnAs\"]\n",
            "['nAs bdAw tytbAkAw wbdyt tnhdr b nfEl bHAl <lY tnhdy AlnAs', \"tEAlt >SwAt AlnAs bAlbkA'،  bd>t >tHdv bAnfEAl w k>nny >hdy fy AlnAs\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSfrbD9pGrZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alg_msa_train, alg_msa_val = train_test_split(padic_alg_msa, test_size=.15)\n",
        "ann_msa_train, ann_msa_val = train_test_split(padic_ann_msa, test_size=.15)\n",
        "syr_msa_train, syr_msa_val = train_test_split(padic_syr_msa, test_size=.15)\n",
        "pal_msa_train, pal_msa_val = train_test_split(padic_pal_msa, test_size=.15)\n",
        "mor_msa_train, mor_msa_val = train_test_split(padic_mor_msa, test_size=.15)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5uDm27ZHOEi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "02ce28e1-35b0-4486-c196-db8ad1fa5071"
      },
      "source": [
        "#all the translations have equal amt. of examples\n",
        "print(len(alg_msa_train))\n",
        "print(len(alg_msa_val))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6131\n",
            "1082\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxCLgMZ2ns4b",
        "colab_type": "text"
      },
      "source": [
        "###Initial Loading from Pickle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aha0xureIKNw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cacbfdc7-03b1-43c1-d09d-ed69d7bf8b45"
      },
      "source": [
        "ls data/AD_NMT-master"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "english-Arabic-both.pkl  LAV-MSA-2-both.pkl  Magribi_MSA-both.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XWKnGRlLIKN9",
        "colab": {}
      },
      "source": [
        "file_path = 'data/AD_NMT-master/'\n",
        "\n",
        "with open(file_path + \"english-Arabic-both.pkl\", 'rb') as handle:\n",
        "    data_MSA_English_both = pickle.load(handle) \n",
        "\n",
        "with open(file_path + \"LAV-MSA-2-both.pkl\", 'rb') as handle:\n",
        "    data_LAV_MSA_both = pickle.load(handle) \n",
        "\n",
        "with open(file_path + \"Magribi_MSA-both.pkl\", 'rb') as handle:\n",
        "    data_Magribi_MSA_both = pickle.load(handle) \n",
        "    "
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ch3APJadIKOH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "5e46a84c-3e15-472c-a103-8dd63c74fa1e"
      },
      "source": [
        "#few dataset examples\n",
        "print(data_MSA_English_both[0:5])\n",
        "print(data_MSA_English_both[-5:])\n",
        "print(data_LAV_MSA_both[0:5])\n",
        "print(data_Magribi_MSA_both[0:5])"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Tom was also there', 'كان توم هنا ايضا'], ['That old woman lives by herself', 'تلك المراة العجوز تسكن بمفردها'], ['He went abroad for the purpose of studying English', 'سافر خارج البلد ليتعلم الانجليزية'], ['There is a fork missing', 'هناك شوكة ناقصة'], [\"I don't know this game\", 'لا اعرف هذه اللعبة']]\n",
            "[['Please send us more information', 'ارسل الينا المزيد من المعلومات اذا تكرمت'], ['I am an only child', 'انا طفل وحيد ابي و امي'], ['Make good use of your time', 'استفد من وقتك جيدا'], [\"Fighting won't settle anything\", 'لن يحل القتال اي شيء'], ['Practice makes perfect', 'الممارسة هي الطريق الى الاتقان']]\n",
            "[['لا انا بعرف وحدة راحت ع فرنسا و معا شنتا حطت فيها الفرش', 'لا اعرف واحدة ذهبت الى فرنسا و لها غرفة و ضعت فيها الافرشة'], ['روح بوشك و فتول عاليسار', 'اذهب تقدم و استدر يسارا'], ['لا لا لازم انه يكون عندك موضوع ما في اشي', ' لا لا يجب ان يكون لديك موضوع هذا ضروري'], ['اوعي تبعدي من هون بلاش تضيعي ', 'لا تبتعد عن هنا حتى لا تفقد الطريق '], ['قصدي صراحة يما انا كمان كرهته من يوم ما عملتيه زي ما بتعمله خالتي كرهته و صرت ما باطيقه بالمرة', 'اقصد صراحة يا امي انا ايضا كرهته من يوم حضرته مثلما تحضره خالتي كرهته و اصبحت لا اطيقه ابدا']]\n",
            "[['يا ربي متخليش حتى لبيوتا ديالهم يوصلو ل البارة', 'يارب لا تدع اهدافهم تصيب حتى العارضة'], ['يعطيك الصحة كريمة', 'يعطيك العافية كريمة'], [' لوكان جوزوزه ساعة و نص و يهنيونا ', 'لو انهم يبثونه ساعة و نصف و يريحوننا'], ['ولا عيط لك واحد و قالك راه فلان رايح يضربك غدوة بكف ', 'او انه قال لك بان فلان سيصفعك غدا'], ['عبرتي انا عقدتها مرة وحدة', 'احسنت عقدتها مرة واحدة']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lp7ncVY6wqET",
        "colab": {}
      },
      "source": [
        "#splits the train dataset into train and validation sets, define test set as datafile\n",
        "msa_en_train, msa_en_val = train_test_split(data_MSA_English_both, test_size=.2)\n",
        "\n",
        "lav_msa_train, lav_msa_val = train_test_split(data_LAV_MSA_both, test_size=.2)\n",
        "\n",
        "mag_msa_train, mag_msa_val = train_test_split(data_Magribi_MSA_both, test_size=.2)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9Ue2fSILwqEW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "7f823787-c98c-46fb-d0a0-822217ea239e"
      },
      "source": [
        "print(len(msa_en_train))\n",
        "print(len(msa_en_val))\n",
        "\n",
        "print(len(lav_msa_train))\n",
        "print(len(lav_msa_val))\n",
        "\n",
        "print(len(mag_msa_train))\n",
        "print(len(mag_msa_val))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8000\n",
            "2001\n",
            "12644\n",
            "3161\n",
            "15788\n",
            "3948\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W3bWU46TIKOb",
        "colab": {}
      },
      "source": [
        "file_path = 'data/'\n",
        "\n",
        "def list_to_csv(ds, src='msa', trg='en', datatype=''):\n",
        "    src_formatted = datatype + '_' + src + '_' + trg + '.' + 'csv'\n",
        "    \n",
        "    with open(file_path + datatype + \"/\" + src_formatted, 'wt') as csv:\n",
        "        for i, arr in enumerate(ds):\n",
        "            csv.write(arr[1] + ',' + arr[0] + '\\n')"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AqwDB9BuKEnV",
        "colab": {}
      },
      "source": [
        "list_to_csv(msa_en_train, 'msa', 'en', 'train')\n",
        "list_to_csv(msa_en_val, 'msa', 'en', 'val')\n",
        "\n",
        "list_to_csv(lav_msa_train, 'lav', 'msa', 'train')\n",
        "list_to_csv(lav_msa_val, 'lav', 'msa', 'val')\n",
        "\n",
        "list_to_csv(mag_msa_train, 'mag', 'msa', 'train')\n",
        "list_to_csv(mag_msa_val, 'mag', 'msa', 'val')"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGqh46GhVHfI",
        "colab_type": "text"
      },
      "source": [
        "## Training SentencePiece Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lcM0dLclt6J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#combine all the training lines of all three languages\n",
        "spm_input_ds = msa_en_train + mag_msa_train + lav_msa_train"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBZ5iR21etNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def list_to_input(ds):\n",
        "    src_formatted = 'spm_input' + '.' + 'txt'\n",
        "\n",
        "    with open(file_path + \"/\" + src_formatted, 'wt') as sentencelinefile:\n",
        "        for i, arr in enumerate(ds):\n",
        "            sentencelinefile.write(arr[0] + '\\n' + arr[1] + '\\n')"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxDKpM1H7vaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_to_input(spm_input_ds)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3YrvcR8anQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = 32128\n",
        "spm.SentencePieceTrainer.train('--input=data/spm_input.txt --model_prefix=data/model/spm --vocab_size=' + str(VOCAB_SIZE) + ' --unk_id=2 --bos_id=-1 --eos_id=1 --pad_id=0 --hard_vocab_limit=False')"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzB-_AeUUNLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath = 'data/model/spm.model'"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrHAuzNW1eIh",
        "colab_type": "text"
      },
      "source": [
        "##Tensor Processing + Add to TaskRegistry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oVVzdeAZd0x",
        "colab_type": "text"
      },
      "source": [
        "### English to Arabic Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWnZcYI_491p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "msa_en_split_csv_path = {\n",
        "    \"train\": \"data/train/train_msa_en.csv\",\n",
        "    \"validation\": \"data/val/val_msa_en.csv\"\n",
        "}\n",
        "msa_en_example_count = {\n",
        "    \"train\": len(msa_en_train),\n",
        "    \"validation\": len(msa_en_val)\n",
        "}"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2e3cM_wP_M8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "8979e519-169f-4dac-dd43-a9c56c34998e"
      },
      "source": [
        "def msa_en_translation_dataset_fn(split, shuffle_files=False):\n",
        "  ds = tf.data.TextLineDataset(msa_en_split_csv_path[split])\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"\",\"\"],\n",
        "                        field_delim=\",\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "  )\n",
        "  ds = ds.map(lambda *example: dict(zip([\"source\", \"target\"], example)) )\n",
        "  return ds\n",
        "\n",
        "for example in tfds.as_numpy(msa_en_translation_dataset_fn(\"train\").take(5)):\n",
        "    print(example)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'source': b'\\xd9\\x87\\xd8\\xb0\\xd8\\xa7 \\xd8\\xa7\\xd9\\x85\\xd8\\xb1 \\xd9\\x8a\\xd8\\xa7 \\xd8\\xaa\\xd9\\x88\\xd9\\x85', 'target': b\"That's an order Tom\"}\n",
            "{'source': b'\\xd8\\xa7\\xd9\\x86\\xd8\\xa7 \\xd8\\xac\\xd8\\xa7\\xd8\\xa6\\xd8\\xb9 \\xd9\\x84\\xd9\\x84\\xd8\\xba\\xd8\\xa7\\xd9\\x8a\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd9\\x86', 'target': b\"I'm very hungry now\"}\n",
            "{'source': b'\\xd8\\xa7\\xd8\\xb3\\xd8\\xaa\\xd9\\x85\\xd8\\xb1 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xb7\\xd8\\xb1 \\xd8\\xae\\xd9\\x85\\xd8\\xb3\\xd8\\xa9 \\xd8\\xa7\\xd9\\x8a\\xd8\\xa7\\xd9\\x85', 'target': b'The rain lasted five days'}\n",
            "{'source': b'\\xd9\\x84\\xd9\\x82\\xd8\\xaf \\xd8\\xb3\\xd9\\x85\\xd8\\xb9\\xd8\\xaa \\xd9\\x87\\xd8\\xb0\\xd9\\x87 \\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd8\\xba\\xd9\\x86\\xd9\\x8a\\xd8\\xa9 \\xd9\\x85\\xd9\\x86 \\xd9\\x82\\xd8\\xa8\\xd9\\x84', 'target': b\"I've heard this song before\"}\n",
            "{'source': b'\\xd8\\xaf\\xd8\\xb1\\xd8\\xa7\\xd8\\xac\\xd8\\xaa\\xd9\\x87 \\xd8\\xb2\\xd8\\xb1\\xd9\\x82\\xd8\\xa7\\xd8\\xa1', 'target': b'His bicycle is blue'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1GXED2IO3pE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#turn the ds of dictionaries and change the keys to inputs and targets that the model\n",
        "def msa_en_translation_preprocessor(ds):\n",
        "  def to_inputs_and_targets(ex):\n",
        "    return{\n",
        "        \"inputs\": tf.strings.join([\"translate MSA to English: \",ex[\"source\"]]),\n",
        "        \"targets\": ex[\"target\"]\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-W6j-SDXcNT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t5.data.TaskRegistry.remove(\"translation_msa_en\")\n",
        "t5.data.TaskRegistry.add(\n",
        "    #name of the Task\n",
        "    \"translation_msa_en\",\n",
        "    #Supply a function which returns a tf.data.Dataset\n",
        "    dataset_fn=msa_en_translation_dataset_fn,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[msa_en_translation_preprocessor],\n",
        "    # Lowercase targets before computing metrics.\n",
        "\n",
        "    #postprocess_fn=t5.data.postprocessors.lower_text, \n",
        "\n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "\n",
        "    metric_fns=[t5.evaluation.metrics.bleu],\n",
        "\n",
        "    # Not required, but helps for mixing and auto-caching.\n",
        "    num_input_examples=msa_en_example_count,\n",
        "    # output_features\n",
        "    output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(filepath)),\n",
        "    # specifying token processor\n",
        "    token_preprocessor=[\n",
        "      functools.partial(\n",
        "          preprocessors.select_random_chunk,\n",
        "          feature_key=\"targets\",\n",
        "          max_length=65536\n",
        "      ),\n",
        "      functools.partial(\n",
        "          preprocessors.reduce_concat_tokens,\n",
        "          feature_key=\"targets\",\n",
        "          batch_size=128\n",
        "      ),\n",
        "      preprocessors.split_tokens_to_inputs_length,\n",
        "      functools.partial(\n",
        "          preprocessors.denoise,\n",
        "          inputs_fn=preprocessors.noise_span_to_unique_sentinel,\n",
        "          targets_fn=preprocessors.nonnoise_span_to_unique_sentinel,\n",
        "          noise_density=0.15,\n",
        "          noise_mask_fn=preprocessors.iid_noise_mask,\n",
        "      )\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGr8wt7naDRL",
        "colab_type": "text"
      },
      "source": [
        "###Levantine to MSA Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e3A3wQdjagH6",
        "colab": {}
      },
      "source": [
        "lav_msa_split_csv_path = {\n",
        "    \"train\": \"data/train/train_lav_msa.csv\",\n",
        "    \"validation\": \"data/val/val_lav_msa.csv\"\n",
        "}\n",
        "lav_msa_example_count = {\n",
        "    \"train\": len(lav_msa_train),\n",
        "    \"validation\": len(lav_msa_val)\n",
        "}"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xCK_j-9BagH_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "403ccb44-5b53-4657-8849-86bc3af088f3"
      },
      "source": [
        "def lav_msa_translation_dataset_fn(split, shuffle_files=False):\n",
        "  ds = tf.data.TextLineDataset(lav_msa_split_csv_path[split])\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"\",\"\"],\n",
        "                        field_delim=\",\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "  )\n",
        "  ds = ds.map(lambda *example: dict(zip([\"source\", \"target\"], example)) )\n",
        "  return ds\n",
        "\n",
        "for example in tfds.as_numpy(lav_msa_translation_dataset_fn(\"train\").take(5)):\n",
        "    print(example)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'source': b'\\xd8\\xa8\\xd8\\xb5\\xd9\\x81\\xd9\\x87 \\xd8\\xb9\\xd8\\xa7\\xd9\\x85\\xd9\\x87 \\xd8\\xa7\\xd9\\x84\\xd8\\xb3\\xd8\\xad\\xd8\\xb1 \\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd8\\xb3\\xd9\\x88\\xd8\\xaf \\xd9\\x81\\xd9\\x89 \\xd9\\x87\\xd8\\xb0\\xd9\\x87 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd9\\x86\\xd8\\xa7\\xd8\\xb7\\xd9\\x82 \\xd9\\x8a\\xd8\\xb9\\xd8\\xaa\\xd9\\x85\\xd8\\xaf \\xd8\\xb9\\xd9\\x84\\xd9\\x89 \\xd8\\xa7\\xd9\\x84\\xd9\\x83\\xd8\\xab\\xd9\\x8a\\xd8\\xb1 \\xd9\\x85\\xd9\\x86 \\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd8\\xb4\\xd9\\x8a\\xd8\\xa7\\xd8\\xa1 ', 'target': b'\\xd8\\xa8\\xd8\\xb4\\xd9\\x83\\xd9\\x84 \\xd8\\xb9\\xd8\\xa7\\xd9\\x85 \\xd9\\x88\\xd9\\x83\\xd8\\xa7\\xd9\\x86 \\xd8\\xa7\\xd9\\x84\\xd8\\xb3\\xd8\\xad\\xd8\\xb1 \\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd8\\xb3\\xd9\\x88\\xd8\\xaf \\xd9\\x81\\xd9\\x8a \\xd8\\xaa\\xd9\\x84\\xd9\\x83 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd9\\x86\\xd8\\xa7\\xd8\\xb7\\xd9\\x82 \\xd9\\x8a\\xd8\\xb9\\xd8\\xaa\\xd9\\x85\\xd8\\xaf \\xd8\\xb9\\xd9\\x84\\xd9\\x89 \\xd9\\x83\\xd8\\xab\\xd9\\x8a\\xd8\\xb1 \\xd9\\x85\\xd9\\x86 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd9\\x85\\xd8\\xa7\\xd8\\xb1\\xd8\\xb3\\xd8\\xa7\\xd8\\xaa'}\n",
            "{'source': b'\\xd9\\x81\\xd9\\x87\\xd9\\x85\\xd8\\xaa \\xd9\\x83\\xd9\\x8a\\xd9\\x81', 'target': b'\\xd9\\x81\\xd9\\x87\\xd9\\x85\\xd8\\xaa \\xd9\\x83\\xd9\\x8a\\xd9\\x81'}\n",
            "{'source': b'\\xd9\\x86\\xd8\\xb9\\xd9\\x85 \\xd8\\xb0\\xd9\\x84\\xd9\\x83 \\xd8\\xac\\xd9\\x8a\\xd8\\xaf  \\xd8\\xa7\\xd9\\x86\\xd9\\x87 \\xd9\\x8a\\xd8\\xb4\\xd8\\xa8\\xd9\\x87 \\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xb7\\xd8\\xb1  \\xd8\\xa7\\xd9\\x84\\xd8\\xb0\\xd9\\x8a \\xd8\\xa7\\xd8\\xad\\xd8\\xb6\\xd8\\xb1\\xd9\\x87 \\xd8\\xa7\\xd8\\xa8\\xd9\\x8a \\xd9\\x85\\xd9\\x86 \\xd9\\x81\\xd8\\xb1\\xd9\\x86\\xd8\\xb3\\xd8\\xa7', 'target': b'\\xd8\\xa7\\xd9\\x8a \\xd9\\x87\\xd8\\xa7\\xd8\\xaf \\xd9\\x85\\xd9\\x86\\xd9\\x8a\\xd8\\xad \\xd8\\xa8\\xd8\\xaa\\xd8\\xb4\\xd8\\xa8\\xd9\\x87 \\xd8\\xb1\\xd9\\x8a\\xd8\\xad\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xb7\\xd8\\xb1 \\xd8\\xa7\\xd9\\x84\\xd9\\x84\\xd9\\x8a \\xd8\\xac\\xd8\\xa7\\xd8\\xa8\\xd9\\x88 \\xd8\\xa8\\xd8\\xa7\\xd8\\xa8\\xd8\\xa7 \\xd9\\x85\\xd9\\x86 \\xd9\\x81\\xd8\\xb1\\xd9\\x86\\xd8\\xb3\\xd8\\xa7'}\n",
            "{'source': b'\\xd8\\xb9\\xd8\\xa7\\xd8\\xaf\\xd9\\x8a  \\xd8\\xa7\\xd9\\x84\\xd8\\xab\\xd9\\x8a\\xd8\\xa7\\xd8\\xa8 \\xd8\\xa7\\xd9\\x88 \\xd8\\xb4\\xd9\\x8a\\xd8\\xa1 \\xd8\\xa7\\xd8\\xae\\xd8\\xb1', 'target': b'\\xd8\\xb9\\xd8\\xa7\\xd8\\xaf\\xd9\\x8a \\xd8\\xa7\\xd9\\x84\\xd8\\xab\\xd9\\x8a\\xd8\\xa7\\xd8\\xa8 \\xd8\\xa7\\xd9\\x88 \\xd8\\xb4\\xd9\\x8a'}\n",
            "{'source': b'\\xd9\\x84\\xd8\\xa7\\xd9\\x86 \\xd8\\xa7\\xd9\\x84\\xd8\\xa8\\xd9\\x8a\\xd9\\x88\\xd9\\x84\\xd9\\x88\\xd8\\xac\\xd9\\x8a\\xd8\\xa7 \\xd9\\x84\\xd9\\x85\\xd8\\xa7\\xd8\\xb0\\xd8\\xa7 \\xd8\\xa7\\xd9\\x84\\xd8\\xa8\\xd9\\x8a\\xd9\\x88\\xd9\\x84\\xd9\\x88\\xd8\\xac\\xd9\\x8a\\xd8\\xa7  \\xd8\\xa7\\xd8\\xb3\\xd8\\xaa\\xd8\\xb7\\xd9\\x8a\\xd8\\xb9 \\xd9\\x85\\xd8\\xab\\xd9\\x84\\xd8\\xa7 \\xd8\\xa7\\xd9\\x86 \\xd8\\xa7\\xd8\\xb9\\xd9\\x85\\xd9\\x84 \\xd9\\x81\\xd9\\x8a \\xd8\\xb5\\xd9\\x8a\\xd8\\xaf\\xd9\\x84\\xd9\\x8a\\xd8\\xa9 ', 'target': b'\\xd8\\xb9\\xd8\\xb4\\xd8\\xa7\\xd9\\x86 \\xd8\\xa7\\xd9\\x84\\xd8\\xa8\\xd9\\x8a\\xd9\\x88\\xd9\\x84\\xd9\\x88\\xd8\\xac\\xd9\\x8a\\xd8\\xa7 \\xd9\\x84\\xd9\\x8a\\xd8\\xb4 \\xd8\\xa7\\xd9\\x84\\xd8\\xa8\\xd9\\x8a\\xd9\\x88\\xd9\\x84\\xd9\\x88\\xd8\\xac\\xd9\\x8a\\xd8\\xa7 \\xd8\\xa8\\xd9\\x82\\xd8\\xaf\\xd8\\xb1 \\xd8\\xa7\\xd9\\x86\\xd9\\x8a \\xd8\\xa7\\xd8\\xb4\\xd8\\xaa\\xd8\\xba\\xd9\\x84 \\xd8\\xb9\\xd9\\x86\\xd8\\xaf \\xd8\\xb5\\xd9\\x8a\\xd8\\xaf\\xd9\\x84\\xd9\\x8a\\xd8\\xa9 \\xd9\\x85\\xd8\\xab\\xd9\\x84\\xd8\\xa7'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fW9eNYoGagID",
        "colab": {}
      },
      "source": [
        "#turn the ds of dictionaries and change the keys to inputs and targets that the model\n",
        "def lav_msa_translation_preprocessor(ds):\n",
        "  def to_inputs_and_targets(ex):\n",
        "    return{\n",
        "        \"inputs\": tf.strings.join([\"translate Levantine to MSA: \",ex[\"source\"]]),\n",
        "        \"targets\": ex[\"target\"]\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mDoUsP1AagIG",
        "colab": {}
      },
      "source": [
        "t5.data.TaskRegistry.remove(\"translation_lav_msa\")\n",
        "t5.data.TaskRegistry.add(\n",
        "    #name of the Task\n",
        "    \"translation_lav_msa\",\n",
        "    #Supply a function which returns a tf.data.Dataset\n",
        "    dataset_fn=lav_msa_translation_dataset_fn,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[lav_msa_translation_preprocessor],\n",
        "    # Lowercase targets before computing metrics.\n",
        "\n",
        "    #postprocess_fn = t5.data.postprocessors.lower_text, \n",
        "\n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "\n",
        "    #metric_fns=[t5.evaluation.metrics.bleu],\n",
        "\n",
        "    # Not required, but helps for mixing and auto-caching.\n",
        "    num_input_examples=lav_msa_example_count,\n",
        "    # output_features\n",
        "    output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(filepath)),\n",
        "    # specifying token processor\n",
        "    token_preprocessor=[\n",
        "      functools.partial(\n",
        "          preprocessors.select_random_chunk,\n",
        "          feature_key=\"targets\",\n",
        "          max_length=65536\n",
        "      ),\n",
        "      functools.partial(\n",
        "          preprocessors.reduce_concat_tokens,\n",
        "          feature_key=\"targets\",\n",
        "          batch_size=128\n",
        "      ),\n",
        "      preprocessors.split_tokens_to_inputs_length,\n",
        "      functools.partial(\n",
        "          preprocessors.denoise,\n",
        "          inputs_fn=preprocessors.noise_span_to_unique_sentinel,\n",
        "          targets_fn=preprocessors.nonnoise_span_to_unique_sentinel,\n",
        "          noise_density=0.15,\n",
        "          noise_mask_fn=preprocessors.iid_noise_mask,\n",
        "      )\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wIK8EWyaLFR",
        "colab_type": "text"
      },
      "source": [
        "###Maghrib to MSA Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bVhYmQnQaj2K",
        "colab": {}
      },
      "source": [
        "mag_msa_split_csv_path = {\n",
        "    \"train\": \"data/train/train_mag_msa.csv\",\n",
        "    \"validation\": \"data/val/val_mag_msa.csv\"\n",
        "}\n",
        "mag_msa_example_count = {\n",
        "    \"train\": len(mag_msa_train),\n",
        "    \"validation\": len(mag_msa_val)\n",
        "}"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WBLF-J2Oaj2T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "c135c6f6-8d07-4a8e-dc33-a12a54988e48"
      },
      "source": [
        "def mag_msa_translation_dataset_fn(split, shuffle_files=False):\n",
        "  ds = tf.data.TextLineDataset(mag_msa_split_csv_path[split])\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"\",\"\"],\n",
        "                        field_delim=\",\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "  )\n",
        "  ds = ds.map(lambda *example: dict(zip([\"source\", \"target\"], example)) )\n",
        "  return ds\n",
        "\n",
        "for example in tfds.as_numpy(mag_msa_translation_dataset_fn(\"train\").take(5)):\n",
        "    print(example)\n",
        "    print(example['source'].decode())\n",
        "    print(example['target'].decode())\n",
        "    print(len(example['source']))\n",
        "    print(len(example['target']))"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'source': b'\\xd9\\x85\\xd9\\x8a\\xd9\\x85\\xd9\\x8a \\xd8\\xaf\\xd8\\xb9\\xd9\\x8a \\xd9\\x87\\xd8\\xa7\\xd8\\xaa\\xd9\\x81\\xd9\\x83 \\xd9\\x88 \\xd8\\xb4\\xd8\\xa7\\xd9\\x86\\xd9\\x87 \\xd9\\x84\\xd9\\x82\\xd8\\xaf \\xd8\\xac\\xd9\\x86 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xb3\\xd9\\x83\\xd9\\x8a\\xd9\\x86', 'target': b'\\xd9\\x85\\xd9\\x8a\\xd9\\x85\\xd9\\x8a \\xd8\\xae\\xd9\\x84\\xd9\\x8a \\xd8\\xa7\\xd9\\x84\\xd8\\xaa\\xd9\\x84\\xd9\\x8a\\xd9\\x81\\xd9\\x88\\xd9\\x86 \\xd9\\x88 \\xd8\\xae\\xd9\\x84\\xd9\\x8a\\xd9\\x87 \\xd8\\xb9\\xd9\\x84\\xd9\\x89 \\xd8\\xae\\xd8\\xa7\\xd8\\xb7\\xd8\\xb1\\xd9\\x88 \\xd8\\xb1\\xd8\\xa7\\xd9\\x87 \\xd9\\x85\\xd8\\xb3\\xd9\\x83\\xd9\\x8a\\xd9\\x86 \\xd8\\xad\\xd9\\x85\\xd9\\x82\\xd8\\xaa\\xd9\\x8a\\xd9\\x87'}\n",
            "ميمي دعي هاتفك و شانه لقد جن المسكين\n",
            "ميمي خلي التليفون و خليه على خاطرو راه مسكين حمقتيه\n",
            "65\n",
            "93\n",
            "{'source': b'\\xd9\\x83\\xd9\\x85\\xd8\\xa7 \\xd9\\x82\\xd9\\x84\\xd8\\xaa \\xd9\\x87\\xd8\\xb0\\xd8\\xa7 \\xd9\\x83\\xd8\\xa7\\xd9\\x81 \\xd8\\xac\\xd8\\xaf\\xd8\\xa7 \\xd9\\x84\\xd8\\xa7\\xd9\\x86\\xd9\\x83 \\xd8\\xb3\\xd8\\xaa\\xd8\\xb1\\xd8\\xa7\\xd9\\x81\\xd9\\x82\\xd9\\x8a\\xd9\\x86 \\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xb1\\xd9\\x88\\xd8\\xb3  \\xd9\\x88 \\xd9\\x81\\xd9\\x8a \\xd8\\xa7\\xd9\\x84\\xd9\\x84\\xd9\\x8a\\xd9\\x84 \\xd8\\xaa\\xd8\\xb1\\xd8\\xaa\\xd8\\xaf\\xd9\\x8a\\xd9\\x86  \\xd8\\xa7\\xd8\\xae\\xd8\\xb1 \\xd8\\xa7 \\xd9\\x88 \\xd8\\xa7\\xd9\\x86\\xd8\\xaa\\xd9\\x87\\xd9\\x89 \\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd9\\x85\\xd8\\xb1', 'target': b'\\xd9\\x83\\xd9\\x8a\\xd9\\x81 \\xd9\\x82\\xd9\\x84\\xd8\\xaa \\xd9\\x87\\xd8\\xaf\\xd8\\xb4\\xd9\\x8a \\xd9\\x83\\xd8\\xa7\\xd9\\x81\\xd9\\x8a \\xd8\\xad\\xd9\\x8a\\xd8\\xaa \\xd8\\xba\\xd8\\xa7\\xd8\\xaf\\xd9\\x8a \\xd8\\xaa\\xd9\\x85\\xd8\\xb4\\xd9\\x8a \\xd9\\x85\\xd8\\xb9 \\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xb1\\xd9\\x88\\xd8\\xb3 \\xd9\\x88 \\xd9\\x81 \\xd8\\xa7\\xd9\\x84\\xd9\\x84\\xd9\\x8a\\xd9\\x84 \\xd9\\x86\\xd8\\xaa\\xd9\\x8a \\xd9\\x84\\xd8\\xa7\\xd8\\xa8\\xd8\\xb3\\xd8\\xa9 \\xd9\\x88\\xd8\\xa7\\xd8\\xad\\xd8\\xaf \\xd8\\xa7\\xd8\\xae\\xd8\\xb1 \\xd9\\x88 \\xd8\\xb3\\xd8\\xa7\\xd9\\x84\\xd9\\x8a\\xd9\\x86\\xd8\\xa7'}\n",
            "كما قلت هذا كاف جدا لانك سترافقين العروس  و في الليل ترتدين  اخر ا و انتهى الامر\n",
            "كيف قلت هدشي كافي حيت غادي تمشي مع العروس و ف الليل نتي لابسة واحد اخر و سالينا\n",
            "142\n",
            "141\n",
            "{'source': b'\\xd8\\xa7\\xd9\\x86\\xd8\\xa7 \\xd8\\xa7\\xd8\\xad\\xd8\\xaf\\xd8\\xab\\xd9\\x83  \\xd8\\xa7\\xd8\\xb0\\xd9\\x87\\xd8\\xa8\\xd9\\x8a \\xd9\\x88 \\xd8\\xa7\\xd8\\xa8\\xd8\\xad\\xd8\\xab\\xd9\\x8a \\xd8\\xb9\\xd9\\x86 \\xd8\\xb9\\xd9\\x85\\xd9\\x84  \\xd8\\xa7\\xd8\\xb9\\xd9\\x85\\xd9\\x84\\xd9\\x8a', 'target': b'\\xd8\\xa7\\xd9\\x86\\xd8\\xa7 \\xd9\\x83\\xd9\\x86\\xd9\\x87\\xd8\\xb6\\xd8\\xb1 \\xd9\\x85\\xd8\\xb9\\xd9\\x83 \\xd8\\xb3\\xd9\\x8a\\xd8\\xb1\\xd9\\x8a \\xd9\\x88\\xd9\\x82\\xd9\\x84\\xd8\\xa8\\xd9\\x8a \\xd8\\xb9\\xd9\\x84\\xd9\\x89 \\xd8\\xae\\xd8\\xaf\\xd9\\x85\\xd8\\xa9'}\n",
            "انا احدثك  اذهبي و ابحثي عن عمل  اعملي\n",
            "انا كنهضر معك سيري وقلبي على خدمة\n",
            "67\n",
            "60\n",
            "{'source': b'\\xd8\\xa7\\xd8\\xaa\\xd9\\x85\\xd9\\x86\\xd9\\x89 \\xd9\\x84\\xd9\\x87\\xd8\\xa7 \\xd8\\xa7\\xd9\\x84\\xd8\\xaa\\xd9\\x88\\xd9\\x81\\xd9\\x8a\\xd9\\x82', 'target': b'\\xd8\\xa8\\xd8\\xa7\\xd9\\x84\\xd8\\xaa\\xd9\\x88\\xd9\\x81\\xd9\\x8a\\xd9\\x82'}\n",
            "اتمنى لها التوفيق\n",
            "بالتوفيق\n",
            "32\n",
            "16\n",
            "{'source': b'\\xd8\\xa7\\xd9\\x86\\xd8\\xa7 \\xd8\\xa7\\xd9\\x82\\xd9\\x88\\xd9\\x84 \\xd9\\x84\\xd9\\x87\\xd8\\xa7 \\xd8\\xb9\\xd8\\xa7\\xd8\\xa6\\xd8\\xb4\\xd8\\xa9 \\xd8\\xa7\\xd9\\x86\\xd8\\xa7 \\xd9\\x85\\xd8\\xb1\\xd9\\x8a\\xd8\\xb6\\xd8\\xa9 \\xd9\\x88 \\xd9\\x87\\xd9\\x8a \\xd8\\xaa\\xd9\\x82\\xd9\\x88\\xd9\\x84 \\xd8\\xa7\\xd8\\xb0\\xd9\\x87\\xd8\\xa8\\xd9\\x8a \\xd9\\x81\\xd8\\xad\\xd8\\xb3\\xd8\\xa8', 'target': b'\\xd8\\xa7\\xd9\\x86\\xd8\\xa7 \\xd8\\xaa\\xd9\\x86\\xd9\\x82\\xd9\\x88\\xd9\\x84 \\xd9\\x84\\xd9\\x8a\\xd9\\x87\\xd8\\xa7 \\xd8\\xb9\\xd8\\xa7\\xd8\\xa1\\xd8\\xb4\\xd8\\xa9 \\xd8\\xa7\\xd9\\x86\\xd8\\xa7 \\xd9\\x85\\xd8\\xb1\\xd9\\x8a\\xd8\\xb6\\xd8\\xa9 \\xd9\\x88 \\xd9\\x87\\xd9\\x8a \\xd8\\xaa\\xd8\\xaa\\xd9\\x82\\xd9\\x88\\xd9\\x84 \\xd9\\x84\\xd9\\x8a \\xd8\\xb5\\xd8\\xa7\\xd9\\x81\\xd9\\x8a \\xd8\\xb3\\xd9\\x8a\\xd8\\xb1\\xd9\\x8a'}\n",
            "انا اقول لها عائشة انا مريضة و هي تقول اذهبي فحسب\n",
            "انا تنقول ليها عاءشة انا مريضة و هي تتقول لي صافي سيري\n",
            "88\n",
            "97\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GgyqkGfYaj2Y",
        "colab": {}
      },
      "source": [
        "  #turn the ds of dictionaries and change the keys to inputs and targets that the model\n",
        "def mag_msa_translation_preprocessor(ds):\n",
        "    def to_inputs_and_targets(ex):\n",
        "      return{\n",
        "          \"inputs\": tf.strings.join([\"translate Maghrib to MSA: \",ex[\"source\"]]),\n",
        "          \"targets\": ex[\"target\"]\n",
        "      }\n",
        "    return ds.map(to_inputs_and_targets, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bmja1M2Eaj2f",
        "colab": {}
      },
      "source": [
        "t5.data.TaskRegistry.remove(\"translation_mag_msa\")\n",
        "t5.data.TaskRegistry.add(\n",
        "    #name of the Task\n",
        "    \"translation_mag_msa\",\n",
        "    #Supply a function which returns a tf.data.Dataset\n",
        "    dataset_fn=mag_msa_translation_dataset_fn,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[mag_msa_translation_preprocessor],\n",
        "    # Lowercase targets before computing metrics.\n",
        "\n",
        "    #postprocess_fn = t5.data.postprocessors.lower_text, \n",
        "    \n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    \n",
        "    #metric_fns=[t5.evaluation.metrics.bleu],\n",
        "    \n",
        "    # Not required, but helps for mixing and auto-caching.\n",
        "    num_input_examples=mag_msa_example_count,\n",
        "    # output_features\n",
        "    output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(filepath)),\n",
        "    # specifying token processor\n",
        "    token_preprocessor=[\n",
        "      functools.partial(\n",
        "          preprocessors.select_random_chunk,\n",
        "          feature_key=\"targets\",\n",
        "          max_length=65536\n",
        "      ),\n",
        "      functools.partial(\n",
        "          preprocessors.reduce_concat_tokens,\n",
        "          feature_key=\"targets\",\n",
        "          batch_size=128\n",
        "      ),\n",
        "      preprocessors.split_tokens_to_inputs_length,\n",
        "      functools.partial(\n",
        "          preprocessors.denoise,\n",
        "          inputs_fn=preprocessors.noise_span_to_unique_sentinel,\n",
        "          targets_fn=preprocessors.nonnoise_span_to_unique_sentinel,\n",
        "          noise_density=0.15,\n",
        "          noise_mask_fn=preprocessors.iid_noise_mask,\n",
        "      )\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLoe7vbFV42o",
        "colab_type": "text"
      },
      "source": [
        "##Dataset Mixture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWMTtMsXWAua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t5.data.MixtureRegistry.remove(\"translation_msa\")\n",
        "t5.data.MixtureRegistry.add(\n",
        "    \"translation_msa\",\n",
        "    [\"translation_msa_en\", \"translation_lav_msa\", \"translation_mag_msa\"],\n",
        "     default_rate=1.0\n",
        ")"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc0bSgP1WenA",
        "colab_type": "text"
      },
      "source": [
        "##Pre-Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw146D5HkkPi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "588da9fd-b97e-4a80-dbe8-7680aff0f90c"
      },
      "source": [
        "#gotta get the base config and add the new tasks' task params\n",
        "!wget \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-config.json\" -O data/config/t5-base-config.json"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-29 02:21:35--  https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-config.json\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.240.142\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.240.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1199 (1.2K) [application/json]\n",
            "Saving to: ‘data/config/t5-base-config.json’\n",
            "\n",
            "\r          data/conf   0%[                    ]       0  --.-KB/s               \rdata/config/t5-base 100%[===================>]   1.17K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-08-29 02:21:35 (64.2 MB/s) - ‘data/config/t5-base-config.json’ saved [1199/1199]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uP2fwStXK5b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a7780e77-69d4-422e-afe2-1978c4ebd5e1"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "#Using the base config from Huggingface T5 Model\n",
        "config = transformers.T5Config.from_json_file(json_file=\"data/config/t5-base-config.json\")\n",
        "model = t5.models.HfPyTorchModel(config, \"/tmp/hft5/\", device)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Loading from /tmp/hft5/model-10000.checkpoint\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M23Y4l0tb4qM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "7cdddb0c-9972-4abf-8fcd-bd2cb84a7d08"
      },
      "source": [
        "ls /tmp/hft5"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "events.out.tfevents.1598661619.6f0b5dc6b701.111.0  model-2000.checkpoint\n",
            "events.out.tfevents.1598667702.6f0b5dc6b701.111.1  model-4000.checkpoint\n",
            "example_predictions.txt                            model-6000.checkpoint\n",
            "model-0.checkpoint                                 model-8000.checkpoint\n",
            "model-10000.checkpoint                             \u001b[0m\u001b[01;34mvalidation_eval\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VV2RSZGxQxYT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1fEVj9jCxvcKn9zg8lO43i2sWZquegg5H' -O data/operative_config.gin"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4ZqkAzufRI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import gin\n",
        "# with gin.unlock_config():\n",
        "#   gin.parse_config_file(\"data/operative_config.gin\")"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6f5uUWXWUKw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "cbd2c93f-5040-4779-9606-8b25f062b07f"
      },
      "source": [
        "STEPS = 1000 #@param {type: \"integer\"}\n",
        "model.train(\n",
        "    mixture_or_task_name=\"translation_msa\",\n",
        "    steps=STEPS,\n",
        "    save_steps=STEPS/5,                                                   \n",
        "    sequence_length={\"inputs\": 32, \"targets\": 32},\n",
        "    split=\"train\",\n",
        "    batch_size=32,\n",
        "    optimizer=functools.partial(transformers.AdamW, lr=1e-4),\n",
        ")"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/t5/data/utils.py:273: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  return dataset.map(my_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
            "INFO:absl:Saving checkpoint for step 12045\n",
            "INFO:absl:Saving checkpoint for step 12245\n",
            "INFO:absl:Saving checkpoint for step 12445\n",
            "INFO:absl:Saving checkpoint for step 12645\n",
            "INFO:absl:Saving checkpoint for step 12845\n",
            "INFO:absl:Saving final checkpoint for step 13045\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Uv-8hgBo8Lb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ac3a097b-9a4d-4538-a9f4-3ebe362dcb34"
      },
      "source": [
        "review_task = t5.data.TaskRegistry.get(\"translation_msa_en\")\n",
        "ds = review_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 128, \"targets\": 32})\n",
        "print(\"A few preprocessed validation examples...\")\n",
        "for ex in tfds.as_numpy(ds.take(5)):\n",
        "  print(ex['inputs'])"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/t5/data/utils.py:273: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  return dataset.map(my_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "A few preprocessed validation examples...\n",
            "[   61     5  5998    32    36   309   184    51   699  2093   232 24734\n",
            "    39  2324 24733  1109   116  8061   190 24732 11311  4913   388 24731\n",
            "   208 24730  1101 24729  4969 24728     5  3396  1638   727    36    39\n",
            " 24727  3437    51 10185   321   303   487 24726  1293   121  1027  3013\n",
            "    62   160    36   487  2188 24725    61  3612   260   135    36    82\n",
            "   256 24724  4950  2881 24723   121  1558   827    39 24722  4677  1793\n",
            "   303 24721  1314   696   150  5001 15869   170    61  1911   336 24720\n",
            "  1813    88  2787 24719   184   256   987    51  4182     5  1393   573\n",
            "    39  1028  2407  6634  2707    62  1616 24718    32   568    72  2150\n",
            " 24717  9990   170 24716  5258 24715  1028   247    72 11275  5568    62\n",
            "     5     2  5556 17237 10085    88     1]\n",
            "[24734  6665   303 24733   597    51  4675  5956   292  3331  6246   528\n",
            "    39   830    88  2093 24732   154    51  2256   330   242    54  2725\n",
            "    62   260    54   827  6357    62   697  1034  2737 24731  1207   231\n",
            "  5568 24730    39 24729   849    39    51   256 24728   118    32    36\n",
            "   216   622     5     2 15485 12937   174   781   118   378 10306   863\n",
            "  5224   309   541  2252   539  2060   174  3332  4082    72  8185   209\n",
            "   154 24727  6354   118    61  5590 24726    32  4102   309 24725   512\n",
            " 24724   184 24723  1342  1142    32    36   309 24722    54  1115  2700\n",
            "    32   184  1767 24721    61  4405 24720    32    36   309  8958    82\n",
            "    32    36   216  2699 24719    36    39   284 15076   321    51   869\n",
            "   322   231  8947  5589    32    36   216     1]\n",
            "[  781   118    51  1608 24734   626  1740  1326  2187    39   292 11651\n",
            "    62   979   241    54 24733   770   828  1608    62    32   404  1115\n",
            "  1639    32   184  1283  1261   656   135    72  3017 24732 16287 13225\n",
            "  3899   116 24731    36    82  2702  3463   725    88 24730   303  1300\n",
            "  1462    39  3809   751 24729 14679  5259 24728    61  1590    51   699\n",
            "   659    88 24727    62   174    36 24726   256   918   150  3916  3478\n",
            "    39   930    32   869    51    61  1829    62    32   281    36    82\n",
            "  2451    54  1616 18245   699 24725   979    88    61     5 24724 14310\n",
            "    62   160  1043   573    88   626   260    54   656 24723    62   330\n",
            "   242    54  5583   171 24722    62   930 24721     5 24720   247    62\n",
            " 24719    36   558  7409   190   116   223     1]\n",
            "[ 5353   118 24734  5990   322  3998    39   659   241    54   241   223\n",
            "    62   336   121  2115  7466    39     5     2 10634   135     5     2\n",
            "  6527   247    51   154  1200   116   379   135    36    82   411     5\n",
            "  1281 24733    72  4426   414   223   573 24732    82   231     5   558\n",
            " 17637  3317  1559 24731   231  8987    36    39  9957    62   824  2476\n",
            "   247  2505   121   208  2136  5961   242   512    39   174 17262   190\n",
            " 24730     5 17227  3200   170    61  2571    32  1849    51     5     2\n",
            "  2247  1027 24729   330 24728 10043    62     5     2  2173    51 24727\n",
            "     5     2 21986 12208 10000 24726   754   170 24725  1590  1375   539\n",
            "  2060   675    36    39 24724   121   184  2841   395  1115  3437 24723\n",
            "     1]\n",
            "[   51  3767   121   601 24734  1616  2075    54  1200 24733    72 24732\n",
            "  1414 17346    62    32 24731   487   231     5 24730   232 24729  1314\n",
            "    51  8168 24728    51  2697  4183  1224    54  1798   503    62    32\n",
            "   184    72  1052  2549 24727   160  1026   241 24726  7558   121   260\n",
            "    54   411     5  1281    51  1685    72   741    62 24725  2785   290\n",
            "    39   512    32  2984  1979    54  8977 24724  6346 24723   302 24722\n",
            " 12497  2484   528    39    88 24721   388  1351    39   727    88   150\n",
            " 15403  2734 24720 14108   223   116 24719    61 21381 11212   863  5114\n",
            "   558 24718    72  6362    51 17258 24717    36   216  3300   190   923\n",
            "   160 24716  1590   322 24715 16657    32  2702  1205    32   281 24714\n",
            "  3470   293    36    39     5     1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev_E4ZadXxmj",
        "colab_type": "text"
      },
      "source": [
        "##Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjZnTOIrYEh4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "e73f40b5-ff9e-4bd5-d757-4593cf36afeb"
      },
      "source": [
        "# Evaluate after fine-tuning\n",
        "model.eval(\n",
        "    mixture_or_task_name=\"translation_msa\",\n",
        "    checkpoint_steps= 13045,\n",
        "    sequence_length={\"inputs\": 32, \"targets\": 32},\n",
        "    batch_size=32,\n",
        ")"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Loading from /tmp/hft5/model-13045.checkpoint\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-100-b4ce4d8ef6dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcheckpoint_steps\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m13045\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"inputs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"targets\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/t5/models/hf_model.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, mixture_or_task_name, sequence_length, batch_size, checkpoint_steps, summary_dir, split, **generate_kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcheckpoint_step\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcheckpoint_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0m_eval_current_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m   def predict(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/t5/models/hf_model.py\u001b[0m in \u001b[0;36m_eval_current_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    436\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m         \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_examples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_targets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'translation_msa_en'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF-3_4u1X-5r",
        "colab_type": "text"
      },
      "source": [
        "##Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL5yLXs4YJw7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = [\n",
        "    \"translate Levantine to MSA: هلا والله\",\n",
        "    \"translate MSA to English: Please help me fix this\",\n",
        "]\n",
        "model.predict(\n",
        "    inputs,\n",
        "    sequence_length={\"inputs\": 32},\n",
        "    batch_size=2,\n",
        "    output_file=\"/tmp/hft5/example_predictions.txt\",\n",
        "    vocabulary=t5.data.SentencePieceVocabulary(filepath),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}