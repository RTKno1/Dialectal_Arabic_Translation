{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XLM_Model.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c1e827ec4f5442c2a00c34ae039d138f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a487da36aa924fb0b5b28a4c29864f53",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a42fa691b4374695b9f0b62d542ba2ed",
              "IPY_MODEL_5511c5f6d2b1493fb85a5dcb9cfbe44b"
            ]
          }
        },
        "a487da36aa924fb0b5b28a4c29864f53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a42fa691b4374695b9f0b62d542ba2ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_90c8d66b0ed24724a3f44b9d979381ca",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 791656,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 791656,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5af7d98fd6694d6fad30c58310123950"
          }
        },
        "5511c5f6d2b1493fb85a5dcb9cfbe44b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7e2218fd03524e6c9fcff617b37480c2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 792k/792k [00:22&lt;00:00, 35.4kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_07592c02541b47f2a32a98fa4ca5deae"
          }
        },
        "90c8d66b0ed24724a3f44b9d979381ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5af7d98fd6694d6fad30c58310123950": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7e2218fd03524e6c9fcff617b37480c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "07592c02541b47f2a32a98fa4ca5deae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jameschartouni/arabic_translation/blob/xlm/Model_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQGE8JtI2EAt",
        "colab_type": "code",
        "outputId": "2783ab85-0467-4252-fd45-0ad114d69ae4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#installation step\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install bpemb\n",
        "#creating the folders \n",
        "!mkdir data/\n",
        "!mkdir data/AD_NMT-master\n",
        "!mkdir data/train/\n",
        "!mkdir data/test/\n",
        "!mkdir data/val/\n",
        "!mkdir data/vocab\n",
        "!mkdir data/model\n",
        "\n",
        "#fetching the pkl files\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW' -O data/AD_NMT-master/english-Arabic-both.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1V8_tp8ZlWUYaX7QQL46t0uSRNrVehSf1' -O data/AD_NMT-master/english-Arabic-test.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1V7X0qtuDIyjTHY0wh-ZNoVwsiF4lId2e' -O data/AD_NMT-master/english-Arabic-train.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV' -O data/AD_NMT-master/LAV-MSA-2-both.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UpfCbkxhztof7dvNjeAs1bHjD4SER6h3' -O data/AD_NMT-master/LAV-MSA-2-test.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UlAZGtYsSfXzK7hrC_PbxQFqTSXD0DMw' -O data/AD_NMT-master/LAV-MSA-2-train.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UjDX7cCG2S23SPfSHxSPdVayMTxB5Y16' -O data/AD_NMT-master/Magribi_MSA-both.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UaVWIqRXo0rxuxDF4KArA4bEK1TaLX3l' -O data/AD_NMT-master/Magribi_MSA-test.pkl\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UYvlhdYAdfa4riP_4hn3-IEVd1ZUXVTQ' -O data/AD_NMT-master/Magribi_MSA-train.pkl"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/b5/ac41e3e95205ebf53439e4dd087c58e9fd371fd8e3724f2b9b4cdb8282e5/transformers-2.10.0-py3-none-any.whl (660kB)\n",
            "\u001b[K     |████████████████████████████████| 665kB 8.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 31.3MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 48.3MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 45.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=283837a16a5f8e968f648d2ead9580794950a7b8c0bb4ccbfbf88d75475a30ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.10.0\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.91)\n",
            "Collecting bpemb\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/70/468a9652095b370f797ed37ff77e742b11565c6fd79eaeca5f2e50b164a7/bpemb-0.3.0-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from bpemb) (2.23.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from bpemb) (0.1.91)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from bpemb) (3.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from bpemb) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from bpemb) (1.18.4)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (1.24.3)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim->bpemb) (1.12.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->bpemb) (2.0.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim->bpemb) (1.4.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->bpemb) (1.13.13)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->bpemb) (2.49.0)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.13 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->bpemb) (1.16.13)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->bpemb) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->bpemb) (0.10.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->smart-open>=1.2.1->gensim->bpemb) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->smart-open>=1.2.1->gensim->bpemb) (2.8.1)\n",
            "Installing collected packages: bpemb\n",
            "Successfully installed bpemb-0.3.0\n",
            "--2020-05-26 23:07:51--  https://docs.google.com/uc?export=download&id=1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.119.138, 108.177.119.113, 108.177.119.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.119.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-10-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/l1cj48ek5c1r89i032rp80kpbdmdptd6/1590534450000/16970776037313924126/*/1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-05-26 23:07:52--  https://doc-10-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/l1cj48ek5c1r89i032rp80kpbdmdptd6/1590534450000/16970776037313924126/*/1V9crCmqvgQcv0Sx2MCNWB9AET2j6M6FW?e=download\n",
            "Resolving doc-10-2s-docs.googleusercontent.com (doc-10-2s-docs.googleusercontent.com)... 108.177.126.132, 2a00:1450:4013:c01::84\n",
            "Connecting to doc-10-2s-docs.googleusercontent.com (doc-10-2s-docs.googleusercontent.com)|108.177.126.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 955428 (933K) [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/english-Arabic-both.pkl’\n",
            "\n",
            "data/AD_NMT-master/ 100%[===================>] 933.04K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2020-05-26 23:07:53 (107 MB/s) - ‘data/AD_NMT-master/english-Arabic-both.pkl’ saved [955428/955428]\n",
            "\n",
            "--2020-05-26 23:07:54--  https://docs.google.com/uc?export=download&id=1V8_tp8ZlWUYaX7QQL46t0uSRNrVehSf1\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.119.139, 108.177.119.101, 108.177.119.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.119.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-14-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/3ekp3tbmd8hg9h71m740iec2s74l69pr/1590534450000/16970776037313924126/*/1V8_tp8ZlWUYaX7QQL46t0uSRNrVehSf1?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-05-26 23:07:54--  https://doc-14-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/3ekp3tbmd8hg9h71m740iec2s74l69pr/1590534450000/16970776037313924126/*/1V8_tp8ZlWUYaX7QQL46t0uSRNrVehSf1?e=download\n",
            "Resolving doc-14-2s-docs.googleusercontent.com (doc-14-2s-docs.googleusercontent.com)... 108.177.126.132, 2a00:1450:4013:c01::84\n",
            "Connecting to doc-14-2s-docs.googleusercontent.com (doc-14-2s-docs.googleusercontent.com)|108.177.126.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 95497 (93K) [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/english-Arabic-test.pkl’\n",
            "\n",
            "data/AD_NMT-master/ 100%[===================>]  93.26K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2020-05-26 23:07:54 (129 MB/s) - ‘data/AD_NMT-master/english-Arabic-test.pkl’ saved [95497/95497]\n",
            "\n",
            "--2020-05-26 23:07:56--  https://docs.google.com/uc?export=download&id=1V7X0qtuDIyjTHY0wh-ZNoVwsiF4lId2e\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.119.101, 108.177.119.139, 108.177.119.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.119.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-10-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/0btk1i5o82dlpp82cq8nugdosmu6i4cj/1590534450000/16970776037313924126/*/1V7X0qtuDIyjTHY0wh-ZNoVwsiF4lId2e?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-05-26 23:07:56--  https://doc-10-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/0btk1i5o82dlpp82cq8nugdosmu6i4cj/1590534450000/16970776037313924126/*/1V7X0qtuDIyjTHY0wh-ZNoVwsiF4lId2e?e=download\n",
            "Resolving doc-10-2s-docs.googleusercontent.com (doc-10-2s-docs.googleusercontent.com)... 108.177.126.132, 2a00:1450:4013:c01::84\n",
            "Connecting to doc-10-2s-docs.googleusercontent.com (doc-10-2s-docs.googleusercontent.com)|108.177.126.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 859172 (839K) [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/english-Arabic-train.pkl’\n",
            "\n",
            "data/AD_NMT-master/ 100%[===================>] 839.04K  --.-KB/s    in 0.006s  \n",
            "\n",
            "2020-05-26 23:07:56 (148 MB/s) - ‘data/AD_NMT-master/english-Arabic-train.pkl’ saved [859172/859172]\n",
            "\n",
            "--2020-05-26 23:07:58--  https://docs.google.com/uc?export=download&id=1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.119.138, 108.177.119.113, 108.177.119.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.119.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-00-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/tolqk2a1sfj63qa6oeegt6drvqhc319n/1590534450000/16970776037313924126/*/1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-05-26 23:07:59--  https://doc-00-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/tolqk2a1sfj63qa6oeegt6drvqhc319n/1590534450000/16970776037313924126/*/1UzL4cOWTMCee83KBUh2QO_H62AFVpDQV?e=download\n",
            "Resolving doc-00-2s-docs.googleusercontent.com (doc-00-2s-docs.googleusercontent.com)... 108.177.126.132, 2a00:1450:4013:c01::84\n",
            "Connecting to doc-00-2s-docs.googleusercontent.com (doc-00-2s-docs.googleusercontent.com)|108.177.126.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/LAV-MSA-2-both.pkl’\n",
            "\n",
            "data/AD_NMT-master/     [ <=>                ]   2.33M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-05-26 23:07:59 (226 MB/s) - ‘data/AD_NMT-master/LAV-MSA-2-both.pkl’ saved [2447014]\n",
            "\n",
            "--2020-05-26 23:08:00--  https://docs.google.com/uc?export=download&id=1UpfCbkxhztof7dvNjeAs1bHjD4SER6h3\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.119.138, 108.177.119.100, 108.177.119.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.119.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0c-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/r412610e91vietljkuh7j14cu0nvehqn/1590534450000/16970776037313924126/*/1UpfCbkxhztof7dvNjeAs1bHjD4SER6h3?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-05-26 23:08:01--  https://doc-0c-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/r412610e91vietljkuh7j14cu0nvehqn/1590534450000/16970776037313924126/*/1UpfCbkxhztof7dvNjeAs1bHjD4SER6h3?e=download\n",
            "Resolving doc-0c-2s-docs.googleusercontent.com (doc-0c-2s-docs.googleusercontent.com)... 108.177.126.132, 2a00:1450:4013:c01::84\n",
            "Connecting to doc-0c-2s-docs.googleusercontent.com (doc-0c-2s-docs.googleusercontent.com)|108.177.126.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 304332 (297K) [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/LAV-MSA-2-test.pkl’\n",
            "\n",
            "data/AD_NMT-master/ 100%[===================>] 297.20K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2020-05-26 23:08:01 (113 MB/s) - ‘data/AD_NMT-master/LAV-MSA-2-test.pkl’ saved [304332/304332]\n",
            "\n",
            "--2020-05-26 23:08:02--  https://docs.google.com/uc?export=download&id=1UlAZGtYsSfXzK7hrC_PbxQFqTSXD0DMw\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.119.139, 108.177.119.101, 108.177.119.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.119.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0c-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/7olg0tojbuk6us8hq78kukig96c70j6k/1590534450000/16970776037313924126/*/1UlAZGtYsSfXzK7hrC_PbxQFqTSXD0DMw?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-05-26 23:08:03--  https://doc-0c-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/7olg0tojbuk6us8hq78kukig96c70j6k/1590534450000/16970776037313924126/*/1UlAZGtYsSfXzK7hrC_PbxQFqTSXD0DMw?e=download\n",
            "Resolving doc-0c-2s-docs.googleusercontent.com (doc-0c-2s-docs.googleusercontent.com)... 108.177.126.132, 2a00:1450:4013:c01::84\n",
            "Connecting to doc-0c-2s-docs.googleusercontent.com (doc-0c-2s-docs.googleusercontent.com)|108.177.126.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/LAV-MSA-2-train.pkl’\n",
            "\n",
            "data/AD_NMT-master/     [ <=>                ]   2.04M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-05-26 23:08:03 (207 MB/s) - ‘data/AD_NMT-master/LAV-MSA-2-train.pkl’ saved [2141923]\n",
            "\n",
            "--2020-05-26 23:08:05--  https://docs.google.com/uc?export=download&id=1UjDX7cCG2S23SPfSHxSPdVayMTxB5Y16\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.119.138, 108.177.119.100, 108.177.119.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.119.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-04-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/lb0gv6284pb4df7fkksibj0dcn25js8m/1590534450000/16970776037313924126/*/1UjDX7cCG2S23SPfSHxSPdVayMTxB5Y16?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-05-26 23:08:06--  https://doc-04-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/lb0gv6284pb4df7fkksibj0dcn25js8m/1590534450000/16970776037313924126/*/1UjDX7cCG2S23SPfSHxSPdVayMTxB5Y16?e=download\n",
            "Resolving doc-04-2s-docs.googleusercontent.com (doc-04-2s-docs.googleusercontent.com)... 108.177.126.132, 2a00:1450:4013:c01::84\n",
            "Connecting to doc-04-2s-docs.googleusercontent.com (doc-04-2s-docs.googleusercontent.com)|108.177.126.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/Magribi_MSA-both.pkl’\n",
            "\n",
            "data/AD_NMT-master/     [ <=>                ]   2.81M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-05-26 23:08:07 (212 MB/s) - ‘data/AD_NMT-master/Magribi_MSA-both.pkl’ saved [2944107]\n",
            "\n",
            "--2020-05-26 23:08:08--  https://docs.google.com/uc?export=download&id=1UaVWIqRXo0rxuxDF4KArA4bEK1TaLX3l\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.119.139, 108.177.119.101, 108.177.119.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.119.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0o-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/lsh2jrcc5b954vebjutrn7v5j4ghkrbl/1590534450000/16970776037313924126/*/1UaVWIqRXo0rxuxDF4KArA4bEK1TaLX3l?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-05-26 23:08:08--  https://doc-0o-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/lsh2jrcc5b954vebjutrn7v5j4ghkrbl/1590534450000/16970776037313924126/*/1UaVWIqRXo0rxuxDF4KArA4bEK1TaLX3l?e=download\n",
            "Resolving doc-0o-2s-docs.googleusercontent.com (doc-0o-2s-docs.googleusercontent.com)... 108.177.126.132, 2a00:1450:4013:c01::84\n",
            "Connecting to doc-0o-2s-docs.googleusercontent.com (doc-0o-2s-docs.googleusercontent.com)|108.177.126.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 290840 (284K) [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/Magribi_MSA-test.pkl’\n",
            "\n",
            "data/AD_NMT-master/ 100%[===================>] 284.02K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2020-05-26 23:08:09 (161 MB/s) - ‘data/AD_NMT-master/Magribi_MSA-test.pkl’ saved [290840/290840]\n",
            "\n",
            "--2020-05-26 23:08:10--  https://docs.google.com/uc?export=download&id=1UYvlhdYAdfa4riP_4hn3-IEVd1ZUXVTQ\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.119.138, 108.177.119.100, 108.177.119.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.119.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0s-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ssk0qrojg3kiac3dred3ni50mflk9cnb/1590534450000/16970776037313924126/*/1UYvlhdYAdfa4riP_4hn3-IEVd1ZUXVTQ?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-05-26 23:08:10--  https://doc-0s-2s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ssk0qrojg3kiac3dred3ni50mflk9cnb/1590534450000/16970776037313924126/*/1UYvlhdYAdfa4riP_4hn3-IEVd1ZUXVTQ?e=download\n",
            "Resolving doc-0s-2s-docs.googleusercontent.com (doc-0s-2s-docs.googleusercontent.com)... 108.177.126.132, 2a00:1450:4013:c01::84\n",
            "Connecting to doc-0s-2s-docs.googleusercontent.com (doc-0s-2s-docs.googleusercontent.com)|108.177.126.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/octet-stream]\n",
            "Saving to: ‘data/AD_NMT-master/Magribi_MSA-train.pkl’\n",
            "\n",
            "data/AD_NMT-master/     [ <=>                ]   2.53M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-05-26 23:08:11 (213 MB/s) - ‘data/AD_NMT-master/Magribi_MSA-train.pkl’ saved [2652508]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHBIZHP717oD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#James Chartouni\n",
        "#Joey Park\n",
        "#Raef Khan\n",
        "\n",
        "import torch\n",
        "from torch.optim import SGD\n",
        "from torch import nn, optim\n",
        "from torchtext import data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os, io, glob\n",
        "import functools\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from bpemb import BPEmb\n",
        "\n",
        "import transformers\n",
        "from transformers import XLMTokenizer, XLMModel\n",
        "from transformers import T5Tokenizer, T5Model, T5Config\n",
        "import torchtext\n",
        "from torchtext.data import Field, BucketIterator\n",
        "from torchtext.datasets import TranslationDataset\n",
        "\n",
        "\n",
        "import sentencepiece as spm\n",
        "\n",
        "import time\n",
        "import random\n",
        "import math\n",
        "import copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0C_7QhY2CpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_path = 'data/AD_NMT-master/'\n",
        "\n",
        "with open(file_path + \"english-Arabic-train.pkl\", 'rb') as handle:\n",
        "    data_English_MSA_trainval = pickle.load(handle)\n",
        "\n",
        "with open(file_path + \"english-Arabic-test.pkl\", 'rb') as handle:\n",
        "    data_English_MSA_test = pickle.load(handle)\n",
        "\n",
        "with open(file_path + \"english-Arabic-both.pkl\", 'rb') as handle:\n",
        "    data_English_MSA_both = pickle.load(handle) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with open(file_path + \"LAV-MSA-2-train.pkl\", 'rb') as handle:\n",
        "    data_LAV_MSA_trainval = pickle.load(handle) \n",
        "\n",
        "with open(file_path + \"LAV-MSA-2-test.pkl\", 'rb') as handle:\n",
        "    data_LAV_MSA_test = pickle.load(handle) \n",
        "\n",
        "with open(file_path + \"LAV-MSA-2-both.pkl\", 'rb') as handle:\n",
        "    data_LAV_MSA_both = pickle.load(handle) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with open(file_path + \"Magribi_MSA-train.pkl\", 'rb') as handle:\n",
        "    data_Magribi_MSA_trainval = pickle.load(handle) \n",
        "    \n",
        "with open(file_path + \"Magribi_MSA-test.pkl\", 'rb') as handle:\n",
        "    data_Magribi_MSA_test = pickle.load(handle) \n",
        "\n",
        "with open(file_path + \"Magribi_MSA-both.pkl\", 'rb') as handle:\n",
        "    data_Magribi_MSA_both = pickle.load(handle) \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAqmicdq2DAt",
        "colab_type": "code",
        "outputId": "0d0659cd-e433-4e2d-cd93-ab840aa2564f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "print(data_English_MSA_both[0:5])\n",
        "print(data_LAV_MSA_both[0:5])\n",
        "print(len(data_English_MSA_trainval))\n",
        "print(len(data_English_MSA_both))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Tom was also there', 'كان توم هنا ايضا'], ['That old woman lives by herself', 'تلك المراة العجوز تسكن بمفردها'], ['He went abroad for the purpose of studying English', 'سافر خارج البلد ليتعلم الانجليزية'], ['There is a fork missing', 'هناك شوكة ناقصة'], [\"I don't know this game\", 'لا اعرف هذه اللعبة']]\n",
            "[['لا انا بعرف وحدة راحت ع فرنسا و معا شنتا حطت فيها الفرش', 'لا اعرف واحدة ذهبت الى فرنسا و لها غرفة و ضعت فيها الافرشة'], ['روح بوشك و فتول عاليسار', 'اذهب تقدم و استدر يسارا'], ['لا لا لازم انه يكون عندك موضوع ما في اشي', ' لا لا يجب ان يكون لديك موضوع هذا ضروري'], ['اوعي تبعدي من هون بلاش تضيعي ', 'لا تبتعد عن هنا حتى لا تفقد الطريق '], ['قصدي صراحة يما انا كمان كرهته من يوم ما عملتيه زي ما بتعمله خالتي كرهته و صرت ما باطيقه بالمرة', 'اقصد صراحة يا امي انا ايضا كرهته من يوم حضرته مثلما تحضره خالتي كرهته و اصبحت لا اطيقه ابدا']]\n",
            "9000\n",
            "10001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYZYCLQC6trn",
        "colab_type": "text"
      },
      "source": [
        "##Prepare Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my3IwzMYlwOH",
        "colab_type": "text"
      },
      "source": [
        "SPM Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmCGcvChlwBH",
        "colab_type": "code",
        "outputId": "bf90052b-1984-44a0-944b-ca47ae426fe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "ls data/AD_NMT-master/"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "english-Arabic-both.pkl   LAV-MSA-2-both.pkl   Magribi_MSA-both.pkl\n",
            "english-Arabic-test.pkl   LAV-MSA-2-test.pkl   Magribi_MSA-test.pkl\n",
            "english-Arabic-train.pkl  LAV-MSA-2-train.pkl  Magribi_MSA-train.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YK09XFxHlv3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#splits the train dataset into train and validation sets, define test set as datafile\n",
        "eng_msa_train, eng_msa_val = train_test_split(data_English_MSA_trainval, test_size=.2, random_state=22)\n",
        "eng_msa_test = data_English_MSA_test\n",
        "\n",
        "lav_msa_train, lav_msa_val = train_test_split(data_LAV_MSA_trainval, test_size=.2, random_state=22)\n",
        "lav_msa_test = data_LAV_MSA_test\n",
        "\n",
        "mag_msa_train, mag_msa_val = train_test_split(data_Magribi_MSA_trainval, test_size=.2, random_state=22)\n",
        "mag_msa_test = data_Magribi_MSA_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76IhUXDDlvus",
        "colab_type": "code",
        "outputId": "b8a27382-54e2-487b-ee56-546244c76b4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "print(len(eng_msa_train))\n",
        "print(len(eng_msa_val))\n",
        "\n",
        "print(len(lav_msa_train))\n",
        "print(len(lav_msa_val))\n",
        "\n",
        "print(len(mag_msa_train))\n",
        "print(len(mag_msa_val))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7200\n",
            "1800\n",
            "11044\n",
            "2761\n",
            "14188\n",
            "3548\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGPhoAAwuLv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_path = 'data/'\n",
        "\n",
        "def pytorch_format(ds, src='en', trg='msa', datatype=''):\n",
        "    src_formatted = datatype + '_' + src + '_' + trg + '.' + src\n",
        "    trg_formatted = datatype + '_' + src + '_' + trg + '.' + trg\n",
        "    \n",
        "    with open(file_path + datatype + \"/\" + src_formatted, 'wt') as srctxt, open(file_path + datatype + \"/\" + trg_formatted, 'wt') as trgtxt:\n",
        "        for i, arr in enumerate(ds):\n",
        "            #srctxt.write(datatype.upper() + '_' + str(i).zfill( len(str(len(ds))) - len(str(i))) + '\\\\01\\\\' + arr[0] + '\\n')\n",
        "            #trgtxt.write(datatype.upper() + '_' + str(i).zfill( len(str(len(ds))) - len(str(i))) + '\\\\01\\\\' + arr[1] + '\\n')\n",
        "            srctxt.write(arr[0] + '\\n')\n",
        "            trgtxt.write(arr[1] + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0ZMa0wguMQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#splits each language pair file into datasets of single language, to be merged again by the pytorch dataset class later\n",
        "\n",
        "pytorch_format(eng_msa_train, 'eng', 'msa', 'train')\n",
        "pytorch_format(eng_msa_val, 'eng', 'msa', 'val')\n",
        "pytorch_format(eng_msa_test, 'eng', 'msa', 'test')\n",
        "\n",
        "pytorch_format(lav_msa_train, 'lav', 'msa', 'train')\n",
        "pytorch_format(lav_msa_val, 'lav', 'msa', 'val')\n",
        "pytorch_format(lav_msa_test, 'lav', 'msa', 'test')\n",
        "\n",
        "pytorch_format(mag_msa_train, 'mag', 'msa', 'train')\n",
        "pytorch_format(mag_msa_val, 'mag', 'msa', 'val')\n",
        "pytorch_format(mag_msa_test, 'mag', 'msa', 'test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPQRYfAguMc5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Testing:\n",
        "Create a text file with all the vocab available from all sources for each language for SentencePiece to create a library \n",
        "\n",
        "TODO: implement bpemb model for MSA vocab/train SPM on larger datasets for dialects\n",
        "\"\"\"\n",
        "\n",
        "en_vocab = open(\"data/vocab/eng_vocab.txt\", \"wt\")\n",
        "msa_vocab = open(\"data/vocab/msa_vocab.txt\", \"wt\")\n",
        "lav_vocab = open(\"data/vocab/lav_vocab.txt\", \"wt\")\n",
        "mag_vocab = open(\"data/vocab/mag_vocab.txt\", \"wt\")\n",
        "\n",
        "MSA_text = \"\"\n",
        "EN_text = \"\"\n",
        "\n",
        "def create_vocab(file='', src='en_vocab', tgt='msa_vocab'):\n",
        "  for line in file:\n",
        "        src_sent = line[0]\n",
        "        src_words = src_sent.split(\" \")\n",
        "        for count, word in enumerate(src_words):\n",
        "            src.write(word)\n",
        "        src.write(\"\\n\")\n",
        "        \n",
        "        tgt_sent = line[1]\n",
        "        tgt_words = tgt_sent.split(\" \")\n",
        "        for count, word in enumerate(tgt_words):\n",
        "            tgt.write(word)\n",
        "        tgt.write(\"\\n\")\n",
        "\n",
        "create_vocab(data_English_MSA_both, en_vocab, msa_vocab)\n",
        "create_vocab(data_LAV_MSA_both, lav_vocab, msa_vocab)\n",
        "create_vocab(data_Magribi_MSA_both, mag_vocab, msa_vocab)\n",
        "\n",
        "en_vocab.close()\n",
        "msa_vocab.close()\n",
        "lav_vocab.close()\n",
        "mag_vocab.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgod9_6BMi4F",
        "colab_type": "code",
        "outputId": "826dc4f7-cb61-4349-aef7-7945294f1bfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "c1e827ec4f5442c2a00c34ae039d138f",
            "a487da36aa924fb0b5b28a4c29864f53",
            "a42fa691b4374695b9f0b62d542ba2ed",
            "5511c5f6d2b1493fb85a5dcb9cfbe44b",
            "90c8d66b0ed24724a3f44b9d979381ca",
            "5af7d98fd6694d6fad30c58310123950",
            "7e2218fd03524e6c9fcff617b37480c2",
            "07592c02541b47f2a32a98fa4ca5deae"
          ]
        }
      },
      "source": [
        "t5tok = T5Tokenizer.from_pretrained(\"t5-base\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c1e827ec4f5442c2a00c34ae039d138f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=791656.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eSxQuFaHrr9",
        "colab_type": "code",
        "outputId": "ab04355c-4541-4596-9b3f-169050a28b6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(t5tok.bos_token, t5tok.bos_token_id)\n",
        "print(t5tok.eos_token, t5tok.eos_token_id)\n",
        "print(t5tok.pad_token, t5tok.pad_token_id)\n",
        "print(t5tok.unk_token, t5tok.unk_token_id)\n",
        "print(t5tok.sep_token, t5tok.sep_token_id)\n",
        "#print(sp.encode_as_ids(\"<unk>\"))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using bos_token, but it is not set yet.\n",
            "Using bos_token, but it is not set yet.\n",
            "Using sep_token, but it is not set yet.\n",
            "Using sep_token, but it is not set yet.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "None None\n",
            "</s> 1\n",
            "<pad> 0\n",
            "<unk> 2\n",
            "None None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6U3XlqeuMl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://github.com/google/sentencepiece/issues/102\n",
        "VOCAB_SIZE = 32128\n",
        "spm.SentencePieceTrainer.train('--input=data/vocab/eng_vocab.txt,data/vocab/msa_vocab.txt,data/vocab/lav_vocab.txt,data/vocab/mag_vocab.txt --model_prefix=data/model/spm --vocab_size=' + str(VOCAB_SIZE) + ' --unk_id=2 --bos_id=-1 --eos_id=1 --pad_id=0')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMeYoGjJuMu_",
        "colab_type": "code",
        "outputId": "b8d3cfee-b1d3-4024-b96f-a9bfe07059ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('data/model/spm.model')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1G_QuNFEuM5M",
        "colab_type": "code",
        "outputId": "841d1048-53a9-455e-ed94-8bd61ab8a245",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(sp.encode_as_pieces('This is a test'))\n",
        "print(sp.encode_as_ids('This is a test'))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁This', '▁', 'is', '▁', 'a', '▁', 't', 'est']\n",
            "[552, 3, 100, 3, 76, 3, 52, 3071]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZWNSIJkwjNR",
        "colab_type": "code",
        "outputId": "48f017c7-3316-4778-8941-fc6eae362abf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(sp.encode_as_pieces('هناك شوكة ناقصة'))\n",
        "print(sp.encode_as_ids('هناك شوكة ناقصة'))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁هناك', '▁', 'شوكة', '▁', 'ناقصة']\n",
            "[396, 3, 10425, 3, 10999]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wo556u17wjTd",
        "colab_type": "code",
        "outputId": "7b8464ae-edcc-4494-fdd1-e1a3648733ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(sp.encode_as_pieces('This is a test'))\n",
        "print(sp.encode_as_ids('This is a test'))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁This', '▁', 'is', '▁', 'a', '▁', 't', 'est']\n",
            "[552, 3, 100, 3, 76, 3, 52, 3071]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVsYc4wj_xOX",
        "colab_type": "code",
        "outputId": "3ae7b5e4-02fd-4348-d051-414834d2bd39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sp.tokenize(\"This is a test\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[552, 3, 100, 3, 76, 3, 52, 3071]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VehVGWs27UKi",
        "colab_type": "text"
      },
      "source": [
        "Pytorch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOh_Y4GZnUHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://pytorch.org/text/_modules/torchtext/data/field.html\n",
        "class CustomField(Field):\n",
        "  \n",
        "  def __init__(self, tokenize=None, init_token=None, eos_token=None, lower=False):\n",
        "        Field.__init__(self)\n",
        "        self.tokenize = tokenize\n",
        "\n",
        "  def preprocess(self, x):\n",
        "    #print(x)\n",
        "    x = sp.tokenize((x.rstrip('\\n')))\n",
        "    return x\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FZyD9qPqVY3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SRC = CustomField(tokenize = sp,\n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower = False)\n",
        "\n",
        "TRG = CustomField(tokenize = sp,\n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower = False)\n",
        "\n",
        "SRC.use_vocab = True\n",
        "TRG.use_vocab = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MadOtKgwkmhW",
        "colab_type": "code",
        "outputId": "01c5606e-6a6d-4a6d-fb26-5b87a0a45d82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(SRC.tokenize.tokenize(\"this is a test\"))\n",
        "callable(SRC.tokenize.tokenize(\"this is a test\"))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3, 245, 3, 100, 3, 76, 3, 52, 3071]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8juNLQxUq1vM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eng_msa_train_dataset = TranslationDataset(path='data/train/train_eng_msa.', exts=('eng', 'msa'), fields=(SRC, TRG))\n",
        "lav_msa_train_dataset = TranslationDataset(path='data/train/train_lav_msa.', exts=('lav', 'msa'), fields=(SRC, TRG))\n",
        "mag_msa_train_dataset = TranslationDataset(path='data/train/train_mag_msa.', exts=('mag', 'msa'), fields=(SRC, TRG))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbjg_bj2OdVb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SRC.build_vocab(eng_msa_train_dataset, lav_msa_train_dataset, mag_msa_train_dataset)\n",
        "TRG.build_vocab(eng_msa_train_dataset, lav_msa_train_dataset, mag_msa_train_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJ-fMU08ENX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, valid_data, test_data = eng_msa_train_dataset.splits(path= 'data/', train='train/train_eng_msa', validation='val/val_eng_msa', test='test/test_eng_msa', exts=('.eng', '.msa'),\n",
        "                                                    fields = (SRC, TRG))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rmmEiTLrGO7",
        "colab_type": "code",
        "outputId": "4e96d8d8-230f-4c4a-9e17-a076ab2dfbff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(train_data[10].src)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[129, 248, 3, 1382, 20]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0A4HJN4FEOdb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE =32\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device,\n",
        "    shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7nLAt5DrjLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pad_idx = TRG.vocab.stoi['<pad>']\n",
        "eos_idx = TRG.vocab.stoi['<eos>']\n",
        "sos_idx = TRG.vocab.stoi['<sos>']\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hq1ASMZt5OZN",
        "colab_type": "code",
        "outputId": "996b715e-3dc6-4afe-ba9c-d3af3e06b181",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "print(pad_idx)\n",
        "print(eos_idx)\n",
        "print(sos_idx)\n",
        "print(TRG.vocab.stoi)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "0\n",
            "0\n",
            "defaultdict(<function _default_unk_index at 0x7f42c11f5e18>, {'<unk>': 0, '<pad>': 1, 3: 2, 23: 3, 254: 4, 34: 5, 627: 6, 247: 7, 6: 8, 238: 9, 43: 10, 8: 11, 7: 12, 4: 13, 15: 14, 227: 15, 39: 16, 153: 17, 42: 18, 735: 19, 116: 20, 104: 21, 12: 22, 16: 23, 25: 24, 1855: 25, 2269: 26, 117: 27, 28: 28, 11: 29, 19: 30, 173: 31, 10: 32, 27: 33, 68: 34, 203: 35, 1597: 36, 2822: 37, 196: 38, 95: 39, 304: 40, 26: 41, 77: 42, 866: 43, 41: 44, 5878: 45, 157: 46, 266: 47, 35: 48, 232: 49, 496: 50, 396: 51, 120: 52, 229: 53, 31: 54, 193: 55, 260: 56, 351: 57, 134: 58, 558: 59, 183: 60, 44: 61, 486: 62, 1118: 63, 328: 64, 36: 65, 220: 66, 299: 67, 511: 68, 884: 69, 2332: 70, 3674: 71, 4127: 72, 2062: 73, 236: 74, 11183: 75, 4062: 76, 387: 77, 1553: 78, 118: 79, 664: 80, 60: 81, 1477: 82, 79: 83, 150: 84, 311: 85, 94: 86, 40: 87, 71: 88, 3011: 89, 128: 90, 144: 91, 802: 92, 2452: 93, 5: 94, 653: 95, 522: 96, 48: 97, 269: 98, 9: 99, 1959: 100, 1241: 101, 219: 102, 6034: 103, 82: 104, 6589: 105, 29: 106, 605: 107, 2716: 108, 7883: 109, 871: 110, 91: 111, 310: 112, 122: 113, 631: 114, 826: 115, 275: 116, 2102: 117, 4297: 118, 1831: 119, 55: 120, 88: 121, 30: 122, 13: 123, 9743: 124, 792: 125, 72: 126, 2271: 127, 3258: 128, 90: 129, 1130: 130, 127: 131, 124: 132, 237: 133, 1744: 134, 8175: 135, 252: 136, 305: 137, 1177: 138, 5222: 139, 33: 140, 190: 141, 2078: 142, 4265: 143, 246: 144, 1034: 145, 2677: 146, 1206: 147, 1345: 148, 1167: 149, 217: 150, 868: 151, 262: 152, 453: 153, 5652: 154, 1725: 155, 761: 156, 11217: 157, 5578: 158, 22: 159, 618: 160, 680: 161, 1363: 162, 145: 163, 5619: 164, 506: 165, 659: 166, 369: 167, 3341: 168, 2878: 169, 84: 170, 175: 171, 135: 172, 272: 173, 267: 174, 623: 175, 21: 176, 221: 177, 455: 178, 1006: 179, 4370: 180, 197: 181, 290: 182, 2852: 183, 928: 184, 2012: 185, 6069: 186, 10886: 187, 563: 188, 198: 189, 1962: 190, 1928: 191, 2130: 192, 1542: 193, 1887: 194, 277: 195, 662: 196, 111: 197, 443: 198, 1052: 199, 4331: 200, 5049: 201, 375: 202, 959: 203, 1048: 204, 93: 205, 598: 206, 1288: 207, 161: 208, 347: 209, 529: 210, 235: 211, 454: 212, 492: 213, 14: 214, 240: 215, 69: 216, 566: 217, 2550: 218, 10055: 219, 446: 220, 189: 221, 413: 222, 414: 223, 1544: 224, 255: 225, 15618: 226, 212: 227, 990: 228, 400: 229, 1065: 230, 239: 231, 2305: 232, 329: 233, 3196: 234, 1311: 235, 6782: 236, 50: 237, 4039: 238, 125: 239, 800: 240, 6561: 241, 4119: 242, 430: 243, 841: 244, 359: 245, 2530: 246, 4650: 247, 419: 248, 799: 249, 158: 250, 685: 251, 472: 252, 688: 253, 395: 254, 530: 255, 1080: 256, 137: 257, 1116: 258, 2538: 259, 9748: 260, 998: 261, 1007: 262, 4760: 263, 341: 264, 905: 265, 674: 266, 54: 267, 10779: 268, 1087: 269, 1325: 270, 176: 271, 360: 272, 2437: 273, 251: 274, 397: 275, 545: 276, 893: 277, 1995: 278, 1333: 279, 1765: 280, 7979: 281, 1574: 282, 2732: 283, 402: 284, 1355: 285, 3829: 286, 4318: 287, 804: 288, 1230: 289, 1366: 290, 4965: 291, 244: 292, 4589: 293, 450: 294, 523: 295, 250: 296, 2315: 297, 10061: 298, 11252: 299, 1457: 300, 284: 301, 8078: 302, 24: 303, 58: 304, 518: 305, 1339: 306, 1443: 307, 85: 308, 501: 309, 1711: 310, 2028: 311, 7943: 312, 2367: 313, 7383: 314, 1152: 315, 6652: 316, 676: 317, 2229: 318, 57: 319, 1077: 320, 1150: 321, 1728: 322, 3609: 323, 925: 324, 1303: 325, 2155: 326, 5573: 327, 550: 328, 1017: 329, 3291: 330, 3808: 331, 5647: 332, 7343: 333, 7873: 334, 6293: 335, 1160: 336, 154: 337, 656: 338, 724: 339, 1946: 340, 4812: 341, 8077: 342, 336: 343, 1304: 344, 1387: 345, 1719: 346, 1886: 347, 2296: 348, 5006: 349, 64: 350, 399: 351, 1064: 352, 4677: 353, 7255: 354, 588: 355, 684: 356, 7162: 357, 2: 358, 439: 359, 1742: 360, 1857: 361, 3281: 362, 541: 363, 742: 364, 1046: 365, 9648: 366, 14488: 367, 390: 368, 1172: 369, 3575: 370, 494: 371, 540: 372, 1085: 373, 1158: 374, 2252: 375, 3225: 376, 9425: 377, 482: 378, 1296: 379, 201: 380, 758: 381, 918: 382, 1059: 383, 340: 384, 649: 385, 1011: 386, 1225: 387, 2968: 388, 4025: 389, 4171: 390, 2765: 391, 4662: 392, 86: 393, 721: 394, 908: 395, 1435: 396, 2188: 397, 2469: 398, 5058: 399, 838: 400, 2067: 401, 2947: 402, 8032: 403, 211: 404, 241: 405, 363: 406, 5603: 407, 1146: 408, 1661: 409, 63: 410, 544: 411, 608: 412, 2038: 413, 440: 414, 510: 415, 743: 416, 1138: 417, 9368: 418, 556: 419, 560: 420, 593: 421, 859: 422, 864: 423, 474: 424, 1496: 425, 1674: 426, 2301: 427, 4193: 428, 9703: 429, 1298: 430, 1365: 431, 1573: 432, 1595: 433, 1996: 434, 620: 435, 2242: 436, 2467: 437, 4299: 438, 12776: 439, 1340: 440, 2246: 441, 3027: 442, 4021: 443, 727: 444, 840: 445, 1003: 446, 1110: 447, 1163: 448, 1547: 449, 2154: 450, 3206: 451, 6028: 452, 6507: 453, 296: 454, 599: 455, 613: 456, 904: 457, 1451: 458, 1582: 459, 15658: 460, 332: 461, 632: 462, 2071: 463, 2111: 464, 2190: 465, 3681: 466, 20553: 467, 18: 468, 774: 469, 1534: 470, 1703: 471, 2090: 472, 2779: 473, 7709: 474, 62: 475, 839: 476, 986: 477, 1051: 478, 1190: 479, 1739: 480, 2534: 481, 4826: 482, 4847: 483, 6198: 484, 11016: 485, 981: 486, 994: 487, 2546: 488, 3316: 489, 12599: 490, 14867: 491, 331: 492, 381: 493, 814: 494, 850: 495, 869: 496, 1009: 497, 1291: 498, 1460: 499, 1545: 500, 1617: 501, 1681: 502, 1983: 503, 2239: 504, 3119: 505, 3849: 506, 4024: 507, 177: 508, 398: 509, 412: 510, 712: 511, 769: 512, 1094: 513, 1799: 514, 3118: 515, 92: 516, 460: 517, 940: 518, 1083: 519, 1423: 520, 1798: 521, 4471: 522, 5341: 523, 46: 524, 188: 525, 204: 526, 283: 527, 569: 528, 832: 529, 926: 530, 1002: 531, 1061: 532, 2273: 533, 4313: 534, 4348: 535, 6032: 536, 488: 537, 524: 538, 903: 539, 957: 540, 1660: 541, 4710: 542, 827: 543, 879: 544, 953: 545, 1027: 546, 1405: 547, 1409: 548, 4307: 549, 7869: 550, 265: 551, 356: 552, 405: 553, 578: 554, 784: 555, 969: 556, 1062: 557, 1577: 558, 2116: 559, 2364: 560, 4292: 561, 6333: 562, 13191: 563, 114: 564, 886: 565, 1098: 566, 1381: 567, 3165: 568, 5192: 569, 6449: 570, 534: 571, 641: 572, 791: 573, 1013: 574, 1240: 575, 1399: 576, 1734: 577, 1956: 578, 5448: 579, 502: 580, 885: 581, 914: 582, 1144: 583, 1948: 584, 2174: 585, 6551: 586, 19251: 587, 195: 588, 601: 589, 768: 590, 3611: 591, 4995: 592, 13115: 593, 442: 594, 952: 595, 1189: 596, 1330: 597, 1815: 598, 2050: 599, 3454: 600, 3507: 601, 10586: 602, 380: 603, 521: 604, 594: 605, 698: 606, 922: 607, 1079: 608, 1221: 609, 1567: 610, 1869: 611, 3104: 612, 3484: 613, 182: 614, 595: 615, 856: 616, 1022: 617, 1121: 618, 2086: 619, 2212: 620, 2402: 621, 2717: 622, 3831: 623, 4060: 624, 5423: 625, 9230: 626, 9909: 627, 12298: 628, 12544: 629, 12692: 630, 15116: 631, 167: 632, 526: 633, 927: 634, 1315: 635, 1488: 636, 1882: 637, 2445: 638, 6634: 639, 9819: 640, 24725: 641, 364: 642, 640: 643, 1004: 644, 1008: 645, 1049: 646, 1084: 647, 1105: 648, 1252: 649, 1371: 650, 2267: 651, 3117: 652, 3823: 653, 5853: 654, 7387: 655, 233: 656, 383: 657, 505: 658, 592: 659, 812: 660, 924: 661, 1039: 662, 1103: 663, 1294: 664, 1580: 665, 1856: 666, 2319: 667, 4270: 668, 7317: 669, 12506: 670, 107: 671, 362: 672, 382: 673, 547: 674, 862: 675, 1054: 676, 1212: 677, 1280: 678, 1558: 679, 1790: 680, 3210: 681, 12361: 682, 12529: 683, 790: 684, 1362: 685, 1539: 686, 2099: 687, 3054: 688, 3333: 689, 3972: 690, 4682: 691, 15981: 692, 314: 693, 495: 694, 617: 695, 971: 696, 1123: 697, 2614: 698, 3439: 699, 4078: 700, 8343: 701, 15480: 702, 1293: 703, 1648: 704, 2409: 705, 2573: 706, 3269: 707, 3437: 708, 7169: 709, 15330: 710, 642: 711, 870: 712, 911: 713, 1018: 714, 1354: 715, 2348: 716, 2992: 717, 4011: 718, 7410: 719, 155: 720, 349: 721, 697: 722, 929: 723, 977: 724, 1016: 725, 1115: 726, 1427: 727, 1664: 728, 2668: 729, 2880: 730, 3414: 731, 4290: 732, 7370: 733, 295: 734, 327: 735, 355: 736, 686: 737, 776: 738, 803: 739, 874: 740, 1310: 741, 1326: 742, 1421: 743, 1537: 744, 1583: 745, 1771: 746, 2131: 747, 2254: 748, 2475: 749, 2712: 750, 2957: 751, 3106: 752, 3643: 753, 4035: 754, 12903: 755, 61: 756, 346: 757, 752: 758, 1023: 759, 1029: 760, 1295: 761, 1348: 762, 1380: 763, 1485: 764, 1672: 765, 1865: 766, 1927: 767, 2422: 768, 2549: 769, 2775: 770, 3371: 771, 3372: 772, 4178: 773, 4311: 774, 4702: 775, 9617: 776, 672: 777, 852: 778, 860: 779, 1180: 780, 1456: 781, 1589: 782, 1950: 783, 1965: 784, 2881: 785, 3730: 786, 4910: 787, 525: 788, 861: 789, 968: 790, 1173: 791, 1222: 792, 1268: 793, 2167: 794, 2787: 795, 3130: 796, 4504: 797, 4920: 798, 200: 799, 319: 800, 371: 801, 906: 802, 1082: 803, 1455: 804, 1555: 805, 1705: 806, 1740: 807, 1982: 808, 2016: 809, 3129: 810, 3214: 811, 5172: 812, 9254: 813, 12104: 814, 519: 815, 648: 816, 937: 817, 962: 818, 1308: 819, 1388: 820, 1434: 821, 1994: 822, 2210: 823, 2278: 824, 2877: 825, 2897: 826, 3134: 827, 3162: 828, 3308: 829, 3433: 830, 3682: 831, 3731: 832, 3745: 833, 4470: 834, 4482: 835, 4793: 836, 6376: 837, 7746: 838, 19634: 839, 159: 840, 353: 841, 459: 842, 989: 843, 1037: 844, 1139: 845, 1186: 846, 1263: 847, 1489: 848, 1507: 849, 1549: 850, 1618: 851, 1679: 852, 2095: 853, 2950: 854, 2977: 855, 4295: 856, 4917: 857, 5191: 858, 5242: 859, 6300: 860, 7730: 861, 8028: 862, 9146: 863, 11078: 864, 231: 865, 302: 866, 493: 867, 709: 868, 865: 869, 1170: 870, 1377: 871, 1557: 872, 1652: 873, 1783: 874, 1913: 875, 1930: 876, 2160: 877, 2286: 878, 2378: 879, 2817: 880, 3504: 881, 4446: 882, 5001: 883, 5540: 884, 12758: 885, 15406: 886, 23863: 887, 37: 888, 626: 889, 639: 890, 669: 891, 1031: 892, 1042: 893, 1057: 894, 1219: 895, 1492: 896, 1807: 897, 1877: 898, 1891: 899, 1953: 900, 2218: 901, 2410: 902, 2495: 903, 3876: 904, 4795: 905, 5782: 906, 7633: 907, 889: 908, 1176: 909, 1224: 910, 1259: 911, 1444: 912, 1623: 913, 1764: 914, 1863: 915, 1937: 916, 1951: 917, 2153: 918, 2176: 919, 2375: 920, 2499: 921, 2681: 922, 2756: 923, 2788: 924, 3821: 925, 4848: 926, 7283: 927, 12619: 928, 19707: 929, 56: 930, 226: 931, 1192: 932, 1653: 933, 1780: 934, 1961: 935, 2030: 936, 2100: 937, 2139: 938, 2555: 939, 2835: 940, 4291: 941, 4334: 942, 4458: 943, 5498: 944, 5867: 945, 6207: 946, 7181: 947, 10962: 948, 12256: 949, 15387: 950, 19401: 951, 354: 952, 875: 953, 1000: 954, 1071: 955, 1109: 956, 1469: 957, 1535: 958, 1632: 959, 1712: 960, 2048: 961, 2961: 962, 3585: 963, 4449: 964, 6179: 965, 6219: 966, 7826: 967, 11090: 968, 23954: 969, 163: 970, 378: 971, 449: 972, 1266: 973, 1316: 974, 1344: 975, 1509: 976, 1603: 977, 1834: 978, 1870: 979, 1908: 980, 2036: 981, 2209: 982, 2356: 983, 2457: 984, 2576: 985, 2603: 986, 2860: 987, 3235: 988, 3416: 989, 4340: 990, 4800: 991, 6592: 992, 6796: 993, 22603: 994, 136: 995, 222: 996, 516: 997, 946: 998, 1346: 999, 1440: 1000, 1454: 1001, 1487: 1002, 1565: 1003, 1760: 1004, 1797: 1005, 1858: 1006, 1934: 1007, 2245: 1008, 2387: 1009, 2700: 1010, 2774: 1011, 2776: 1012, 2848: 1013, 3043: 1014, 3274: 1015, 3455: 1016, 3687: 1017, 4192: 1018, 4819: 1019, 5175: 1020, 5226: 1021, 6472: 1022, 10740: 1023, 12700: 1024, 504: 1025, 625: 1026, 728: 1027, 1035: 1028, 1047: 1029, 1108: 1030, 1143: 1031, 1217: 1032, 1270: 1033, 1314: 1034, 1368: 1035, 1646: 1036, 1801: 1037, 1916: 1038, 2032: 1039, 2265: 1040, 2293: 1041, 3192: 1042, 3459: 1043, 3503: 1044, 3549: 1045, 3677: 1046, 4444: 1047, 4587: 1048, 6680: 1049, 9682: 1050, 10423: 1051, 11306: 1052, 12939: 1053, 259: 1054, 307: 1055, 809: 1056, 1019: 1057, 1069: 1058, 1424: 1059, 1520: 1060, 1587: 1061, 1686: 1062, 1775: 1063, 1806: 1064, 1859: 1065, 1904: 1066, 2270: 1067, 2341: 1068, 2940: 1069, 3046: 1070, 3276: 1071, 3393: 1072, 3404: 1073, 3567: 1074, 3617: 1075, 4103: 1076, 4364: 1077, 4964: 1078, 5231: 1079, 5397: 1080, 5620: 1081, 6536: 1082, 10873: 1083, 12907: 1084, 20640: 1085, 108: 1086, 574: 1087, 604: 1088, 786: 1089, 810: 1090, 958: 1091, 966: 1092, 1072: 1093, 1097: 1094, 1188: 1095, 1205: 1096, 1364: 1097, 1609: 1098, 2000: 1099, 2057: 1100, 2101: 1101, 2350: 1102, 2449: 1103, 2572: 1104, 2833: 1105, 3173: 1106, 3215: 1107, 3432: 1108, 3533: 1109, 4054: 1110, 6728: 1111, 7486: 1112, 7820: 1113, 9637: 1114, 10496: 1115, 15190: 1116, 15192: 1117, 24805: 1118, 883: 1119, 1050: 1120, 1286: 1121, 1290: 1122, 1506: 1123, 1581: 1124, 1591: 1125, 1810: 1126, 1888: 1127, 2414: 1128, 2638: 1129, 2840: 1130, 3186: 1131, 4000: 1132, 4141: 1133, 4344: 1134, 4350: 1135, 4408: 1136, 4651: 1137, 5345: 1138, 5716: 1139, 6137: 1140, 9453: 1141, 9840: 1142, 10037: 1143, 10767: 1144, 13036: 1145, 391: 1146, 663: 1147, 794: 1148, 1128: 1149, 1272: 1150, 1297: 1151, 1306: 1152, 1527: 1153, 1722: 1154, 1752: 1155, 1812: 1156, 2197: 1157, 2320: 1158, 2388: 1159, 2536: 1160, 2553: 1161, 2571: 1162, 2703: 1163, 3236: 1164, 3618: 1165, 4154: 1166, 4594: 1167, 5599: 1168, 6082: 1169, 9084: 1170, 9612: 1171, 11136: 1172, 13067: 1173, 13210: 1174, 14665: 1175, 15556: 1176, 23781: 1177, 51: 1178, 583: 1179, 716: 1180, 1044: 1181, 1302: 1182, 1526: 1183, 1658: 1184, 1827: 1185, 1910: 1186, 1941: 1187, 2104: 1188, 2311: 1189, 2370: 1190, 2441: 1191, 2795: 1192, 2894: 1193, 3004: 1194, 3429: 1195, 3558: 1196, 3582: 1197, 3906: 1198, 4033: 1199, 4430: 1200, 4547: 1201, 4753: 1202, 5143: 1203, 5145: 1204, 5518: 1205, 6700: 1206, 6702: 1207, 8110: 1208, 9800: 1209, 20008: 1210, 258: 1211, 426: 1212, 546: 1213, 1247: 1214, 1279: 1215, 1531: 1216, 1538: 1217, 1561: 1218, 1650: 1219, 2026: 1220, 2539: 1221, 2675: 1222, 2853: 1223, 3039: 1224, 3088: 1225, 3394: 1226, 3441: 1227, 3593: 1228, 4076: 1229, 4805: 1230, 5606: 1231, 5811: 1232, 5848: 1233, 7277: 1234, 8371: 1235, 9307: 1236, 11214: 1237, 12107: 1238, 87: 1239, 293: 1240, 315: 1241, 508: 1242, 571: 1243, 772: 1244, 1678: 1245, 1781: 1246, 1880: 1247, 1924: 1248, 1968: 1249, 2017: 1250, 2084: 1251, 2140: 1252, 2143: 1253, 2169: 1254, 2282: 1255, 2347: 1256, 2381: 1257, 2464: 1258, 2637: 1259, 2939: 1260, 3361: 1261, 3523: 1262, 3528: 1263, 3752: 1264, 3865: 1265, 3870: 1266, 3951: 1267, 4083: 1268, 4093: 1269, 4442: 1270, 4827: 1271, 5338: 1272, 5902: 1273, 6980: 1274, 7099: 1275, 14474: 1276, 14968: 1277, 15234: 1278, 20049: 1279, 47: 1280, 845: 1281, 849: 1282, 881: 1283, 1056: 1284, 1135: 1285, 1233: 1286, 1400: 1287, 1445: 1288, 1474: 1289, 1584: 1290, 1630: 1291, 1800: 1292, 1816: 1293, 1835: 1294, 1846: 1295, 1931: 1296, 2080: 1297, 2091: 1298, 2217: 1299, 2275: 1300, 2346: 1301, 2365: 1302, 2386: 1303, 2415: 1304, 2731: 1305, 2909: 1306, 2969: 1307, 3019: 1308, 3111: 1309, 3195: 1310, 3207: 1311, 3640: 1312, 3835: 1313, 3899: 1314, 4167: 1315, 4293: 1316, 4302: 1317, 4665: 1318, 5605: 1319, 5636: 1320, 6455: 1321, 7351: 1322, 8259: 1323, 10722: 1324, 10916: 1325, 12670: 1326, 13041: 1327, 18451: 1328, 19174: 1329, 28169: 1330, 70: 1331, 418: 1332, 553: 1333, 1556: 1334, 1680: 1335, 1770: 1336, 1820: 1337, 1978: 1338, 2020: 1339, 2152: 1340, 2285: 1341, 2401: 1342, 2434: 1343, 2567: 1344, 2578: 1345, 2620: 1346, 2652: 1347, 2876: 1348, 2936: 1349, 3142: 1350, 3217: 1351, 3606: 1352, 3613: 1353, 3663: 1354, 3705: 1355, 3765: 1356, 4380: 1357, 5011: 1358, 6394: 1359, 6534: 1360, 6611: 1361, 6681: 1362, 7012: 1363, 7972: 1364, 10786: 1365, 12826: 1366, 130: 1367, 725: 1368, 873: 1369, 1148: 1370, 1237: 1371, 1276: 1372, 1410: 1373, 1675: 1374, 1767: 1375, 1907: 1376, 2044: 1377, 2051: 1378, 2205: 1379, 2326: 1380, 2413: 1381, 2466: 1382, 2470: 1383, 2628: 1384, 2649: 1385, 2727: 1386, 2747: 1387, 2778: 1388, 3091: 1389, 3139: 1390, 3356: 1391, 3365: 1392, 3527: 1393, 3529: 1394, 3599: 1395, 3726: 1396, 3756: 1397, 3826: 1398, 3949: 1399, 3973: 1400, 4063: 1401, 4106: 1402, 4175: 1403, 4684: 1404, 4752: 1405, 5140: 1406, 5684: 1407, 5773: 1408, 5969: 1409, 6107: 1410, 7636: 1411, 7717: 1412, 7935: 1413, 11089: 1414, 12162: 1415, 12627: 1416, 12770: 1417, 15788: 1418, 17344: 1419, 45: 1420, 330: 1421, 350: 1422, 388: 1423, 551: 1424, 704: 1425, 754: 1426, 789: 1427, 973: 1428, 1165: 1429, 1500: 1430, 1532: 1431, 1543: 1432, 1596: 1433, 1611: 1434, 1784: 1435, 1842: 1436, 1889: 1437, 1935: 1438, 1949: 1439, 2118: 1440, 2181: 1441, 2339: 1442, 2431: 1443, 2594: 1444, 2845: 1445, 2859: 1446, 3059: 1447, 3223: 1448, 3336: 1449, 3389: 1450, 3392: 1451, 3714: 1452, 3787: 1453, 4208: 1454, 4227: 1455, 4323: 1456, 4821: 1457, 4835: 1458, 4969: 1459, 5641: 1460, 5893: 1461, 5903: 1462, 6684: 1463, 7334: 1464, 9271: 1465, 9630: 1466, 9725: 1467, 10203: 1468, 10760: 1469, 15719: 1470, 19899: 1471, 23901: 1472, 184: 1473, 557: 1474, 615: 1475, 997: 1476, 1283: 1477, 1323: 1478, 1372: 1479, 1483: 1480, 1494: 1481, 1714: 1482, 1735: 1483, 1736: 1484, 1773: 1485, 1795: 1486, 1902: 1487, 2037: 1488, 2451: 1489, 2474: 1490, 2489: 1491, 2490: 1492, 2659: 1493, 2794: 1494, 2984: 1495, 3014: 1496, 3033: 1497, 3227: 1498, 3272: 1499, 3302: 1500, 3542: 1501, 3635: 1502, 3644: 1503, 3754: 1504, 3842: 1505, 4046: 1506, 4335: 1507, 4530: 1508, 4720: 1509, 4841: 1510, 4986: 1511, 5685: 1512, 5938: 1513, 6039: 1514, 6468: 1515, 7000: 1516, 7014: 1517, 7225: 1518, 7388: 1519, 7789: 1520, 8176: 1521, 8812: 1522, 9272: 1523, 9681: 1524, 9739: 1525, 10109: 1526, 10528: 1527, 10731: 1528, 11226: 1529, 11328: 1530, 12882: 1531, 20266: 1532, 101: 1533, 699: 1534, 880: 1535, 898: 1536, 902: 1537, 917: 1538, 1119: 1539, 1223: 1540, 1373: 1541, 1374: 1542, 1406: 1543, 1430: 1544, 1572: 1545, 1579: 1546, 1601: 1547, 1610: 1548, 1635: 1549, 1779: 1550, 1912: 1551, 2107: 1552, 2182: 1553, 2248: 1554, 2307: 1555, 2345: 1556, 2411: 1557, 2433: 1558, 2629: 1559, 2645: 1560, 2670: 1561, 2676: 1562, 2870: 1563, 2928: 1564, 3028: 1565, 3329: 1566, 3359: 1567, 3362: 1568, 3376: 1569, 3379: 1570, 3382: 1571, 3430: 1572, 3434: 1573, 3583: 1574, 3598: 1575, 3628: 1576, 3834: 1577, 3848: 1578, 4065: 1579, 4080: 1580, 4468: 1581, 4554: 1582, 4603: 1583, 4935: 1584, 5024: 1585, 5043: 1586, 5446: 1587, 5501: 1588, 5563: 1589, 5575: 1590, 5722: 1591, 5844: 1592, 5925: 1593, 6158: 1594, 6237: 1595, 6892: 1596, 6923: 1597, 7451: 1598, 7715: 1599, 11119: 1600, 11286: 1601, 14898: 1602, 14936: 1603, 15483: 1604, 22899: 1605, 192: 1606, 194: 1607, 213: 1608, 464: 1609, 658: 1610, 944: 1611, 1501: 1612, 1602: 1613, 1713: 1614, 1824: 1615, 2054: 1616, 2144: 1617, 2227: 1618, 2234: 1619, 2570: 1620, 2575: 1621, 2729: 1622, 2920: 1623, 2980: 1624, 3006: 1625, 3025: 1626, 3053: 1627, 3197: 1628, 3323: 1629, 3377: 1630, 3569: 1631, 3649: 1632, 3696: 1633, 3792: 1634, 3815: 1635, 3841: 1636, 3909: 1637, 4105: 1638, 4128: 1639, 4781: 1640, 4799: 1641, 4923: 1642, 4993: 1643, 5164: 1644, 5266: 1645, 5517: 1646, 5523: 1647, 5774: 1648, 6168: 1649, 6407: 1650, 7617: 1651, 7806: 1652, 7913: 1653, 9894: 1654, 11436: 1655, 12613: 1656, 19611: 1657, 103: 1658, 169: 1659, 749: 1660, 1122: 1661, 1448: 1662, 1481: 1663, 1503: 1664, 1550: 1665, 1823: 1666, 1845: 1667, 2066: 1668, 2072: 1669, 2112: 1670, 2355: 1671, 2399: 1672, 2471: 1673, 2509: 1674, 2512: 1675, 2529: 1676, 2564: 1677, 2769: 1678, 2785: 1679, 2799: 1680, 2998: 1681, 3041: 1682, 3050: 1683, 3109: 1684, 3175: 1685, 3247: 1686, 3277: 1687, 3340: 1688, 3548: 1689, 3584: 1690, 3669: 1691, 3763: 1692, 4010: 1693, 4245: 1694, 4312: 1695, 4443: 1696, 4546: 1697, 4786: 1698, 5000: 1699, 5034: 1700, 5158: 1701, 5483: 1702, 6203: 1703, 6231: 1704, 6492: 1705, 7084: 1706, 7128: 1707, 7139: 1708, 7493: 1709, 8151: 1710, 8356: 1711, 9608: 1712, 9667: 1713, 10060: 1714, 10623: 1715, 11147: 1716, 13089: 1717, 13194: 1718, 14181: 1719, 14739: 1720, 20325: 1721, 26846: 1722, 49: 1723, 701: 1724, 896: 1725, 1187: 1726, 1350: 1727, 1453: 1728, 1701: 1729, 1833: 1730, 1945: 1731, 2260: 1732, 2715: 1733, 2751: 1734, 2782: 1735, 2827: 1736, 2872: 1737, 2908: 1738, 3068: 1739, 3296: 1740, 3385: 1741, 3524: 1742, 3684: 1743, 3689: 1744, 3703: 1745, 3767: 1746, 3800: 1747, 3917: 1748, 4159: 1749, 4204: 1750, 4367: 1751, 4441: 1752, 4792: 1753, 5050: 1754, 5376: 1755, 5643: 1756, 5980: 1757, 5986: 1758, 6335: 1759, 6646: 1760, 6701: 1761, 6760: 1762, 6786: 1763, 7029: 1764, 7177: 1765, 7404: 1766, 7753: 1767, 7815: 1768, 8065: 1769, 8123: 1770, 8148: 1771, 8264: 1772, 8917: 1773, 9427: 1774, 9464: 1775, 9633: 1776, 9994: 1777, 10046: 1778, 10057: 1779, 10958: 1780, 12614: 1781, 12955: 1782, 13288: 1783, 15112: 1784, 15686: 1785, 170: 1786, 273: 1787, 437: 1788, 1153: 1789, 1238: 1790, 1327: 1791, 1682: 1792, 1745: 1793, 1751: 1794, 1837: 1795, 1862: 1796, 1874: 1797, 1903: 1798, 2093: 1799, 2166: 1800, 2223: 1801, 2250: 1802, 2397: 1803, 2408: 1804, 2416: 1805, 2425: 1806, 2510: 1807, 2574: 1808, 2622: 1809, 2640: 1810, 2704: 1811, 2900: 1812, 2910: 1813, 3034: 1814, 3128: 1815, 3145: 1816, 3159: 1817, 3397: 1818, 3419: 1819, 3596: 1820, 3666: 1821, 3750: 1822, 3828: 1823, 3880: 1824, 4139: 1825, 4301: 1826, 4493: 1827, 4516: 1828, 4518: 1829, 4555: 1830, 4758: 1831, 4930: 1832, 4988: 1833, 5104: 1834, 5202: 1835, 5417: 1836, 5595: 1837, 5597: 1838, 5629: 1839, 5634: 1840, 5635: 1841, 5712: 1842, 6010: 1843, 6423: 1844, 6491: 1845, 6578: 1846, 6579: 1847, 6628: 1848, 6663: 1849, 6683: 1850, 7072: 1851, 7491: 1852, 7547: 1853, 7696: 1854, 7931: 1855, 8281: 1856, 8769: 1857, 8859: 1858, 9502: 1859, 9790: 1860, 9982: 1861, 11192: 1862, 11204: 1863, 11249: 1864, 11346: 1865, 12405: 1866, 12652: 1867, 15024: 1868, 15032: 1869, 15095: 1870, 17940: 1871, 18668: 1872, 19076: 1873, 19325: 1874, 23422: 1875, 24264: 1876, 26591: 1877, 105: 1878, 422: 1879, 471: 1880, 726: 1881, 843: 1882, 894: 1883, 1159: 1884, 1175: 1885, 1236: 1886, 1274: 1887, 1275: 1888, 1324: 1889, 1336: 1890, 1594: 1891, 1762: 1892, 1879: 1893, 1894: 1894, 1969: 1895, 1984: 1896, 2085: 1897, 2235: 1898, 2313: 1899, 2560: 1900, 2562: 1901, 2589: 1902, 2657: 1903, 2724: 1904, 2755: 1905, 2823: 1906, 2952: 1907, 2971: 1908, 2993: 1909, 3063: 1910, 3127: 1911, 3148: 1912, 3239: 1913, 3357: 1914, 3453: 1915, 3621: 1916, 3678: 1917, 3716: 1918, 3734: 1919, 3736: 1920, 3753: 1921, 3779: 1922, 3956: 1923, 3970: 1924, 3982: 1925, 4014: 1926, 4112: 1927, 4326: 1928, 4399: 1929, 4415: 1930, 4561: 1931, 4685: 1932, 4725: 1933, 4842: 1934, 4972: 1935, 5149: 1936, 5367: 1937, 5508: 1938, 6171: 1939, 6217: 1940, 6228: 1941, 6395: 1942, 6432: 1943, 6509: 1944, 6949: 1945, 7002: 1946, 7123: 1947, 7126: 1948, 7263: 1949, 7286: 1950, 7433: 1951, 7969: 1952, 7978: 1953, 8862: 1954, 9328: 1955, 9495: 1956, 9515: 1957, 9571: 1958, 9685: 1959, 9931: 1960, 10110: 1961, 12283: 1962, 13268: 1963, 14770: 1964, 18927: 1965, 80: 1966, 83: 1967, 199: 1968, 316: 1969, 700: 1970, 807: 1971, 1338: 1972, 1349: 1973, 1605: 1974, 1657: 1975, 1668: 1976, 1804: 1977, 1989: 1978, 2019: 1979, 2042: 1980, 2061: 1981, 2133: 1982, 2261: 1983, 2366: 1984, 2542: 1985, 2582: 1986, 2609: 1987, 2641: 1988, 2824: 1989, 2825: 1990, 2913: 1991, 3018: 1992, 3048: 1993, 3078: 1994, 3105: 1995, 3261: 1996, 3338: 1997, 3366: 1998, 3509: 1999, 3516: 2000, 3534: 2001, 3581: 2002, 3588: 2003, 3631: 2004, 3653: 2005, 3671: 2006, 3715: 2007, 3761: 2008, 3837: 2009, 3886: 2010, 3901: 2011, 3912: 2012, 3927: 2013, 4069: 2014, 4201: 2015, 4611: 2016, 4711: 2017, 4736: 2018, 4757: 2019, 5013: 2020, 5216: 2021, 5556: 2022, 5630: 2023, 5693: 2024, 5772: 2025, 5961: 2026, 5989: 2027, 6187: 2028, 6221: 2029, 6553: 2030, 6615: 2031, 6720: 2032, 7019: 2033, 7212: 2034, 7220: 2035, 7464: 2036, 7501: 2037, 7987: 2038, 8226: 2039, 8373: 2040, 8390: 2041, 10083: 2042, 10094: 2043, 10516: 2044, 10574: 2045, 10778: 2046, 10983: 2047, 11116: 2048, 11121: 2049, 11276: 2050, 12302: 2051, 12439: 2052, 12609: 2053, 12631: 2054, 14420: 2055, 14826: 2056, 15602: 2057, 15874: 2058, 15907: 2059, 15975: 2060, 24586: 2061, 133: 2062, 138: 2063, 151: 2064, 162: 2065, 344: 2066, 345: 2067, 366: 2068, 448: 2069, 480: 2070, 554: 2071, 665: 2072, 992: 2073, 1058: 2074, 1352: 2075, 1357: 2076, 1370: 2077, 1413: 2078, 1578: 2079, 1640: 2080, 1794: 2081, 1906: 2082, 1980: 2083, 2008: 2084, 2018: 2085, 2043: 2086, 2077: 2087, 2360: 2088, 2435: 2089, 2438: 2090, 2521: 2091, 2672: 2092, 2684: 2093, 2738: 2094, 2804: 2095, 2864: 2096, 3035: 2097, 3185: 2098, 3282: 2099, 3293: 2100, 3487: 2101, 3492: 2102, 3537: 2103, 3547: 2104, 3580: 2105, 3616: 2106, 3768: 2107, 3788: 2108, 3896: 2109, 4041: 2110, 4043: 2111, 4191: 2112, 4282: 2113, 4287: 2114, 4395: 2115, 4412: 2116, 4481: 2117, 4609: 2118, 4731: 2119, 4743: 2120, 4764: 2121, 4797: 2122, 4951: 2123, 4974: 2124, 5099: 2125, 5176: 2126, 5248: 2127, 5400: 2128, 5617: 2129, 5679: 2130, 5700: 2131, 5787: 2132, 5858: 2133, 5951: 2134, 5982: 2135, 6119: 2136, 6211: 2137, 6247: 2138, 6391: 2139, 6574: 2140, 6653: 2141, 6882: 2142, 6963: 2143, 7106: 2144, 7171: 2145, 7213: 2146, 7341: 2147, 7455: 2148, 7602: 2149, 7958: 2150, 8016: 2151, 8075: 2152, 8087: 2153, 8240: 2154, 9517: 2155, 10019: 2156, 10646: 2157, 11162: 2158, 13236: 2159, 14305: 2160, 15118: 2161, 15821: 2162, 18278: 2163, 18804: 2164, 19702: 2165, 23340: 2166, 24637: 2167, 25873: 2168, 26763: 2169, 392: 2170, 478: 2171, 736: 2172, 963: 2173, 1142: 2174, 1925: 2175, 1932: 2176, 2129: 2177, 2372: 2178, 2492: 2179, 2604: 2180, 2654: 2181, 2725: 2182, 2793: 2183, 2800: 2184, 2811: 2185, 2866: 2186, 2887: 2187, 2964: 2188, 3021: 2189, 3080: 2190, 3093: 2191, 3100: 2192, 3155: 2193, 3180: 2194, 3191: 2195, 3193: 2196, 3202: 2197, 3203: 2198, 3383: 2199, 3417: 2200, 3467: 2201, 3471: 2202, 3474: 2203, 3478: 2204, 3480: 2205, 3505: 2206, 3648: 2207, 3820: 2208, 3862: 2209, 3929: 2210, 4104: 2211, 4126: 2212, 4176: 2213, 4233: 2214, 4285: 2215, 4288: 2216, 4327: 2217, 4387: 2218, 4497: 2219, 4573: 2220, 4595: 2221, 4606: 2222, 4811: 2223, 4929: 2224, 5206: 2225, 5342: 2226, 5463: 2227, 5492: 2228, 5503: 2229, 5526: 2230, 5568: 2231, 5618: 2232, 5752: 2233, 5884: 2234, 5926: 2235, 6155: 2236, 6156: 2237, 6238: 2238, 6250: 2239, 6443: 2240, 6505: 2241, 6610: 2242, 6655: 2243, 6748: 2244, 6955: 2245, 7022: 2246, 7070: 2247, 7098: 2248, 7168: 2249, 7174: 2250, 7360: 2251, 7431: 2252, 7989: 2253, 8069: 2254, 8098: 2255, 9550: 2256, 9613: 2257, 9898: 2258, 9971: 2259, 10578: 2260, 10604: 2261, 10681: 2262, 10834: 2263, 11260: 2264, 11332: 2265, 12525: 2266, 12594: 2267, 12695: 2268, 13176: 2269, 13336: 2270, 13858: 2271, 14745: 2272, 15690: 2273, 18281: 2274, 19341: 2275, 19864: 2276, 20575: 2277, 20899: 2278, 22547: 2279, 23886: 2280, 24510: 2281, 225: 2282, 317: 2283, 337: 2284, 358: 2285, 462: 2286, 637: 2287, 1132: 2288, 1320: 2289, 1391: 2290, 1397: 2291, 1398: 2292, 1670: 2293, 1747: 2294, 1753: 2295, 1839: 2296, 1893: 2297, 1943: 2298, 2040: 2299, 2115: 2300, 2159: 2301, 2322: 2302, 2507: 2303, 2520: 2304, 2686: 2305, 2721: 2306, 2901: 2307, 3089: 2308, 3312: 2309, 3370: 2310, 3399: 2311, 3407: 2312, 3491: 2313, 3502: 2314, 3514: 2315, 3662: 2316, 3836: 2317, 3884: 2318, 3936: 2319, 3938: 2320, 3976: 2321, 4079: 2322, 4109: 2323, 4148: 2324, 4205: 2325, 4261: 2326, 4304: 2327, 4398: 2328, 4436: 2329, 4528: 2330, 4545: 2331, 4661: 2332, 4673: 2333, 4690: 2334, 4808: 2335, 4818: 2336, 4839: 2337, 4914: 2338, 4992: 2339, 5030: 2340, 5048: 2341, 5144: 2342, 5185: 2343, 5312: 2344, 5346: 2345, 5356: 2346, 5522: 2347, 5577: 2348, 5613: 2349, 5756: 2350, 5843: 2351, 5877: 2352, 5915: 2353, 6163: 2354, 6351: 2355, 6442: 2356, 6603: 2357, 6662: 2358, 7091: 2359, 7147: 2360, 7252: 2361, 7300: 2362, 7441: 2363, 7465: 2364, 7520: 2365, 7765: 2366, 7855: 2367, 7895: 2368, 7955: 2369, 7973: 2370, 8037: 2371, 8088: 2372, 8301: 2373, 8317: 2374, 8903: 2375, 9119: 2376, 9165: 2377, 9270: 2378, 9377: 2379, 9395: 2380, 9410: 2381, 9484: 2382, 9540: 2383, 9871: 2384, 10323: 2385, 10562: 2386, 10672: 2387, 10705: 2388, 10756: 2389, 10783: 2390, 10887: 2391, 12262: 2392, 12552: 2393, 12643: 2394, 12680: 2395, 12694: 2396, 12766: 2397, 12863: 2398, 13017: 2399, 13249: 2400, 14006: 2401, 14605: 2402, 15676: 2403, 18554: 2404, 19903: 2405, 20234: 2406, 22395: 2407, 24083: 2408, 166: 2409, 208: 2410, 306: 2411, 515: 2412, 542: 2413, 611: 2414, 629: 2415, 644: 2416, 705: 2417, 723: 2418, 798: 2419, 1162: 2420, 1171: 2421, 1215: 2422, 1231: 2423, 1257: 2424, 1447: 2425, 1530: 2426, 1720: 2427, 1844: 2428, 2015: 2429, 2039: 2430, 2336: 2431, 2352: 2432, 2363: 2433, 2444: 2434, 2454: 2435, 2485: 2436, 2548: 2437, 2600: 2438, 2602: 2439, 2683: 2440, 2873: 2441, 2886: 2442, 2943: 2443, 2982: 2444, 2990: 2445, 3015: 2446, 3092: 2447, 3151: 2448, 3187: 2449, 3251: 2450, 3468: 2451, 3476: 2452, 3545: 2453, 3620: 2454, 3661: 2455, 3804: 2456, 3845: 2457, 3911: 2458, 3950: 2459, 3964: 2460, 3995: 2461, 4002: 2462, 4015: 2463, 4036: 2464, 4090: 2465, 4155: 2466, 4169: 2467, 4455: 2468, 4457: 2469, 4498: 2470, 4637: 2471, 4644: 2472, 4706: 2473, 4723: 2474, 4730: 2475, 4775: 2476, 4860: 2477, 4924: 2478, 4963: 2479, 4991: 2480, 5135: 2481, 5256: 2482, 5347: 2483, 5374: 2484, 5510: 2485, 5545: 2486, 5789: 2487, 5868: 2488, 6104: 2489, 6232: 2490, 6361: 2491, 6388: 2492, 6441: 2493, 6463: 2494, 6510: 2495, 6790: 2496, 7097: 2497, 7193: 2498, 7205: 2499, 7331: 2500, 7406: 2501, 7598: 2502, 9001: 2503, 9144: 2504, 9716: 2505, 9807: 2506, 10174: 2507, 10647: 2508, 10739: 2509, 10856: 2510, 10899: 2511, 10936: 2512, 11163: 2513, 11376: 2514, 12230: 2515, 12606: 2516, 12844: 2517, 13259: 2518, 14362: 2519, 14865: 2520, 14972: 2521, 18209: 2522, 19566: 2523, 19983: 2524, 21032: 2525, 23874: 2526, 24532: 2527, 27976: 2528, 142: 2529, 186: 2530, 268: 2531, 334: 2532, 376: 2533, 423: 2534, 619: 2535, 760: 2536, 797: 2537, 932: 2538, 945: 2539, 1093: 2540, 1666: 2541, 2058: 2542, 2226: 2543, 2392: 2544, 2426: 2545, 2524: 2546, 2535: 2547, 2648: 2548, 2707: 2549, 2932: 2550, 2960: 2551, 2979: 2552, 2996: 2553, 3008: 2554, 3029: 2555, 3174: 2556, 3178: 2557, 3220: 2558, 3324: 2559, 3409: 2560, 3448: 2561, 3461: 2562, 3501: 2563, 3652: 2564, 3656: 2565, 3660: 2566, 3822: 2567, 3905: 2568, 3957: 2569, 3960: 2570, 3968: 2571, 4101: 2572, 4153: 2573, 4194: 2574, 4332: 2575, 4338: 2576, 4386: 2577, 4396: 2578, 4480: 2579, 4510: 2580, 4535: 2581, 4564: 2582, 4652: 2583, 4654: 2584, 4663: 2585, 4701: 2586, 4724: 2587, 4762: 2588, 4763: 2589, 4807: 2590, 4850: 2591, 4865: 2592, 4937: 2593, 4954: 2594, 4970: 2595, 4976: 2596, 4999: 2597, 5008: 2598, 5015: 2599, 5066: 2600, 5113: 2601, 5132: 2602, 5354: 2603, 5364: 2604, 5382: 2605, 5419: 2606, 5464: 2607, 5505: 2608, 5513: 2609, 5549: 2610, 5670: 2611, 5699: 2612, 5797: 2613, 5923: 2614, 5965: 2615, 5985: 2616, 6002: 2617, 6057: 2618, 6076: 2619, 6130: 2620, 6166: 2621, 6220: 2622, 6236: 2623, 6602: 2624, 6605: 2625, 6968: 2626, 6970: 2627, 7081: 2628, 7111: 2629, 7116: 2630, 7196: 2631, 7199: 2632, 7240: 2633, 7247: 2634, 7325: 2635, 7342: 2636, 7427: 2637, 7589: 2638, 7681: 2639, 7684: 2640, 8219: 2641, 8353: 2642, 8685: 2643, 9085: 2644, 9126: 2645, 9215: 2646, 9551: 2647, 9645: 2648, 9649: 2649, 9701: 2650, 9760: 2651, 10797: 2652, 11019: 2653, 11033: 2654, 11142: 2655, 11151: 2656, 11156: 2657, 11237: 2658, 11315: 2659, 11882: 2660, 12433: 2661, 12825: 2662, 14710: 2663, 15077: 2664, 15922: 2665, 19296: 2666, 20866: 2667, 21031: 2668, 23075: 2669, 23230: 2670, 24168: 2671, 25741: 2672, 27757: 2673, 67: 2674, 463: 2675, 796: 2676, 1216: 2677, 1249: 2678, 1250: 2679, 1396: 2680, 1510: 2681, 1541: 2682, 1598: 2683, 1622: 2684, 1897: 2685, 2003: 2686, 2092: 2687, 2304: 2688, 2377: 2689, 2390: 2690, 2440: 2691, 2491: 2692, 2497: 2693, 2613: 2694, 2666: 2695, 2726: 2696, 2743: 2697, 2789: 2698, 2808: 2699, 2834: 2700, 2839: 2701, 2843: 2702, 2854: 2703, 2879: 2704, 2896: 2705, 2925: 2706, 3003: 2707, 3044: 2708, 3067: 2709, 3138: 2710, 3164: 2711, 3263: 2712, 3266: 2713, 3388: 2714, 3520: 2715, 3698: 2716, 3859: 2717, 3863: 2718, 3971: 2719, 3986: 2720, 3998: 2721, 4110: 2722, 4132: 2723, 4230: 2724, 4259: 2725, 4306: 2726, 4316: 2727, 4345: 2728, 4428: 2729, 4565: 2730, 4729: 2731, 4796: 2732, 4849: 2733, 4864: 2734, 5019: 2735, 5038: 2736, 5062: 2737, 5094: 2738, 5116: 2739, 5214: 2740, 5251: 2741, 5257: 2742, 5340: 2743, 5421: 2744, 5424: 2745, 5447: 2746, 5504: 2747, 5557: 2748, 5650: 2749, 5735: 2750, 5816: 2751, 5864: 2752, 5979: 2753, 6075: 2754, 6122: 2755, 6135: 2756, 6153: 2757, 6184: 2758, 6229: 2759, 6316: 2760, 6393: 2761, 6473: 2762, 6474: 2763, 6877: 2764, 6934: 2765, 7031: 2766, 7117: 2767, 7170: 2768, 7288: 2769, 7497: 2770, 7537: 2771, 7731: 2772, 7756: 2773, 7818: 2774, 7827: 2775, 7856: 2776, 7932: 2777, 7951: 2778, 8199: 2779, 8393: 2780, 9003: 2781, 9057: 2782, 9403: 2783, 9626: 2784, 9917: 2785, 10436: 2786, 10858: 2787, 11030: 2788, 11071: 2789, 11203: 2790, 11232: 2791, 11344: 2792, 11392: 2793, 11469: 2794, 12229: 2795, 12301: 2796, 12800: 2797, 12838: 2798, 13092: 2799, 13248: 2800, 14399: 2801, 14915: 2802, 14942: 2803, 15033: 2804, 15168: 2805, 15700: 2806, 18324: 2807, 19267: 2808, 19491: 2809, 19549: 2810, 19568: 2811, 19695: 2812, 19772: 2813, 20660: 2814, 22350: 2815, 22957: 2816, 23304: 2817, 25658: 2818, 26543: 2819, 26573: 2820, 26813: 2821, 28927: 2822, 168: 2823, 403: 2824, 411: 2825, 481: 2826, 941: 2827, 1213: 2828, 1234: 2829, 1273: 2830, 1285: 2831, 1412: 2832, 1588: 2833, 1599: 2834, 1647: 2835, 1696: 2836, 1778: 2837, 1786: 2838, 1920: 2839, 1963: 2840, 1985: 2841, 1990: 2842, 2025: 2843, 2049: 2844, 2199: 2845, 2230: 2846, 2655: 2847, 2656: 2848, 2701: 2849, 2708: 2850, 2767: 2851, 2771: 2852, 2773: 2853, 2946: 2854, 2981: 2855, 3061: 2856, 3077: 2857, 3084: 2858, 3252: 2859, 3299: 2860, 3330: 2861, 3436: 2862, 3479: 2863, 3508: 2864, 3521: 2865, 3586: 2866, 3603: 2867, 3610: 2868, 3622: 2869, 3645: 2870, 3739: 2871, 3769: 2872, 3991: 2873, 3999: 2874, 4073: 2875, 4133: 2876, 4224: 2877, 4236: 2878, 4276: 2879, 4280: 2880, 4465: 2881, 4512: 2882, 4525: 2883, 4629: 2884, 4721: 2885, 4744: 2886, 4889: 2887, 4981: 2888, 5042: 2889, 5044: 2890, 5053: 2891, 5159: 2892, 5246: 2893, 5349: 2894, 5485: 2895, 5535: 2896, 5559: 2897, 5708: 2898, 5777: 2899, 5786: 2900, 5820: 2901, 5919: 2902, 5930: 2903, 5963: 2904, 5999: 2905, 6006: 2906, 6027: 2907, 6078: 2908, 6142: 2909, 6147: 2910, 6170: 2911, 6178: 2912, 6192: 2913, 6205: 2914, 6306: 2915, 6383: 2916, 6404: 2917, 6538: 2918, 6565: 2919, 6569: 2920, 6623: 2921, 6743: 2922, 6749: 2923, 6759: 2924, 6890: 2925, 6913: 2926, 6950: 2927, 7146: 2928, 7210: 2929, 7224: 2930, 7336: 2931, 7382: 2932, 7418: 2933, 7505: 2934, 7766: 2935, 8053: 2936, 8066: 2937, 8074: 2938, 8086: 2939, 8136: 2940, 8170: 2941, 8321: 2942, 8872: 2943, 9204: 2944, 9257: 2945, 9363: 2946, 9454: 2947, 9575: 2948, 9695: 2949, 9780: 2950, 9869: 2951, 9979: 2952, 10067: 2953, 10135: 2954, 10307: 2955, 10359: 2956, 10485: 2957, 10558: 2958, 10612: 2959, 10787: 2960, 11313: 2961, 11409: 2962, 11547: 2963, 11879: 2964, 11952: 2965, 12172: 2966, 12201: 2967, 12319: 2968, 12509: 2969, 12559: 2970, 12672: 2971, 12698: 2972, 12736: 2973, 13005: 2974, 13154: 2975, 13184: 2976, 13918: 2977, 14273: 2978, 14335: 2979, 14615: 2980, 14646: 2981, 14734: 2982, 14896: 2983, 15079: 2984, 15743: 2985, 15852: 2986, 17694: 2987, 17809: 2988, 19103: 2989, 19186: 2990, 19307: 2991, 19924: 2992, 19982: 2993, 20074: 2994, 20844: 2995, 22246: 2996, 22404: 2997, 22618: 2998, 22857: 2999, 23286: 3000, 23702: 3001, 23708: 3002, 23768: 3003, 23890: 3004, 24720: 3005, 24723: 3006, 24821: 3007, 26679: 3008, 26772: 3009, 205: 3010, 348: 3011, 451: 3012, 512: 3013, 673: 3014, 777: 3015, 825: 3016, 1075: 3017, 1149: 3018, 1262: 3019, 1416: 3020, 1468: 3021, 1528: 3022, 1803: 3023, 1885: 3024, 1947: 3025, 1977: 3026, 1986: 3027, 2009: 3028, 2029: 3029, 2136: 3030, 2318: 3031, 2321: 3032, 2523: 3033, 2590: 3034, 2621: 3035, 2631: 3036, 2891: 3037, 2944: 3038, 2976: 3039, 2986: 3040, 3010: 3041, 3020: 3042, 3107: 3043, 3158: 3044, 3200: 3045, 3254: 3046, 3279: 3047, 3408: 3048, 3577: 3049, 3668: 3050, 3797: 3051, 3816: 3052, 3851: 3053, 3920: 3054, 3942: 3055, 3989: 3056, 4031: 3057, 4085: 3058, 4087: 3059, 4150: 3060, 4156: 3061, 4158: 3062, 4212: 3063, 4271: 3064, 4274: 3065, 4283: 3066, 4388: 3067, 4404: 3068, 4473: 3069, 4509: 3070, 4531: 3071, 4544: 3072, 4558: 3073, 4586: 3074, 4597: 3075, 4602: 3076, 4614: 3077, 4656: 3078, 4670: 3079, 4717: 3080, 4759: 3081, 4788: 3082, 4816: 3083, 4832: 3084, 4902: 3085, 4943: 3086, 4948: 3087, 4961: 3088, 4989: 3089, 5027: 3090, 5096: 3091, 5122: 3092, 5171: 3093, 5186: 3094, 5197: 3095, 5213: 3096, 5286: 3097, 5298: 3098, 5316: 3099, 5328: 3100, 5392: 3101, 5438: 3102, 5621: 3103, 5666: 3104, 5728: 3105, 5837: 3106, 5839: 3107, 5882: 3108, 5949: 3109, 5962: 3110, 6038: 3111, 6064: 3112, 6077: 3113, 6089: 3114, 6099: 3115, 6124: 3116, 6127: 3117, 6180: 3118, 6216: 3119, 6253: 3120, 6254: 3121, 6286: 3122, 6294: 3123, 6387: 3124, 6409: 3125, 6424: 3126, 6446: 3127, 6448: 3128, 6476: 3129, 6590: 3130, 6618: 3131, 6698: 3132, 6753: 3133, 6754: 3134, 6756: 3135, 6800: 3136, 6803: 3137, 6860: 3138, 6868: 3139, 6905: 3140, 7004: 3141, 7113: 3142, 7190: 3143, 7218: 3144, 7310: 3145, 7349: 3146, 7394: 3147, 7420: 3148, 7500: 3149, 7502: 3150, 7618: 3151, 7691: 3152, 7865: 3153, 7877: 3154, 8038: 3155, 8114: 3156, 8214: 3157, 8268: 3158, 8329: 3159, 8348: 3160, 8387: 3161, 8631: 3162, 8894: 3163, 9042: 3164, 9145: 3165, 9214: 3166, 9333: 3167, 9399: 3168, 9435: 3169, 9497: 3170, 9604: 3171, 9635: 3172, 9773: 3173, 9783: 3174, 9824: 3175, 9833: 3176, 9862: 3177, 9915: 3178, 10090: 3179, 10100: 3180, 10118: 3181, 10431: 3182, 10860: 3183, 10869: 3184, 10872: 3185, 11031: 3186, 11077: 3187, 11428: 3188, 11762: 3189, 12044: 3190, 12161: 3191, 12286: 3192, 12371: 3193, 12476: 3194, 12520: 3195, 12753: 3196, 12773: 3197, 12799: 3198, 13272: 3199, 14754: 3200, 14868: 3201, 15078: 3202, 15450: 3203, 15474: 3204, 15596: 3205, 15671: 3206, 15694: 3207, 18700: 3208, 18701: 3209, 18955: 3210, 19567: 3211, 19827: 3212, 19847: 3213, 20618: 3214, 20629: 3215, 21145: 3216, 24075: 3217, 24309: 3218, 24359: 3219, 25657: 3220, 26438: 3221, 26466: 3222, 333: 3223, 424: 3224, 468: 3225, 762: 3226, 766: 3227, 801: 3228, 822: 3229, 835: 3230, 1127: 3231, 1472: 3232, 1559: 3233, 1654: 3234, 1707: 3235, 1724: 3236, 2031: 3237, 2034: 3238, 2186: 3239, 2233: 3240, 2283: 3241, 2306: 3242, 2384: 3243, 2421: 3244, 2477: 3245, 2487: 3246, 2580: 3247, 2583: 3248, 2736: 3249, 2739: 3250, 2919: 3251, 2922: 3252, 2970: 3253, 3076: 3254, 3240: 3255, 3246: 3256, 3273: 3257, 3297: 3258, 3307: 3259, 3630: 3260, 3718: 3261, 3777: 3262, 3781: 3263, 3915: 3264, 4037: 3265, 4168: 3266, 4229: 3267, 4552: 3268, 4556: 3269, 4591: 3270, 4612: 3271, 4645: 3272, 4707: 3273, 4782: 3274, 4879: 3275, 4898: 3276, 4909: 3277, 4915: 3278, 5047: 3279, 5078: 3280, 5097: 3281, 5130: 3282, 5166: 3283, 5233: 3284, 5271: 3285, 5282: 3286, 5287: 3287, 5307: 3288, 5308: 3289, 5360: 3290, 5416: 3291, 5495: 3292, 5507: 3293, 5531: 3294, 5572: 3295, 5582: 3296, 5587: 3297, 5667: 3298, 5672: 3299, 5683: 3300, 5721: 3301, 5742: 3302, 5761: 3303, 5836: 3304, 5984: 3305, 6023: 3306, 6088: 3307, 6209: 3308, 6245: 3309, 6267: 3310, 6334: 3311, 6454: 3312, 6467: 3313, 6469: 3314, 6588: 3315, 6631: 3316, 6688: 3317, 6711: 3318, 6725: 3319, 6798: 3320, 6862: 3321, 6946: 3322, 6972: 3323, 7011: 3324, 7017: 3325, 7030: 3326, 7156: 3327, 7239: 3328, 7254: 3329, 7313: 3330, 7324: 3331, 7365: 3332, 7389: 3333, 7416: 3334, 7459: 3335, 7487: 3336, 7632: 3337, 7704: 3338, 7707: 3339, 7718: 3340, 7767: 3341, 7793: 3342, 7916: 3343, 7929: 3344, 7947: 3345, 8029: 3346, 8050: 3347, 8145: 3348, 8172: 3349, 8184: 3350, 8222: 3351, 8255: 3352, 8372: 3353, 8433: 3354, 8599: 3355, 8662: 3356, 8827: 3357, 8857: 3358, 8907: 3359, 9108: 3360, 9176: 3361, 9186: 3362, 9202: 3363, 9226: 3364, 9332: 3365, 9433: 3366, 9494: 3367, 9526: 3368, 9702: 3369, 9704: 3370, 9722: 3371, 9744: 3372, 9766: 3373, 9949: 3374, 10050: 3375, 10097: 3376, 10115: 3377, 10363: 3378, 10507: 3379, 10648: 3380, 10733: 3381, 10801: 3382, 10922: 3383, 10941: 3384, 11112: 3385, 11261: 3386, 11271: 3387, 11371: 3388, 11378: 3389, 11465: 3390, 12290: 3391, 12346: 3392, 12543: 3393, 12593: 3394, 12662: 3395, 12682: 3396, 12743: 3397, 12843: 3398, 12887: 3399, 12929: 3400, 13014: 3401, 13021: 3402, 13107: 3403, 13297: 3404, 14541: 3405, 14606: 3406, 14805: 3407, 14842: 3408, 14866: 3409, 15074: 3410, 15140: 3411, 15299: 3412, 15401: 3413, 15403: 3414, 15438: 3415, 15471: 3416, 15490: 3417, 15623: 3418, 15631: 3419, 15807: 3420, 15842: 3421, 17953: 3422, 18235: 3423, 18911: 3424, 18954: 3425, 19246: 3426, 19556: 3427, 19730: 3428, 20145: 3429, 20668: 3430, 21918: 3431, 22826: 3432, 23968: 3433, 24301: 3434, 24666: 3435, 24698: 3436, 26362: 3437, 26624: 3438, 28298: 3439, 28884: 3440, 28949: 3441, 73: 3442, 131: 3443, 243: 3444, 271: 3445, 343: 3446, 420: 3447, 533: 3448, 562: 3449, 614: 3450, 863: 3451, 882: 3452, 923: 3453, 951: 3454, 1028: 3455, 1068: 3456, 1183: 3457, 1184: 3458, 1209: 3459, 1383: 3460, 1475: 3461, 1524: 3462, 1548: 3463, 1592: 3464, 1643: 3465, 1710: 3466, 1761: 3467, 1938: 3468, 2024: 3469, 2076: 3470, 2279: 3471, 2309: 3472, 2488: 3473, 2522: 3474, 2752: 3475, 2754: 3476, 2871: 3477, 2890: 3478, 2933: 3479, 2988: 3480, 3055: 3481, 3086: 3482, 3102: 3483, 3232: 3484, 3286: 3485, 3290: 3486, 3300: 3487, 3344: 3488, 3360: 3489, 3405: 3490, 3510: 3491, 3539: 3492, 3639: 3493, 3675: 3494, 3719: 3495, 3796: 3496, 3814: 3497, 3877: 3498, 3890: 3499, 3910: 3500, 4004: 3501, 4057: 3502, 4096: 3503, 4198: 3504, 4200: 3505, 4242: 3506, 4256: 3507, 4264: 3508, 4296: 3509, 4423: 3510, 4476: 3511, 4477: 3512, 4490: 3513, 4523: 3514, 4524: 3515, 4605: 3516, 4648: 3517, 4660: 3518, 4761: 3519, 4820: 3520, 4830: 3521, 4838: 3522, 4900: 3523, 5114: 3524, 5141: 3525, 5157: 3526, 5193: 3527, 5420: 3528, 5443: 3529, 5509: 3530, 5546: 3531, 5562: 3532, 5638: 3533, 5710: 3534, 5801: 3535, 5808: 3536, 5819: 3537, 5822: 3538, 5890: 3539, 5899: 3540, 5948: 3541, 6005: 3542, 6060: 3543, 6095: 3544, 6112: 3545, 6145: 3546, 6194: 3547, 6199: 3548, 6268: 3549, 6303: 3550, 6344: 3551, 6356: 3552, 6504: 3553, 6511: 3554, 6513: 3555, 6522: 3556, 6597: 3557, 6601: 3558, 6614: 3559, 6621: 3560, 6691: 3561, 6853: 3562, 6859: 3563, 6908: 3564, 6975: 3565, 7066: 3566, 7108: 3567, 7214: 3568, 7281: 3569, 7323: 3570, 7348: 3571, 7371: 3572, 7412: 3573, 7414: 3574, 7488: 3575, 7519: 3576, 7666: 3577, 7677: 3578, 7694: 3579, 7785: 3580, 7847: 3581, 7852: 3582, 7872: 3583, 7921: 3584, 7960: 3585, 7964: 3586, 8008: 3587, 8119: 3588, 8178: 3589, 8392: 3590, 8553: 3591, 8722: 3592, 8752: 3593, 8920: 3594, 8961: 3595, 9122: 3596, 9163: 3597, 9292: 3598, 9324: 3599, 9331: 3600, 9340: 3601, 9380: 3602, 9413: 3603, 9439: 3604, 9504: 3605, 9543: 3606, 9624: 3607, 9663: 3608, 9745: 3609, 9803: 3610, 9880: 3611, 9892: 3612, 9910: 3613, 9957: 3614, 9981: 3615, 10001: 3616, 10011: 3617, 10058: 3618, 10091: 3619, 10117: 3620, 10285: 3621, 10458: 3622, 10576: 3623, 10670: 3624, 10725: 3625, 10734: 3626, 10852: 3627, 10943: 3628, 11041: 3629, 11111: 3630, 11153: 3631, 11219: 3632, 11268: 3633, 11274: 3634, 11285: 3635, 11291: 3636, 11300: 3637, 11377: 3638, 11382: 3639, 11433: 3640, 11459: 3641, 11568: 3642, 11749: 3643, 11899: 3644, 11975: 3645, 11995: 3646, 12025: 3647, 12245: 3648, 12303: 3649, 12434: 3650, 12442: 3651, 12507: 3652, 12563: 3653, 12586: 3654, 12587: 3655, 12696: 3656, 12757: 3657, 12793: 3658, 12797: 3659, 12944: 3660, 13007: 3661, 13054: 3662, 13103: 3663, 13211: 3664, 13391: 3665, 13891: 3666, 14175: 3667, 14525: 3668, 14748: 3669, 14766: 3670, 14813: 3671, 14907: 3672, 14998: 3673, 15061: 3674, 15117: 3675, 15158: 3676, 15180: 3677, 15247: 3678, 15345: 3679, 15348: 3680, 15571: 3681, 15577: 3682, 15613: 3683, 15858: 3684, 15959: 3685, 17382: 3686, 17524: 3687, 17789: 3688, 17791: 3689, 18018: 3690, 18115: 3691, 18212: 3692, 18233: 3693, 18531: 3694, 18542: 3695, 18978: 3696, 18987: 3697, 19109: 3698, 19570: 3699, 19734: 3700, 19911: 3701, 19922: 3702, 20333: 3703, 20387: 3704, 20529: 3705, 22376: 3706, 22485: 3707, 23112: 3708, 23119: 3709, 23363: 3710, 23585: 3711, 23672: 3712, 24101: 3713, 24325: 3714, 24785: 3715, 26008: 3716, 26314: 3717, 26378: 3718, 26458: 3719, 27902: 3720, 28965: 3721, 119: 3722, 441: 3723, 543: 3724, 573: 3725, 576: 3726, 581: 3727, 733: 3728, 750: 3729, 831: 3730, 877: 3731, 890: 3732, 897: 3733, 1036: 3734, 1154: 3735, 1459: 3736, 1607: 3737, 1667: 3738, 1717: 3739, 1792: 3740, 1884: 3741, 1917: 3742, 1940: 3743, 2128: 3744, 2257: 3745, 2374: 3746, 2412: 3747, 2423: 3748, 2455: 3749, 2486: 3750, 2525: 3751, 2630: 3752, 2790: 3753, 2820: 3754, 2904: 3755, 2931: 3756, 3042: 3757, 3331: 3758, 3339: 3759, 3355: 3760, 3369: 3761, 3496: 3762, 3543: 3763, 3544: 3764, 3717: 3765, 3843: 3766, 3854: 3767, 3869: 3768, 3885: 3769, 3931: 3770, 3935: 3771, 4094: 3772, 4123: 3773, 4180: 3774, 4272: 3775, 4278: 3776, 4363: 3777, 4379: 3778, 4488: 3779, 4578: 3780, 4649: 3781, 4718: 3782, 4815: 3783, 5012: 3784, 5035: 3785, 5051: 3786, 5054: 3787, 5063: 3788, 5083: 3789, 5110: 3790, 5118: 3791, 5147: 3792, 5163: 3793, 5245: 3794, 5255: 3795, 5388: 3796, 5395: 3797, 5469: 3798, 5506: 3799, 5511: 3800, 5550: 3801, 5551: 3802, 5791: 3803, 5847: 3804, 5895: 3805, 5916: 3806, 5922: 3807, 6033: 3808, 6129: 3809, 6136: 3810, 6183: 3811, 6186: 3812, 6271: 3813, 6292: 3814, 6295: 3815, 6296: 3816, 6304: 3817, 6321: 3818, 6349: 3819, 6360: 3820, 6377: 3821, 6428: 3822, 6433: 3823, 6447: 3824, 6495: 3825, 6523: 3826, 6573: 3827, 6639: 3828, 6642: 3829, 6670: 3830, 6737: 3831, 6755: 3832, 6816: 3833, 6836: 3834, 6880: 3835, 6898: 3836, 6907: 3837, 7025: 3838, 7028: 3839, 7148: 3840, 7198: 3841, 7289: 3842, 7305: 3843, 7373: 3844, 7453: 3845, 7473: 3846, 7492: 3847, 7521: 3848, 7551: 3849, 7608: 3850, 7610: 3851, 7628: 3852, 7658: 3853, 7662: 3854, 7667: 3855, 7739: 3856, 7741: 3857, 7774: 3858, 7778: 3859, 7882: 3860, 7886: 3861, 7896: 3862, 7897: 3863, 8014: 3864, 8018: 3865, 8034: 3866, 8080: 3867, 8107: 3868, 8112: 3869, 8182: 3870, 8188: 3871, 8204: 3872, 8277: 3873, 8304: 3874, 8306: 3875, 8479: 3876, 8674: 3877, 8679: 3878, 8691: 3879, 8695: 3880, 8718: 3881, 8728: 3882, 8730: 3883, 8748: 3884, 8785: 3885, 8797: 3886, 8820: 3887, 8936: 3888, 8939: 3889, 9007: 3890, 9021: 3891, 9029: 3892, 9032: 3893, 9096: 3894, 9129: 3895, 9151: 3896, 9184: 3897, 9195: 3898, 9209: 3899, 9300: 3900, 9304: 3901, 9384: 3902, 9396: 3903, 9448: 3904, 9459: 3905, 9466: 3906, 9496: 3907, 9510: 3908, 9518: 3909, 9527: 3910, 9541: 3911, 9581: 3912, 9596: 3913, 9638: 3914, 9657: 3915, 9693: 3916, 9736: 3917, 9798: 3918, 9811: 3919, 10209: 3920, 10230: 3921, 10240: 3922, 10296: 3923, 10301: 3924, 10326: 3925, 10372: 3926, 10375: 3927, 10406: 3928, 10434: 3929, 10515: 3930, 10546: 3931, 10638: 3932, 10660: 3933, 10822: 3934, 10849: 3935, 10898: 3936, 10961: 3937, 11008: 3938, 11068: 3939, 11113: 3940, 11185: 3941, 11245: 3942, 11326: 3943, 11334: 3944, 11340: 3945, 11421: 3946, 11739: 3947, 12017: 3948, 12080: 3949, 12132: 3950, 12144: 3951, 12243: 3952, 12285: 3953, 12342: 3954, 12349: 3955, 12382: 3956, 12408: 3957, 12409: 3958, 12430: 3959, 12491: 3960, 12510: 3961, 12620: 3962, 12653: 3963, 12656: 3964, 12681: 3965, 12691: 3966, 12771: 3967, 12796: 3968, 12836: 3969, 12893: 3970, 12916: 3971, 12933: 3972, 12976: 3973, 13020: 3974, 13030: 3975, 13074: 3976, 13159: 3977, 13213: 3978, 13220: 3979, 13234: 3980, 13246: 3981, 13291: 3982, 13337: 3983, 14429: 3984, 14544: 3985, 14609: 3986, 14614: 3987, 14662: 3988, 14743: 3989, 14820: 3990, 14835: 3991, 14926: 3992, 14960: 3993, 14981: 3994, 15206: 3995, 15342: 3996, 15388: 3997, 15433: 3998, 15489: 3999, 15727: 4000, 15737: 4001, 15829: 4002, 15833: 4003, 15861: 4004, 15928: 4005, 15957: 4006, 17462: 4007, 17795: 4008, 17975: 4009, 18095: 4010, 18206: 4011, 18516: 4012, 18719: 4013, 18970: 4014, 19079: 4015, 19083: 4016, 19124: 4017, 19203: 4018, 19837: 4019, 20116: 4020, 20117: 4021, 20335: 4022, 20435: 4023, 20438: 4024, 20475: 4025, 20778: 4026, 20816: 4027, 20861: 4028, 20864: 4029, 20889: 4030, 22006: 4031, 22040: 4032, 22242: 4033, 22253: 4034, 22589: 4035, 22951: 4036, 22974: 4037, 22976: 4038, 23100: 4039, 23139: 4040, 23303: 4041, 23497: 4042, 23511: 4043, 23722: 4044, 23812: 4045, 24003: 4046, 24229: 4047, 24253: 4048, 24432: 4049, 24463: 4050, 24827: 4051, 25387: 4052, 26270: 4053, 26370: 4054, 26431: 4055, 26807: 4056, 26849: 4057, 27764: 4058, 28446: 4059, 28607: 4060, 28626: 4061, 28834: 4062, 30544: 4063, 96: 4064, 179: 4065, 261: 4066, 300: 4067, 326: 4068, 429: 4069, 458: 4070, 473: 4071, 590: 4072, 602: 4073, 630: 4074, 666: 4075, 775: 4076, 853: 4077, 878: 4078, 974: 4079, 1181: 4080, 1199: 4081, 1214: 4082, 1470: 4083, 1493: 4084, 1564: 4085, 1566: 4086, 1840: 4087, 1971: 4088, 2158: 4089, 2308: 4090, 2447: 4091, 2506: 4092, 2674: 4093, 2740: 4094, 2792: 4095, 2846: 4096, 2857: 4097, 2889: 4098, 2892: 4099, 2958: 4100, 3022: 4101, 3153: 4102, 3177: 4103, 3334: 4104, 3595: 4105, 3699: 4106, 3704: 4107, 3711: 4108, 3878: 4109, 3881: 4110, 3895: 4111, 3961: 4112, 3975: 4113, 4068: 4114, 4114: 4115, 4317: 4116, 4626: 4117, 4712: 4118, 4714: 4119, 4745: 4120, 4773: 4121, 4854: 4122, 4877: 4123, 4895: 4124, 4901: 4125, 4907: 4126, 4932: 4127, 4962: 4128, 5061: 4129, 5146: 4130, 5182: 4131, 5225: 4132, 5351: 4133, 5383: 4134, 5436: 4135, 5441: 4136, 5480: 4137, 5484: 4138, 5579: 4139, 5622: 4140, 5661: 4141, 5714: 4142, 5732: 4143, 5736: 4144, 5765: 4145, 5806: 4146, 5866: 4147, 5974: 4148, 5996: 4149, 6018: 4150, 6055: 4151, 6058: 4152, 6081: 4153, 6139: 4154, 6165: 4155, 6202: 4156, 6208: 4157, 6272: 4158, 6277: 4159, 6310: 4160, 6365: 4161, 6398: 4162, 6403: 4163, 6419: 4164, 6440: 4165, 6456: 4166, 6477: 4167, 6506: 4168, 6555: 4169, 6636: 4170, 6660: 4171, 6667: 4172, 6696: 4173, 6712: 4174, 6721: 4175, 6805: 4176, 6998: 4177, 7037: 4178, 7085: 4179, 7104: 4180, 7211: 4181, 7223: 4182, 7260: 4183, 7272: 4184, 7303: 4185, 7308: 4186, 7318: 4187, 7328: 4188, 7374: 4189, 7378: 4190, 7385: 4191, 7563: 4192, 7571: 4193, 7624: 4194, 7641: 4195, 7762: 4196, 7786: 4197, 7810: 4198, 7866: 4199, 7868: 4200, 7980: 4201, 8004: 4202, 8022: 4203, 8036: 4204, 8061: 4205, 8109: 4206, 8129: 4207, 8140: 4208, 8152: 4209, 8338: 4210, 8370: 4211, 8397: 4212, 8423: 4213, 8435: 4214, 8464: 4215, 8484: 4216, 8515: 4217, 8580: 4218, 8584: 4219, 8625: 4220, 8640: 4221, 8648: 4222, 8734: 4223, 8835: 4224, 8847: 4225, 8865: 4226, 8888: 4227, 8897: 4228, 8925: 4229, 8995: 4230, 9043: 4231, 9069: 4232, 9077: 4233, 9141: 4234, 9181: 4235, 9208: 4236, 9289: 4237, 9313: 4238, 9394: 4239, 9485: 4240, 9499: 4241, 9507: 4242, 9508: 4243, 9548: 4244, 9593: 4245, 9611: 4246, 9683: 4247, 9746: 4248, 9813: 4249, 9849: 4250, 9851: 4251, 9992: 4252, 10072: 4253, 10156: 4254, 10210: 4255, 10227: 4256, 10264: 4257, 10267: 4258, 10295: 4259, 10332: 4260, 10403: 4261, 10409: 4262, 10428: 4263, 10430: 4264, 10452: 4265, 10455: 4266, 10506: 4267, 10511: 4268, 10517: 4269, 10563: 4270, 10652: 4271, 10732: 4272, 10750: 4273, 10773: 4274, 10784: 4275, 10843: 4276, 10863: 4277, 10927: 4278, 10954: 4279, 10959: 4280, 11002: 4281, 11025: 4282, 11069: 4283, 11092: 4284, 11137: 4285, 11139: 4286, 11172: 4287, 11220: 4288, 11248: 4289, 11259: 4290, 11322: 4291, 11423: 4292, 11600: 4293, 11673: 4294, 11773: 4295, 11824: 4296, 11871: 4297, 11902: 4298, 11966: 4299, 12027: 4300, 12071: 4301, 12087: 4302, 12109: 4303, 12110: 4304, 12147: 4305, 12310: 4306, 12321: 4307, 12602: 4308, 12626: 4309, 12709: 4310, 12713: 4311, 12720: 4312, 12749: 4313, 12778: 4314, 12795: 4315, 12870: 4316, 12911: 4317, 12924: 4318, 13129: 4319, 13324: 4320, 13413: 4321, 13527: 4322, 13602: 4323, 13615: 4324, 13799: 4325, 14050: 4326, 14174: 4327, 14351: 4328, 14366: 4329, 14421: 4330, 14479: 4331, 14485: 4332, 14576: 4333, 14643: 4334, 14680: 4335, 14681: 4336, 14701: 4337, 14725: 4338, 14778: 4339, 14903: 4340, 14947: 4341, 15059: 4342, 15076: 4343, 15094: 4344, 15152: 4345, 15203: 4346, 15275: 4347, 15306: 4348, 15310: 4349, 15416: 4350, 15440: 4351, 15493: 4352, 15607: 4353, 15611: 4354, 15620: 4355, 15643: 4356, 15670: 4357, 15695: 4358, 15781: 4359, 15843: 4360, 15870: 4361, 15936: 4362, 17137: 4363, 17520: 4364, 17904: 4365, 17992: 4366, 18162: 4367, 18295: 4368, 18661: 4369, 18704: 4370, 18727: 4371, 18744: 4372, 18802: 4373, 18974: 4374, 19189: 4375, 19245: 4376, 19278: 4377, 19423: 4378, 19544: 4379, 19598: 4380, 19614: 4381, 19867: 4382, 19891: 4383, 19987: 4384, 20034: 4385, 20046: 4386, 20082: 4387, 20132: 4388, 20310: 4389, 20599: 4390, 20623: 4391, 20742: 4392, 20794: 4393, 21693: 4394, 22029: 4395, 22050: 4396, 22053: 4397, 22145: 4398, 22187: 4399, 22681: 4400, 22782: 4401, 22937: 4402, 23050: 4403, 23208: 4404, 23275: 4405, 23429: 4406, 23652: 4407, 23804: 4408, 23910: 4409, 23999: 4410, 24641: 4411, 24669: 4412, 24736: 4413, 24747: 4414, 25542: 4415, 25558: 4416, 25633: 4417, 25692: 4418, 25807: 4419, 25939: 4420, 26136: 4421, 26242: 4422, 26396: 4423, 26560: 4424, 26599: 4425, 27509: 4426, 27511: 4427, 28082: 4428, 29062: 4429, 106: 4430, 147: 4431, 178: 4432, 373: 4433, 386: 4434, 406: 4435, 415: 4436, 433: 4437, 447: 4438, 479: 4439, 489: 4440, 503: 4441, 555: 4442, 580: 4443, 740: 4444, 980: 4445, 1038: 4446, 1100: 4447, 1229: 4448, 1404: 4449, 1417: 4450, 1721: 4451, 1785: 4452, 1866: 4453, 1867: 4454, 2002: 4455, 2087: 4456, 2142: 4457, 2183: 4458, 2294: 4459, 2519: 4460, 2802: 4461, 2967: 4462, 3037: 4463, 3040: 4464, 3045: 4465, 3115: 4466, 3141: 4467, 3168: 4468, 3243: 4469, 3499: 4470, 3591: 4471, 3722: 4472, 3738: 4473, 3819: 4474, 3838: 4475, 4102: 4476, 4221: 4477, 4234: 4478, 4262: 4479, 4385: 4480, 4640: 4481, 4735: 4482, 4772: 4483, 4806: 4484, 4866: 4485, 4934: 4486, 4973: 4487, 4978: 4488, 5087: 4489, 5445: 4490, 5580: 4491, 5596: 4492, 5600: 4493, 5682: 4494, 5688: 4495, 5838: 4496, 5879: 4497, 5904: 4498, 5913: 4499, 6011: 4500, 6014: 4501, 6035: 4502, 6054: 4503, 6102: 4504, 6269: 4505, 6285: 4506, 6378: 4507, 6399: 4508, 6411: 4509, 6471: 4510, 6530: 4511, 6532: 4512, 6685: 4513, 6733: 4514, 6763: 4515, 6830: 4516, 6861: 4517, 6919: 4518, 6977: 4519, 7026: 4520, 7041: 4521, 7058: 4522, 7127: 4523, 7309: 4524, 7333: 4525, 7372: 4526, 7379: 4527, 7381: 4528, 7407: 4529, 7429: 4530, 7516: 4531, 7531: 4532, 7540: 4533, 7586: 4534, 7599: 4535, 7609: 4536, 7622: 4537, 7639: 4538, 7668: 4539, 7728: 4540, 7769: 4541, 7782: 4542, 7791: 4543, 7798: 4544, 7809: 4545, 7821: 4546, 7824: 4547, 7876: 4548, 7881: 4549, 7977: 4550, 8017: 4551, 8019: 4552, 8056: 4553, 8073: 4554, 8093: 4555, 8100: 4556, 8103: 4557, 8146: 4558, 8216: 4559, 8285: 4560, 8404: 4561, 8563: 4562, 8590: 4563, 8620: 4564, 8632: 4565, 8637: 4566, 8655: 4567, 8706: 4568, 8765: 4569, 8814: 4570, 8826: 4571, 8866: 4572, 8970: 4573, 8978: 4574, 8994: 4575, 9061: 4576, 9290: 4577, 9305: 4578, 9428: 4579, 9447: 4580, 9506: 4581, 9563: 4582, 9567: 4583, 9659: 4584, 9694: 4585, 9752: 4586, 9762: 4587, 9785: 4588, 9912: 4589, 9935: 4590, 9972: 4591, 10093: 4592, 10098: 4593, 10134: 4594, 10253: 4595, 10352: 4596, 10362: 4597, 10382: 4598, 10425: 4599, 10468: 4600, 10479: 4601, 10502: 4602, 10587: 4603, 10591: 4604, 10680: 4605, 10790: 4606, 10900: 4607, 10933: 4608, 10942: 4609, 10972: 4610, 10990: 4611, 11013: 4612, 11066: 4613, 11079: 4614, 11093: 4615, 11135: 4616, 11207: 4617, 11270: 4618, 11278: 4619, 11304: 4620, 11311: 4621, 11388: 4622, 11449: 4623, 11458: 4624, 11477: 4625, 11537: 4626, 11627: 4627, 11652: 4628, 11726: 4629, 11818: 4630, 11849: 4631, 11875: 4632, 11940: 4633, 11948: 4634, 11989: 4635, 12004: 4636, 12008: 4637, 12063: 4638, 12067: 4639, 12097: 4640, 12116: 4641, 12157: 4642, 12171: 4643, 12196: 4644, 12248: 4645, 12322: 4646, 12328: 4647, 12336: 4648, 12362: 4649, 12378: 4650, 12432: 4651, 12481: 4652, 12495: 4653, 12557: 4654, 12712: 4655, 12735: 4656, 12742: 4657, 12772: 4658, 12781: 4659, 12803: 4660, 12900: 4661, 13004: 4662, 13038: 4663, 13044: 4664, 13097: 4665, 13127: 4666, 13166: 4667, 13257: 4668, 13266: 4669, 13279: 4670, 13293: 4671, 13306: 4672, 13318: 4673, 13483: 4674, 13643: 4675, 13791: 4676, 13818: 4677, 13984: 4678, 14019: 4679, 14097: 4680, 14490: 4681, 14572: 4682, 14596: 4683, 14688: 4684, 14755: 4685, 14761: 4686, 14948: 4687, 14969: 4688, 15068: 4689, 15113: 4690, 15125: 4691, 15141: 4692, 15239: 4693, 15284: 4694, 15399: 4695, 15405: 4696, 15443: 4697, 15535: 4698, 15591: 4699, 15614: 4700, 15664: 4701, 15716: 4702, 15744: 4703, 15823: 4704, 16460: 4705, 16743: 4706, 17064: 4707, 17102: 4708, 17232: 4709, 17252: 4710, 17369: 4711, 17426: 4712, 17433: 4713, 17500: 4714, 17765: 4715, 17810: 4716, 17870: 4717, 17931: 4718, 18223: 4719, 18339: 4720, 18392: 4721, 18423: 4722, 18463: 4723, 18568: 4724, 18663: 4725, 18681: 4726, 18779: 4727, 18914: 4728, 18942: 4729, 18946: 4730, 18965: 4731, 18990: 4732, 19180: 4733, 19256: 4734, 19316: 4735, 19411: 4736, 19489: 4737, 19550: 4738, 19564: 4739, 19596: 4740, 19619: 4741, 19710: 4742, 19819: 4743, 20002: 4744, 20166: 4745, 20264: 4746, 20581: 4747, 20631: 4748, 20781: 4749, 20886: 4750, 21637: 4751, 21818: 4752, 21854: 4753, 21863: 4754, 22002: 4755, 22202: 4756, 22291: 4757, 22303: 4758, 22542: 4759, 23178: 4760, 23624: 4761, 23760: 4762, 24067: 4763, 24159: 4764, 24332: 4765, 24348: 4766, 24508: 4767, 24520: 4768, 24603: 4769, 24656: 4770, 24665: 4771, 24773: 4772, 24782: 4773, 24894: 4774, 25321: 4775, 25580: 4776, 25966: 4777, 26010: 4778, 26170: 4779, 26389: 4780, 26408: 4781, 26510: 4782, 26714: 4783, 26793: 4784, 26841: 4785, 26858: 4786, 26864: 4787, 27937: 4788, 28482: 4789, 78: 4790, 99: 4791, 242: 4792, 257: 4793, 379: 4794, 417: 4795, 461: 4796, 607: 4797, 677: 4798, 679: 4799, 681: 4800, 702: 4801, 708: 4802, 734: 4803, 745: 4804, 785: 4805, 821: 4806, 824: 4807, 844: 4808, 899: 4809, 991: 4810, 1025: 4811, 1095: 4812, 1242: 4813, 1361: 4814, 1431: 4815, 1439: 4816, 1441: 4817, 1776: 4818, 1818: 4819, 1919: 4820, 1936: 4821, 2069: 4822, 2105: 4823, 2165: 4824, 2200: 4825, 2243: 4826, 2300: 4827, 2329: 4828, 2462: 4829, 2465: 4830, 2626: 4831, 2749: 4832, 2759: 4833, 2801: 4834, 2816: 4835, 2855: 4836, 3062: 4837, 3136: 4838, 3233: 4839, 3280: 4840, 3315: 4841, 3319: 4842, 3337: 4843, 3350: 4844, 3426: 4845, 3518: 4846, 3570: 4847, 3626: 4848, 3667: 4849, 3770: 4850, 3807: 4851, 3892: 4852, 4003: 4853, 4009: 4854, 4013: 4855, 4026: 4856, 4056: 4857, 4157: 4858, 4160: 4859, 4220: 4860, 4403: 4861, 4450: 4862, 4454: 4863, 4475: 4864, 4557: 4865, 4580: 4866, 4666: 4867, 4751: 4868, 4771: 4869, 4790: 4870, 4814: 4871, 4857: 4872, 4956: 4873, 4966: 4874, 5080: 4875, 5102: 4876, 5154: 4877, 5235: 4878, 5249: 4879, 5262: 4880, 5363: 4881, 5366: 4882, 5553: 4883, 5602: 4884, 5681: 4885, 5763: 4886, 5795: 4887, 5854: 4888, 5871: 4889, 5886: 4890, 5911: 4891, 5936: 4892, 5950: 4893, 5998: 4894, 6097: 4895, 6131: 4896, 6200: 4897, 6307: 4898, 6385: 4899, 6420: 4900, 6421: 4901, 6487: 4902, 6488: 4903, 6517: 4904, 6518: 4905, 6539: 4906, 6671: 4907, 6677: 4908, 6704: 4909, 6707: 4910, 6724: 4911, 6741: 4912, 6776: 4913, 6801: 4914, 6825: 4915, 6906: 4916, 6943: 4917, 6985: 4918, 7076: 4919, 7093: 4920, 7101: 4921, 7150: 4922, 7163: 4923, 7194: 4924, 7216: 4925, 7259: 4926, 7261: 4927, 7292: 4928, 7316: 4929, 7320: 4930, 7327: 4931, 7432: 4932, 7446: 4933, 7448: 4934, 7469: 4935, 7470: 4936, 7482: 4937, 7485: 4938, 7506: 4939, 7541: 4940, 7549: 4941, 7588: 4942, 7607: 4943, 7650: 4944, 7651: 4945, 7653: 4946, 7670: 4947, 7736: 4948, 7796: 4949, 7819: 4950, 7834: 4951, 7863: 4952, 7920: 4953, 8026: 4954, 8049: 4955, 8117: 4956, 8125: 4957, 8166: 4958, 8223: 4959, 8238: 4960, 8250: 4961, 8254: 4962, 8261: 4963, 8270: 4964, 8275: 4965, 8375: 4966, 8528: 4967, 8561: 4968, 8565: 4969, 8574: 4970, 8614: 4971, 8634: 4972, 8647: 4973, 8733: 4974, 8880: 4975, 8889: 4976, 8905: 4977, 8944: 4978, 8947: 4979, 8971: 4980, 9078: 4981, 9092: 4982, 9100: 4983, 9103: 4984, 9120: 4985, 9198: 4986, 9224: 4987, 9274: 4988, 9371: 4989, 9390: 4990, 9415: 4991, 9419: 4992, 9426: 4993, 9442: 4994, 9457: 4995, 9528: 4996, 9584: 4997, 9606: 4998, 9623: 4999, 9634: 5000, 9751: 5001, 9777: 5002, 9809: 5003, 9908: 5004, 9928: 5005, 9934: 5006, 9983: 5007, 10027: 5008, 10040: 5009, 10146: 5010, 10193: 5011, 10238: 5012, 10248: 5013, 10265: 5014, 10284: 5015, 10287: 5016, 10320: 5017, 10322: 5018, 10330: 5019, 10335: 5020, 10371: 5021, 10419: 5022, 10475: 5023, 10489: 5024, 10518: 5025, 10520: 5026, 10575: 5027, 10585: 5028, 10606: 5029, 10613: 5030, 10617: 5031, 10628: 5032, 10656: 5033, 10679: 5034, 10771: 5035, 10776: 5036, 10836: 5037, 10842: 5038, 10864: 5039, 10877: 5040, 10930: 5041, 10931: 5042, 10945: 5043, 10994: 5044, 10996: 5045, 10997: 5046, 11034: 5047, 11054: 5048, 11063: 5049, 11065: 5050, 11087: 5051, 11129: 5052, 11155: 5053, 11181: 5054, 11255: 5055, 11265: 5056, 11282: 5057, 11283: 5058, 11302: 5059, 11319: 5060, 11363: 5061, 11411: 5062, 11417: 5063, 11418: 5064, 11425: 5065, 11457: 5066, 11504: 5067, 11548: 5068, 11745: 5069, 11798: 5070, 11815: 5071, 11820: 5072, 11846: 5073, 11922: 5074, 11982: 5075, 11997: 5076, 12058: 5077, 12079: 5078, 12115: 5079, 12126: 5080, 12163: 5081, 12214: 5082, 12215: 5083, 12268: 5084, 12297: 5085, 12334: 5086, 12367: 5087, 12412: 5088, 12417: 5089, 12419: 5090, 12423: 5091, 12450: 5092, 12458: 5093, 12463: 5094, 12523: 5095, 12564: 5096, 12596: 5097, 12612: 5098, 12666: 5099, 12677: 5100, 12683: 5101, 12730: 5102, 12755: 5103, 12762: 5104, 12854: 5105, 12859: 5106, 12877: 5107, 12912: 5108, 12930: 5109, 12936: 5110, 12938: 5111, 12954: 5112, 12986: 5113, 13079: 5114, 13110: 5115, 13192: 5116, 13216: 5117, 13225: 5118, 13231: 5119, 13241: 5120, 13256: 5121, 13325: 5122, 13328: 5123, 13663: 5124, 13811: 5125, 13899: 5126, 13911: 5127, 14059: 5128, 14083: 5129, 14135: 5130, 14210: 5131, 14252: 5132, 14258: 5133, 14382: 5134, 14454: 5135, 14455: 5136, 14462: 5137, 14489: 5138, 14553: 5139, 14563: 5140, 14616: 5141, 14647: 5142, 14696: 5143, 14719: 5144, 14730: 5145, 14759: 5146, 14777: 5147, 14828: 5148, 14872: 5149, 14893: 5150, 15002: 5151, 15029: 5152, 15167: 5153, 15172: 5154, 15176: 5155, 15214: 5156, 15236: 5157, 15309: 5158, 15314: 5159, 15346: 5160, 15353: 5161, 15370: 5162, 15390: 5163, 15415: 5164, 15468: 5165, 15488: 5166, 15500: 5167, 15504: 5168, 15576: 5169, 15583: 5170, 15588: 5171, 15598: 5172, 15641: 5173, 15698: 5174, 15758: 5175, 15764: 5176, 15799: 5177, 15804: 5178, 15853: 5179, 15855: 5180, 15871: 5181, 15872: 5182, 15878: 5183, 15949: 5184, 15974: 5185, 15993: 5186, 16163: 5187, 17195: 5188, 17319: 5189, 17323: 5190, 17539: 5191, 17579: 5192, 17594: 5193, 17617: 5194, 17719: 5195, 17808: 5196, 17822: 5197, 17846: 5198, 17892: 5199, 17897: 5200, 18001: 5201, 18002: 5202, 18114: 5203, 18143: 5204, 18176: 5205, 18290: 5206, 18319: 5207, 18347: 5208, 18422: 5209, 18456: 5210, 18458: 5211, 18514: 5212, 18519: 5213, 18526: 5214, 18603: 5215, 18658: 5216, 18784: 5217, 18986: 5218, 19003: 5219, 19024: 5220, 19026: 5221, 19037: 5222, 19059: 5223, 19067: 5224, 19155: 5225, 19218: 5226, 19262: 5227, 19271: 5228, 19287: 5229, 19326: 5230, 19348: 5231, 19523: 5232, 19582: 5233, 19617: 5234, 19652: 5235, 19717: 5236, 19726: 5237, 19743: 5238, 19796: 5239, 19883: 5240, 19978: 5241, 20091: 5242, 20180: 5243, 20189: 5244, 20197: 5245, 20255: 5246, 20291: 5247, 20296: 5248, 20309: 5249, 20340: 5250, 20461: 5251, 20506: 5252, 20568: 5253, 20649: 5254, 20762: 5255, 20846: 5256, 21540: 5257, 21692: 5258, 21749: 5259, 21941: 5260, 22057: 5261, 22109: 5262, 22184: 5263, 22247: 5264, 22313: 5265, 22396: 5266, 22442: 5267, 22846: 5268, 22871: 5269, 22915: 5270, 22968: 5271, 23003: 5272, 23189: 5273, 23317: 5274, 23385: 5275, 23388: 5276, 23424: 5277, 23463: 5278, 23509: 5279, 23550: 5280, 23590: 5281, 23620: 5282, 23659: 5283, 23668: 5284, 23740: 5285, 23883: 5286, 23897: 5287, 23919: 5288, 24038: 5289, 24082: 5290, 24304: 5291, 24480: 5292, 24515: 5293, 24530: 5294, 24678: 5295, 24826: 5296, 25297: 5297, 25327: 5298, 25647: 5299, 25854: 5300, 26074: 5301, 26089: 5302, 26164: 5303, 26525: 5304, 26581: 5305, 26605: 5306, 27459: 5307, 27769: 5308, 27782: 5309, 28177: 5310, 28645: 5311, 28796: 5312, 28872: 5313, 28973: 5314, 53: 5315, 59: 5316, 76: 5317, 149: 5318, 249: 5319, 256: 5320, 263: 5321, 313: 5322, 335: 5323, 342: 5324, 361: 5325, 431: 5326, 490: 5327, 597: 5328, 610: 5329, 670: 5330, 675: 5331, 719: 5332, 811: 5333, 813: 5334, 817: 5335, 848: 5336, 935: 5337, 943: 5338, 949: 5339, 950: 5340, 1137: 5341, 1203: 5342, 1227: 5343, 1299: 5344, 1328: 5345, 1395: 5346, 1420: 5347, 1429: 5348, 1523: 5349, 1554: 5350, 1568: 5351, 1620: 5352, 1689: 5353, 1692: 5354, 1758: 5355, 1788: 5356, 1852: 5357, 1854: 5358, 1864: 5359, 1966: 5360, 1973: 5361, 1988: 5362, 2068: 5363, 2109: 5364, 2113: 5365, 2141: 5366, 2173: 5367, 2195: 5368, 2249: 5369, 2251: 5370, 2255: 5371, 2272: 5372, 2385: 5373, 2448: 5374, 2463: 5375, 2479: 5376, 2563: 5377, 2611: 5378, 2617: 5379, 2633: 5380, 2662: 5381, 2768: 5382, 2850: 5383, 2985: 5384, 3051: 5385, 3096: 5386, 3205: 5387, 3218: 5388, 3238: 5389, 3262: 5390, 3264: 5391, 3342: 5392, 3363: 5393, 3427: 5394, 3444: 5395, 3458: 5396, 3571: 5397, 3578: 5398, 3590: 5399, 3707: 5400, 3708: 5401, 3712: 5402, 3825: 5403, 4007: 5404, 4064: 5405, 4177: 5406, 4238: 5407, 4375: 5408, 4397: 5409, 4424: 5410, 4435: 5411, 4486: 5412, 4502: 5413, 4689: 5414, 4697: 5415, 4749: 5416, 4766: 5417, 4784: 5418, 5112: 5419, 5138: 5420, 5493: 5421, 5496: 5422, 5502: 5423, 5530: 5424, 5609: 5425, 5637: 5426, 5673: 5427, 5680: 5428, 5766: 5429, 5798: 5430, 5800: 5431, 5802: 5432, 5818: 5433, 5846: 5434, 5860: 5435, 5872: 5436, 5937: 5437, 5941: 5438, 5975: 5439, 5997: 5440, 6037: 5441, 6049: 5442, 6091: 5443, 6096: 5444, 6226: 5445, 6235: 5446, 6374: 5447, 6389: 5448, 6397: 5449, 6436: 5450, 6503: 5451, 6526: 5452, 6542: 5453, 6544: 5454, 6580: 5455, 6582: 5456, 6585: 5457, 6609: 5458, 6638: 5459, 6645: 5460, 6656: 5461, 6672: 5462, 6678: 5463, 6727: 5464, 6766: 5465, 6852: 5466, 6858: 5467, 6869: 5468, 7003: 5469, 7051: 5470, 7056: 5471, 7112: 5472, 7130: 5473, 7179: 5474, 7188: 5475, 7201: 5476, 7208: 5477, 7229: 5478, 7319: 5479, 7350: 5480, 7352: 5481, 7397: 5482, 7419: 5483, 7437: 5484, 7447: 5485, 7480: 5486, 7495: 5487, 7556: 5488, 7623: 5489, 7671: 5490, 7685: 5491, 7729: 5492, 7748: 5493, 7749: 5494, 7797: 5495, 7803: 5496, 7837: 5497, 7890: 5498, 7893: 5499, 7962: 5500, 7971: 5501, 8009: 5502, 8045: 5503, 8047: 5504, 8059: 5505, 8067: 5506, 8081: 5507, 8105: 5508, 8118: 5509, 8121: 5510, 8130: 5511, 8163: 5512, 8164: 5513, 8179: 5514, 8180: 5515, 8233: 5516, 8251: 5517, 8265: 5518, 8295: 5519, 8330: 5520, 8363: 5521, 8367: 5522, 8385: 5523, 8386: 5524, 8543: 5525, 8618: 5526, 8645: 5527, 8770: 5528, 8793: 5529, 8807: 5530, 8829: 5531, 8858: 5532, 8874: 5533, 8919: 5534, 9008: 5535, 9010: 5536, 9174: 5537, 9244: 5538, 9252: 5539, 9342: 5540, 9367: 5541, 9382: 5542, 9398: 5543, 9406: 5544, 9438: 5545, 9478: 5546, 9513: 5547, 9535: 5548, 9546: 5549, 9555: 5550, 9749: 5551, 9772: 5552, 9796: 5553, 9817: 5554, 9839: 5555, 9843: 5556, 9857: 5557, 9874: 5558, 9918: 5559, 9950: 5560, 9956: 5561, 9963: 5562, 10003: 5563, 10031: 5564, 10051: 5565, 10080: 5566, 10108: 5567, 10186: 5568, 10222: 5569, 10241: 5570, 10274: 5571, 10275: 5572, 10310: 5573, 10316: 5574, 10333: 5575, 10360: 5576, 10384: 5577, 10398: 5578, 10401: 5579, 10402: 5580, 10433: 5581, 10454: 5582, 10470: 5583, 10488: 5584, 10526: 5585, 10527: 5586, 10529: 5587, 10544: 5588, 10565: 5589, 10588: 5590, 10605: 5591, 10631: 5592, 10635: 5593, 10639: 5594, 10640: 5595, 10661: 5596, 10662: 5597, 10697: 5598, 10710: 5599, 10713: 5600, 10717: 5601, 10724: 5602, 10728: 5603, 10730: 5604, 10813: 5605, 10879: 5606, 10966: 5607, 10973: 5608, 10979: 5609, 10981: 5610, 10992: 5611, 10999: 5612, 11000: 5613, 11012: 5614, 11061: 5615, 11084: 5616, 11165: 5617, 11227: 5618, 11229: 5619, 11320: 5620, 11338: 5621, 11368: 5622, 11384: 5623, 11395: 5624, 11439: 5625, 11472: 5626, 11486: 5627, 11649: 5628, 11672: 5629, 11687: 5630, 11697: 5631, 11719: 5632, 11722: 5633, 11725: 5634, 11732: 5635, 11780: 5636, 11809: 5637, 11810: 5638, 11838: 5639, 11839: 5640, 11844: 5641, 11894: 5642, 11900: 5643, 11915: 5644, 11961: 5645, 12012: 5646, 12035: 5647, 12064: 5648, 12114: 5649, 12123: 5650, 12135: 5651, 12191: 5652, 12202: 5653, 12265: 5654, 12270: 5655, 12287: 5656, 12289: 5657, 12300: 5658, 12323: 5659, 12325: 5660, 12354: 5661, 12368: 5662, 12375: 5663, 12468: 5664, 12469: 5665, 12487: 5666, 12497: 5667, 12528: 5668, 12531: 5669, 12622: 5670, 12640: 5671, 12660: 5672, 12688: 5673, 12703: 5674, 12706: 5675, 12714: 5676, 12745: 5677, 12746: 5678, 12779: 5679, 12780: 5680, 12812: 5681, 12856: 5682, 12857: 5683, 12890: 5684, 12895: 5685, 12940: 5686, 12945: 5687, 12964: 5688, 13098: 5689, 13133: 5690, 13137: 5691, 13139: 5692, 13188: 5693, 13206: 5694, 13233: 5695, 13303: 5696, 13315: 5697, 13316: 5698, 13464: 5699, 13534: 5700, 13633: 5701, 13667: 5702, 13826: 5703, 13833: 5704, 13842: 5705, 13882: 5706, 14009: 5707, 14034: 5708, 14037: 5709, 14044: 5710, 14073: 5711, 14080: 5712, 14105: 5713, 14139: 5714, 14159: 5715, 14176: 5716, 14197: 5717, 14314: 5718, 14355: 5719, 14361: 5720, 14369: 5721, 14432: 5722, 14447: 5723, 14448: 5724, 14459: 5725, 14514: 5726, 14543: 5727, 14559: 5728, 14571: 5729, 14594: 5730, 14601: 5731, 14607: 5732, 14621: 5733, 14631: 5734, 14634: 5735, 14644: 5736, 14655: 5737, 14666: 5738, 14683: 5739, 14686: 5740, 14765: 5741, 14794: 5742, 14834: 5743, 14863: 5744, 14976: 5745, 15009: 5746, 15027: 5747, 15081: 5748, 15085: 5749, 15090: 5750, 15159: 5751, 15173: 5752, 15189: 5753, 15198: 5754, 15220: 5755, 15221: 5756, 15282: 5757, 15285: 5758, 15294: 5759, 15305: 5760, 15354: 5761, 15366: 5762, 15391: 5763, 15393: 5764, 15400: 5765, 15413: 5766, 15420: 5767, 15425: 5768, 15475: 5769, 15487: 5770, 15513: 5771, 15560: 5772, 15579: 5773, 15581: 5774, 15589: 5775, 15610: 5776, 15642: 5777, 15667: 5778, 15681: 5779, 15685: 5780, 15697: 5781, 15718: 5782, 15754: 5783, 15755: 5784, 15761: 5785, 15771: 5786, 15787: 5787, 15817: 5788, 15857: 5789, 15863: 5790, 15884: 5791, 15978: 5792, 16012: 5793, 16047: 5794, 16178: 5795, 16220: 5796, 16264: 5797, 16301: 5798, 16307: 5799, 16321: 5800, 16354: 5801, 16661: 5802, 16770: 5803, 16858: 5804, 16945: 5805, 16961: 5806, 17071: 5807, 17121: 5808, 17215: 5809, 17301: 5810, 17304: 5811, 17313: 5812, 17331: 5813, 17357: 5814, 17410: 5815, 17417: 5816, 17509: 5817, 17525: 5818, 17548: 5819, 17562: 5820, 17570: 5821, 17603: 5822, 17608: 5823, 17644: 5824, 17685: 5825, 17712: 5826, 17723: 5827, 17878: 5828, 17906: 5829, 17914: 5830, 17963: 5831, 17991: 5832, 18014: 5833, 18105: 5834, 18111: 5835, 18237: 5836, 18267: 5837, 18279: 5838, 18296: 5839, 18298: 5840, 18354: 5841, 18355: 5842, 18372: 5843, 18384: 5844, 18390: 5845, 18406: 5846, 18407: 5847, 18448: 5848, 18459: 5849, 18466: 5850, 18470: 5851, 18480: 5852, 18490: 5853, 18496: 5854, 18507: 5855, 18539: 5856, 18572: 5857, 18590: 5858, 18597: 5859, 18606: 5860, 18680: 5861, 18735: 5862, 18748: 5863, 18759: 5864, 18810: 5865, 18828: 5866, 18858: 5867, 18860: 5868, 18874: 5869, 18890: 5870, 18903: 5871, 18913: 5872, 18988: 5873, 18992: 5874, 18996: 5875, 19039: 5876, 19041: 5877, 19075: 5878, 19119: 5879, 19121: 5880, 19127: 5881, 19147: 5882, 19196: 5883, 19224: 5884, 19231: 5885, 19232: 5886, 19244: 5887, 19272: 5888, 19275: 5889, 19283: 5890, 19310: 5891, 19370: 5892, 19437: 5893, 19468: 5894, 19511: 5895, 19551: 5896, 19562: 5897, 19574: 5898, 19632: 5899, 19675: 5900, 19683: 5901, 19733: 5902, 19765: 5903, 19777: 5904, 19813: 5905, 19814: 5906, 19879: 5907, 19977: 5908, 19992: 5909, 20042: 5910, 20059: 5911, 20159: 5912, 20170: 5913, 20171: 5914, 20196: 5915, 20206: 5916, 20220: 5917, 20233: 5918, 20256: 5919, 20270: 5920, 20279: 5921, 20287: 5922, 20321: 5923, 20324: 5924, 20410: 5925, 20479: 5926, 20549: 5927, 20566: 5928, 20587: 5929, 20669: 5930, 20693: 5931, 20711: 5932, 20722: 5933, 20757: 5934, 20763: 5935, 20783: 5936, 20878: 5937, 20938: 5938, 21015: 5939, 21552: 5940, 21661: 5941, 21711: 5942, 21745: 5943, 21901: 5944, 21929: 5945, 21931: 5946, 21945: 5947, 21974: 5948, 22059: 5949, 22079: 5950, 22089: 5951, 22122: 5952, 22144: 5953, 22150: 5954, 22161: 5955, 22168: 5956, 22177: 5957, 22254: 5958, 22260: 5959, 22320: 5960, 22328: 5961, 22356: 5962, 22367: 5963, 22386: 5964, 22416: 5965, 22418: 5966, 22481: 5967, 22525: 5968, 22535: 5969, 22576: 5970, 22596: 5971, 22610: 5972, 22632: 5973, 22639: 5974, 22738: 5975, 22742: 5976, 22763: 5977, 22830: 5978, 22875: 5979, 22884: 5980, 22930: 5981, 22972: 5982, 23025: 5983, 23051: 5984, 23113: 5985, 23150: 5986, 23163: 5987, 23225: 5988, 23283: 5989, 23379: 5990, 23393: 5991, 23411: 5992, 23452: 5993, 23461: 5994, 23468: 5995, 23544: 5996, 23646: 5997, 23649: 5998, 23664: 5999, 23665: 6000, 23669: 6001, 23687: 6002, 23727: 6003, 23747: 6004, 23794: 6005, 23805: 6006, 23854: 6007, 23885: 6008, 23906: 6009, 23934: 6010, 23962: 6011, 23974: 6012, 24059: 6013, 24064: 6014, 24104: 6015, 24119: 6016, 24138: 6017, 24142: 6018, 24361: 6019, 24364: 6020, 24411: 6021, 24436: 6022, 24566: 6023, 24585: 6024, 24650: 6025, 24709: 6026, 24741: 6027, 24742: 6028, 24770: 6029, 24794: 6030, 24845: 6031, 24865: 6032, 24866: 6033, 24901: 6034, 25053: 6035, 25061: 6036, 25108: 6037, 25209: 6038, 25323: 6039, 25334: 6040, 25372: 6041, 25386: 6042, 25390: 6043, 25449: 6044, 25490: 6045, 25604: 6046, 25661: 6047, 25689: 6048, 25723: 6049, 25749: 6050, 25783: 6051, 25801: 6052, 25847: 6053, 26001: 6054, 26109: 6055, 26113: 6056, 26121: 6057, 26151: 6058, 26168: 6059, 26211: 6060, 26244: 6061, 26289: 6062, 26297: 6063, 26322: 6064, 26368: 6065, 26425: 6066, 26496: 6067, 26684: 6068, 26688: 6069, 26693: 6070, 26853: 6071, 27595: 6072, 27678: 6073, 27756: 6074, 28159: 6075, 28214: 6076, 28338: 6077, 28469: 6078, 28694: 6079, 28874: 6080, 28907: 6081, 28922: 6082, 29105: 6083, 29425: 6084, 31853: 6085, 148: 6086, 174: 6087, 215: 6088, 298: 6089, 309: 6090, 339: 6091, 367: 6092, 389: 6093, 394: 6094, 456: 6095, 469: 6096, 491: 6097, 600: 6098, 603: 6099, 689: 6100, 710: 6101, 714: 6102, 759: 6103, 851: 6104, 887: 6105, 916: 6106, 920: 6107, 960: 6108, 982: 6109, 995: 6110, 996: 6111, 1033: 6112, 1198: 6113, 1356: 6114, 1484: 6115, 1499: 6116, 1533: 6117, 1608: 6118, 1615: 6119, 1625: 6120, 1669: 6121, 1671: 6122, 1706: 6123, 1727: 6124, 1768: 6125, 1787: 6126, 1817: 6127, 1929: 6128, 1960: 6129, 2004: 6130, 2060: 6131, 2178: 6132, 2241: 6133, 2268: 6134, 2389: 6135, 2461: 6136, 2514: 6137, 2606: 6138, 2650: 6139, 2746: 6140, 2862: 6141, 2884: 6142, 2949: 6143, 3016: 6144, 3024: 6145, 3140: 6146, 3157: 6147, 3295: 6148, 3348: 6149, 3401: 6150, 3402: 6151, 3418: 6152, 3546: 6153, 3780: 6154, 3824: 6155, 3916: 6156, 3919: 6157, 3981: 6158, 4019: 6159, 4134: 6160, 4140: 6161, 4241: 6162, 4286: 6163, 4592: 6164, 4610: 6165, 4618: 6166, 4789: 6167, 5025: 6168, 5059: 6169, 5088: 6170, 5155: 6171, 5223: 6172, 5238: 6173, 5254: 6174, 5350: 6175, 5368: 6176, 5384: 6177, 5456: 6178, 5544: 6179, 5555: 6180, 5601: 6181, 5614: 6182, 5653: 6183, 5694: 6184, 5796: 6185, 5810: 6186, 5821: 6187, 5840: 6188, 5852: 6189, 5859: 6190, 5977: 6191, 6015: 6192, 6050: 6193, 6083: 6194, 6181: 6195, 6224: 6196, 6234: 6197, 6358: 6198, 6359: 6199, 6392: 6200, 6445: 6201, 6475: 6202, 6493: 6203, 6575: 6204, 6591: 6205, 6625: 6206, 6644: 6207, 6659: 6208, 6666: 6209, 6676: 6210, 6697: 6211, 6717: 6212, 6839: 6213, 6849: 6214, 6873: 6215, 6912: 6216, 7036: 6217, 7046: 6218, 7048: 6219, 7062: 6220, 7115: 6221, 7142: 6222, 7149: 6223, 7234: 6224, 7553: 6225, 7582: 6226, 7587: 6227, 7606: 6228, 7649: 6229, 7700: 6230, 7726: 6231, 7727: 6232, 7740: 6233, 7754: 6234, 7758: 6235, 7851: 6236, 7871: 6237, 7902: 6238, 7905: 6239, 7954: 6240, 7966: 6241, 8096: 6242, 8097: 6243, 8116: 6244, 8126: 6245, 8171: 6246, 8190: 6247, 8212: 6248, 8256: 6249, 8282: 6250, 8294: 6251, 8354: 6252, 8510: 6253, 8673: 6254, 8831: 6255, 8841: 6256, 8892: 6257, 8899: 6258, 8926: 6259, 8980: 6260, 9031: 6261, 9113: 6262, 9139: 6263, 9197: 6264, 9213: 6265, 9237: 6266, 9260: 6267, 9329: 6268, 9345: 6269, 9412: 6270, 9514: 6271, 9603: 6272, 9610: 6273, 9631: 6274, 9775: 6275, 9834: 6276, 9836: 6277, 9873: 6278, 9876: 6279, 9902: 6280, 9914: 6281, 9943: 6282, 9975: 6283, 10044: 6284, 10079: 6285, 10081: 6286, 10139: 6287, 10176: 6288, 10206: 6289, 10242: 6290, 10247: 6291, 10269: 6292, 10321: 6293, 10324: 6294, 10357: 6295, 10429: 6296, 10460: 6297, 10467: 6298, 10533: 6299, 10564: 6300, 10567: 6301, 10584: 6302, 10592: 6303, 10596: 6304, 10622: 6305, 10654: 6306, 10666: 6307, 10674: 6308, 10694: 6309, 10772: 6310, 10826: 6311, 10829: 6312, 10830: 6313, 10895: 6314, 10901: 6315, 10914: 6316, 10938: 6317, 10950: 6318, 10953: 6319, 11029: 6320, 11042: 6321, 11067: 6322, 11103: 6323, 11110: 6324, 11243: 6325, 11246: 6326, 11305: 6327, 11401: 6328, 11450: 6329, 11456: 6330, 11628: 6331, 11700: 6332, 11778: 6333, 11783: 6334, 11793: 6335, 11852: 6336, 11865: 6337, 11873: 6338, 11943: 6339, 11953: 6340, 11972: 6341, 12002: 6342, 12030: 6343, 12045: 6344, 12077: 6345, 12154: 6346, 12160: 6347, 12169: 6348, 12224: 6349, 12233: 6350, 12399: 6351, 12400: 6352, 12435: 6353, 12477: 6354, 12480: 6355, 12567: 6356, 12605: 6357, 12608: 6358, 12635: 6359, 12648: 6360, 12659: 6361, 12710: 6362, 12723: 6363, 12739: 6364, 12751: 6365, 12756: 6366, 12764: 6367, 12801: 6368, 12831: 6369, 12849: 6370, 12862: 6371, 12927: 6372, 12941: 6373, 12942: 6374, 12962: 6375, 12975: 6376, 13052: 6377, 13058: 6378, 13062: 6379, 13084: 6380, 13117: 6381, 13144: 6382, 13167: 6383, 13169: 6384, 13172: 6385, 13179: 6386, 13186: 6387, 13227: 6388, 13238: 6389, 13276: 6390, 13299: 6391, 13302: 6392, 13386: 6393, 13491: 6394, 13497: 6395, 13546: 6396, 13623: 6397, 13692: 6398, 13720: 6399, 13722: 6400, 13752: 6401, 13806: 6402, 13853: 6403, 13922: 6404, 13932: 6405, 14049: 6406, 14088: 6407, 14093: 6408, 14113: 6409, 14166: 6410, 14200: 6411, 14213: 6412, 14225: 6413, 14237: 6414, 14241: 6415, 14324: 6416, 14384: 6417, 14408: 6418, 14450: 6419, 14461: 6420, 14467: 6421, 14477: 6422, 14519: 6423, 14523: 6424, 14562: 6425, 14627: 6426, 14657: 6427, 14664: 6428, 14677: 6429, 14678: 6430, 14684: 6431, 14702: 6432, 14707: 6433, 14708: 6434, 14753: 6435, 14806: 6436, 14909: 6437, 14917: 6438, 14918: 6439, 14934: 6440, 14943: 6441, 14957: 6442, 14970: 6443, 15007: 6444, 15034: 6445, 15037: 6446, 15110: 6447, 15111: 6448, 15136: 6449, 15138: 6450, 15147: 6451, 15155: 6452, 15183: 6453, 15202: 6454, 15235: 6455, 15240: 6456, 15252: 6457, 15292: 6458, 15301: 6459, 15315: 6460, 15350: 6461, 15381: 6462, 15396: 6463, 15426: 6464, 15460: 6465, 15524: 6466, 15638: 6467, 15650: 6468, 15682: 6469, 15691: 6470, 15704: 6471, 15725: 6472, 15729: 6473, 15740: 6474, 15767: 6475, 15792: 6476, 15796: 6477, 15810: 6478, 15827: 6479, 15838: 6480, 16227: 6481, 16833: 6482, 16996: 6483, 17031: 6484, 17117: 6485, 17132: 6486, 17151: 6487, 17159: 6488, 17172: 6489, 17257: 6490, 17264: 6491, 17279: 6492, 17310: 6493, 17373: 6494, 17385: 6495, 17389: 6496, 17542: 6497, 17545: 6498, 17615: 6499, 17687: 6500, 17692: 6501, 17724: 6502, 17747: 6503, 17759: 6504, 17796: 6505, 17802: 6506, 17848: 6507, 17922: 6508, 17956: 6509, 17994: 6510, 18003: 6511, 18004: 6512, 18021: 6513, 18031: 6514, 18074: 6515, 18141: 6516, 18161: 6517, 18172: 6518, 18175: 6519, 18241: 6520, 18320: 6521, 18365: 6522, 18377: 6523, 18432: 6524, 18501: 6525, 18505: 6526, 18587: 6527, 18614: 6528, 18626: 6529, 18645: 6530, 18646: 6531, 18649: 6532, 18675: 6533, 18705: 6534, 18809: 6535, 18823: 6536, 18824: 6537, 18850: 6538, 18855: 6539, 18867: 6540, 18875: 6541, 18917: 6542, 18928: 6543, 18938: 6544, 18949: 6545, 18994: 6546, 19004: 6547, 19062: 6548, 19164: 6549, 19166: 6550, 19201: 6551, 19222: 6552, 19236: 6553, 19298: 6554, 19304: 6555, 19337: 6556, 19376: 6557, 19413: 6558, 19420: 6559, 19428: 6560, 19441: 6561, 19485: 6562, 19500: 6563, 19516: 6564, 19537: 6565, 19616: 6566, 19643: 6567, 19673: 6568, 19682: 6569, 19703: 6570, 19744: 6571, 19781: 6572, 19835: 6573, 19952: 6574, 19970: 6575, 20014: 6576, 20073: 6577, 20112: 6578, 20134: 6579, 20181: 6580, 20190: 6581, 20200: 6582, 20240: 6583, 20261: 6584, 20269: 6585, 20273: 6586, 20332: 6587, 20395: 6588, 20474: 6589, 20511: 6590, 20515: 6591, 20518: 6592, 20524: 6593, 20564: 6594, 20596: 6595, 20658: 6596, 20670: 6597, 20698: 6598, 20871: 6599, 20885: 6600, 20910: 6601, 20962: 6602, 20988: 6603, 20999: 6604, 21037: 6605, 21112: 6606, 21220: 6607, 21226: 6608, 21548: 6609, 21670: 6610, 21677: 6611, 21721: 6612, 21778: 6613, 21795: 6614, 21857: 6615, 21858: 6616, 21908: 6617, 21916: 6618, 21928: 6619, 21983: 6620, 22080: 6621, 22123: 6622, 22176: 6623, 22180: 6624, 22228: 6625, 22249: 6626, 22269: 6627, 22281: 6628, 22294: 6629, 22358: 6630, 22370: 6631, 22430: 6632, 22447: 6633, 22473: 6634, 22539: 6635, 22556: 6636, 22580: 6637, 22621: 6638, 22633: 6639, 22695: 6640, 22696: 6641, 22731: 6642, 22737: 6643, 22768: 6644, 22800: 6645, 22859: 6646, 22882: 6647, 22888: 6648, 22890: 6649, 22914: 6650, 22917: 6651, 22961: 6652, 22963: 6653, 22988: 6654, 22990: 6655, 23016: 6656, 23101: 6657, 23120: 6658, 23127: 6659, 23136: 6660, 23235: 6661, 23236: 6662, 23253: 6663, 23302: 6664, 23338: 6665, 23368: 6666, 23409: 6667, 23418: 6668, 23426: 6669, 23447: 6670, 23502: 6671, 23518: 6672, 23523: 6673, 23614: 6674, 23641: 6675, 23682: 6676, 23721: 6677, 23779: 6678, 23822: 6679, 23895: 6680, 23933: 6681, 24060: 6682, 24063: 6683, 24081: 6684, 24089: 6685, 24134: 6686, 24224: 6687, 24261: 6688, 24273: 6689, 24275: 6690, 24279: 6691, 24321: 6692, 24378: 6693, 24391: 6694, 24455: 6695, 24503: 6696, 24638: 6697, 24679: 6698, 24700: 6699, 24703: 6700, 24715: 6701, 24739: 6702, 24753: 6703, 24761: 6704, 24831: 6705, 24841: 6706, 24910: 6707, 25059: 6708, 25270: 6709, 25283: 6710, 25373: 6711, 25382: 6712, 25484: 6713, 25520: 6714, 25548: 6715, 25556: 6716, 25565: 6717, 25608: 6718, 25622: 6719, 25654: 6720, 25669: 6721, 25684: 6722, 25738: 6723, 25769: 6724, 25802: 6725, 25811: 6726, 25842: 6727, 25883: 6728, 25907: 6729, 25928: 6730, 25988: 6731, 26030: 6732, 26057: 6733, 26137: 6734, 26210: 6735, 26223: 6736, 26224: 6737, 26382: 6738, 26391: 6739, 26439: 6740, 26451: 6741, 26453: 6742, 26494: 6743, 26497: 6744, 26498: 6745, 26562: 6746, 26585: 6747, 26622: 6748, 26706: 6749, 26791: 6750, 26811: 6751, 26911: 6752, 26915: 6753, 27034: 6754, 27380: 6755, 27655: 6756, 27777: 6757, 27851: 6758, 28192: 6759, 28440: 6760, 28443: 6761, 28576: 6762, 28705: 6763, 28761: 6764, 28947: 6765, 29004: 6766, 30075: 6767, 30465: 6768, 113: 6769, 156: 6770, 278: 6771, 291: 6772, 432: 6773, 445: 6774, 537: 6775, 636: 6776, 815: 6777, 933: 6778, 972: 6779, 979: 6780, 1020: 6781, 1041: 6782, 1092: 6783, 1131: 6784, 1178: 6785, 1179: 6786, 1197: 6787, 1208: 6788, 1211: 6789, 1245: 6790, 1269: 6791, 1292: 6792, 1495: 6793, 1517: 6794, 1656: 6795, 1789: 6796, 1871: 6797, 1942: 6798, 2135: 6799, 2371: 6800, 2419: 6801, 2420: 6802, 2593: 6803, 2838: 6804, 2965: 6805, 3032: 6806, 3097: 6807, 3481: 6808, 3574: 6809, 3597: 6810, 3636: 6811, 3664: 6812, 3728: 6813, 3830: 6814, 4077: 6815, 4086: 6816, 4117: 6817, 4190: 6818, 4273: 6819, 4405: 6820, 4508: 6821, 4566: 6822, 4600: 6823, 4653: 6824, 4695: 6825, 4704: 6826, 4754: 6827, 4769: 6828, 4798: 6829, 4968: 6830, 5046: 6831, 5401: 6832, 5585: 6833, 5817: 6834, 5992: 6835, 6016: 6836, 6017: 6837, 6070: 6838, 6071: 6839, 6363: 6840, 6458: 6841, 6481: 6842, 6606: 6843, 6674: 6844, 6682: 6845, 6689: 6846, 6881: 6847, 6997: 6848, 7071: 6849, 7087: 6850, 7109: 6851, 7118: 6852, 7137: 6853, 7172: 6854, 7299: 6855, 7368: 6856, 7425: 6857, 7449: 6858, 7454: 6859, 7472: 6860, 7566: 6861, 7669: 6862, 7682: 6863, 7831: 6864, 7901: 6865, 7930: 6866, 7968: 6867, 7974: 6868, 8023: 6869, 8033: 6870, 8064: 6871, 8115: 6872, 8241: 6873, 8273: 6874, 8332: 6875, 8552: 6876, 8661: 6877, 8801: 6878, 8805: 6879, 9343: 6880, 9449: 6881, 9476: 6882, 9598: 6883, 9620: 6884, 9675: 6885, 9691: 6886, 9895: 6887, 10077: 6888, 10096: 6889, 10163: 6890, 10298: 6891, 10303: 6892, 10571: 6893, 10682: 6894, 10690: 6895, 10703: 6896, 10805: 6897, 10896: 6898, 10991: 6899, 11017: 6900, 11052: 6901, 11109: 6902, 11198: 6903, 11318: 6904, 11330: 6905, 11373: 6906, 11386: 6907, 11391: 6908, 11426: 6909, 11634: 6910, 11827: 6911, 11892: 6912, 11906: 6913, 11919: 6914, 12001: 6915, 12021: 6916, 12150: 6917, 12175: 6918, 12282: 6919, 12295: 6920, 12296: 6921, 12333: 6922, 12372: 6923, 12404: 6924, 12411: 6925, 12413: 6926, 12415: 6927, 12431: 6928, 12512: 6929, 12595: 6930, 12610: 6931, 12646: 6932, 12727: 6933, 12731: 6934, 12738: 6935, 12840: 6936, 12913: 6937, 12966: 6938, 12987: 6939, 13032: 6940, 13061: 6941, 13073: 6942, 13099: 6943, 13222: 6944, 13333: 6945, 13374: 6946, 13429: 6947, 13840: 6948, 13930: 6949, 14007: 6950, 14018: 6951, 14065: 6952, 14072: 6953, 14119: 6954, 14259: 6955, 14304: 6956, 14315: 6957, 14473: 6958, 14538: 6959, 14661: 6960, 14670: 6961, 14695: 6962, 14879: 6963, 15003: 6964, 15031: 6965, 15053: 6966, 15066: 6967, 15143: 6968, 15156: 6969, 15270: 6970, 15374: 6971, 15427: 6972, 15498: 6973, 15575: 6974, 15692: 6975, 15750: 6976, 15757: 6977, 15776: 6978, 15826: 6979, 15830: 6980, 15847: 6981, 15848: 6982, 15850: 6983, 15867: 6984, 15971: 6985, 17463: 6986, 17653: 6987, 17740: 6988, 17752: 6989, 17790: 6990, 17798: 6991, 17806: 6992, 18127: 6993, 18170: 6994, 18184: 6995, 18348: 6996, 18359: 6997, 18369: 6998, 18383: 6999, 18421: 7000, 18426: 7001, 18430: 7002, 18441: 7003, 18481: 7004, 18528: 7005, 18556: 7006, 18633: 7007, 18641: 7008, 18669: 7009, 18709: 7010, 18711: 7011, 18715: 7012, 18746: 7013, 18764: 7014, 18814: 7015, 18837: 7016, 18873: 7017, 18886: 7018, 18961: 7019, 18966: 7020, 19160: 7021, 19214: 7022, 19227: 7023, 19379: 7024, 19434: 7025, 19521: 7026, 19573: 7027, 19678: 7028, 19687: 7029, 19812: 7030, 19915: 7031, 20081: 7032, 20160: 7033, 20235: 7034, 20334: 7035, 20522: 7036, 20590: 7037, 20633: 7038, 20754: 7039, 20857: 7040, 20908: 7041, 20911: 7042, 21405: 7043, 21727: 7044, 22054: 7045, 22067: 7046, 22158: 7047, 22167: 7048, 22170: 7049, 22500: 7050, 22688: 7051, 22689: 7052, 22704: 7053, 22770: 7054, 22771: 7055, 22780: 7056, 22864: 7057, 23045: 7058, 23123: 7059, 23247: 7060, 23332: 7061, 23352: 7062, 23394: 7063, 23450: 7064, 23610: 7065, 23728: 7066, 23807: 7067, 23825: 7068, 23852: 7069, 23881: 7070, 23905: 7071, 23914: 7072, 23932: 7073, 23957: 7074, 24002: 7075, 24023: 7076, 24092: 7077, 24095: 7078, 24123: 7079, 24180: 7080, 24308: 7081, 24328: 7082, 24342: 7083, 24413: 7084, 24488: 7085, 24574: 7086, 24575: 7087, 24606: 7088, 24667: 7089, 24802: 7090, 24857: 7091, 24915: 7092, 25179: 7093, 25276: 7094, 25322: 7095, 25343: 7096, 25356: 7097, 25401: 7098, 25438: 7099, 25559: 7100, 25576: 7101, 25736: 7102, 25740: 7103, 25765: 7104, 25874: 7105, 25878: 7106, 25905: 7107, 25923: 7108, 26085: 7109, 26175: 7110, 26183: 7111, 26200: 7112, 26325: 7113, 26379: 7114, 26457: 7115, 26557: 7116, 26593: 7117, 26607: 7118, 26639: 7119, 26750: 7120, 26820: 7121, 26840: 7122, 27270: 7123, 27296: 7124, 27342: 7125, 27455: 7126, 27622: 7127, 27711: 7128, 27741: 7129, 27799: 7130, 27850: 7131, 27924: 7132, 27982: 7133, 28010: 7134, 28049: 7135, 28065: 7136, 28156: 7137, 28165: 7138, 28278: 7139, 28284: 7140, 28294: 7141, 28329: 7142, 28411: 7143, 28413: 7144, 28459: 7145, 28563: 7146, 28956: 7147, 28993: 7148, 29602: 7149, 30373: 7150, 30393: 7151, 30929: 7152, 31357: 7153, 31475: 7154, 20: 7155, 38: 7156, 65: 7157, 75: 7158, 89: 7159, 97: 7160, 98: 7161, 187: 7162, 214: 7163, 224: 7164, 230: 7165, 253: 7166, 279: 7167, 285: 7168, 325: 7169, 476: 7170, 485: 7171, 548: 7172, 565: 7173, 616: 7174, 682: 7175, 751: 7176, 779: 7177, 833: 7178, 842: 7179, 876: 7180, 948: 7181, 975: 7182, 1134: 7183, 1157: 7184, 1239: 7185, 1244: 7186, 1253: 7187, 1300: 7188, 1317: 7189, 1389: 7190, 1450: 7191, 1651: 7192, 1683: 7193, 1822: 7194, 1828: 7195, 1895: 7196, 1933: 7197, 1972: 7198, 2001: 7199, 2089: 7200, 2137: 7201, 2171: 7202, 2184: 7203, 2258: 7204, 2287: 7205, 2291: 7206, 2369: 7207, 2428: 7208, 2436: 7209, 2442: 7210, 2456: 7211, 2482: 7212, 2527: 7213, 2566: 7214, 2618: 7215, 2710: 7216, 2714: 7217, 2760: 7218, 2927: 7219, 3009: 7220, 3099: 7221, 3256: 7222, 3310: 7223, 3352: 7224, 3412: 7225, 3425: 7226, 3438: 7227, 3690: 7228, 3735: 7229, 3783: 7230, 3799: 7231, 3990: 7232, 4032: 7233, 4135: 7234, 4372: 7235, 4381: 7236, 4382: 7237, 4394: 7238, 4619: 7239, 4632: 7240, 4639: 7241, 4949: 7242, 5009: 7243, 5041: 7244, 5081: 7245, 5148: 7246, 5167: 7247, 5173: 7248, 5183: 7249, 5209: 7250, 5211: 7251, 5215: 7252, 5319: 7253, 5452: 7254, 5457: 7255, 5478: 7256, 5482: 7257, 5709: 7258, 5760: 7259, 6045: 7260, 6051: 7261, 6056: 7262, 6092: 7263, 6138: 7264, 6201: 7265, 6357: 7266, 6531: 7267, 6556: 7268, 6613: 7269, 6778: 7270, 6804: 7271, 6921: 7272, 6978: 7273, 7075: 7274, 7110: 7275, 7125: 7276, 7293: 7277, 7326: 7278, 7332: 7279, 7346: 7280, 7435: 7281, 7616: 7282, 7625: 7283, 7738: 7284, 7997: 7285, 8052: 7286, 8162: 7287, 8231: 7288, 8310: 7289, 8325: 7290, 8351: 7291, 8912: 7292, 8997: 7293, 9095: 7294, 9099: 7295, 9133: 7296, 9134: 7297, 9336: 7298, 9361: 7299, 9422: 7300, 9525: 7301, 9547: 7302, 9574: 7303, 9591: 7304, 9621: 7305, 9713: 7306, 9786: 7307, 9907: 7308, 9920: 7309, 10013: 7310, 10086: 7311, 10099: 7312, 10104: 7313, 10288: 7314, 10325: 7315, 10560: 7316, 10636: 7317, 10696: 7318, 10780: 7319, 10785: 7320, 10841: 7321, 10882: 7322, 10960: 7323, 10969: 7324, 11117: 7325, 11199: 7326, 11244: 7327, 11288: 7328, 11293: 7329, 11383: 7330, 11432: 7331, 11766: 7332, 11801: 7333, 11814: 7334, 11931: 7335, 12042: 7336, 12057: 7337, 12093: 7338, 12212: 7339, 12267: 7340, 12374: 7341, 12377: 7342, 12511: 7343, 12517: 7344, 12546: 7345, 12565: 7346, 12618: 7347, 12872: 7348, 12959: 7349, 12974: 7350, 13039: 7351, 13051: 7352, 13083: 7353, 13101: 7354, 13221: 7355, 13239: 7356, 13852: 7357, 13978: 7358, 14043: 7359, 14151: 7360, 14246: 7361, 14313: 7362, 14374: 7363, 14592: 7364, 14650: 7365, 14785: 7366, 14814: 7367, 14902: 7368, 14949: 7369, 15010: 7370, 15157: 7371, 15258: 7372, 15339: 7373, 15371: 7374, 15436: 7375, 15439: 7376, 15457: 7377, 15520: 7378, 15521: 7379, 15582: 7380, 15673: 7381, 15801: 7382, 15912: 7383, 15960: 7384, 17376: 7385, 17981: 7386, 18165: 7387, 18315: 7388, 18750: 7389, 18763: 7390, 18789: 7391, 18849: 7392, 18919: 7393, 19290: 7394, 19323: 7395, 19405: 7396, 19476: 7397, 19618: 7398, 19657: 7399, 19718: 7400, 19759: 7401, 19783: 7402, 19893: 7403, 19935: 7404, 20006: 7405, 20247: 7406, 20293: 7407, 20795: 7408, 20838: 7409, 21724: 7410, 22119: 7411, 22336: 7412, 22586: 7413, 22743: 7414, 23205: 7415, 23256: 7416, 23574: 7417, 23734: 7418, 23791: 7419, 24237: 7420, 24435: 7421, 24558: 7422, 24630: 7423, 25180: 7424, 25480: 7425, 25487: 7426, 25514: 7427, 25516: 7428, 25667: 7429, 25699: 7430, 25867: 7431, 26086: 7432, 26097: 7433, 26178: 7434, 26640: 7435, 26664: 7436, 26824: 7437, 27103: 7438, 27528: 7439, 27899: 7440, 28050: 7441, 28173: 7442, 28260: 7443, 28393: 7444, 28447: 7445, 28502: 7446, 28597: 7447, 28622: 7448, 28657: 7449, 28986: 7450, 29066: 7451, 29264: 7452, 30404: 7453, 30462: 7454, 31185: 7455, '<eos>': 0, '<sos>': 0})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0jZdRoSAqM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for batch in train_iterator:\n",
        "  #print((batch.trg).size())\n",
        "  #print(batch.batch_size)\n",
        "  #print(batch.trg)\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpIy0T8SS4tZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model: nn.Module,\n",
        "          iterator: BucketIterator,\n",
        "          optimizer: optim.Optimizer,\n",
        "          criterion: nn.Module,\n",
        "          clip: float):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for _, batch in enumerate(iterator):\n",
        "\n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, trg)\n",
        "        \n",
        "\n",
        "        output = output[1:].view(-1, output.shape[-1])\n",
        "        trg = trg[1:].view(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxuCY5pIS5C-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model: nn.Module,\n",
        "             iterator: BucketIterator,\n",
        "             criterion: nn.Module):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for _, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output = model(src, trg, 0) #turn off teacher forcing?\n",
        "\n",
        "            output = output[1:].view(-1, output.shape[-1])\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVZyqYBAUWoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_time(start_time: int,\n",
        "               end_time: int):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfEa6l1VSm-W",
        "colab_type": "text"
      },
      "source": [
        "##Simple NN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oiunRuTSsez",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "86d5ce91-55e9-4fa7-d48f-0cf879b43878"
      },
      "source": [
        "from typing import Tuple\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_dim: int,\n",
        "                 emb_dim: int,\n",
        "                 enc_hid_dim: int,\n",
        "                 dec_hid_dim: int,\n",
        "                 dropout: float):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.emb_dim = emb_dim\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "\n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "\n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor) -> Tuple[Tensor]:\n",
        "\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "\n",
        "        return outputs, hidden\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 enc_hid_dim: int,\n",
        "                 dec_hid_dim: int,\n",
        "                 attn_dim: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "\n",
        "        self.attn_in = (enc_hid_dim * 2) + dec_hid_dim\n",
        "\n",
        "        self.attn = nn.Linear(self.attn_in, attn_dim)\n",
        "\n",
        "    def forward(self,\n",
        "                decoder_hidden: Tensor,\n",
        "                encoder_outputs: Tensor) -> Tensor:\n",
        "\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "\n",
        "        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "\n",
        "        energy = torch.tanh(self.attn(torch.cat((\n",
        "            repeated_decoder_hidden,\n",
        "            encoder_outputs),\n",
        "            dim = 2)))\n",
        "\n",
        "        attention = torch.sum(energy, dim=2)\n",
        "\n",
        "        return F.softmax(attention, dim=1)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 output_dim: int,\n",
        "                 emb_dim: int,\n",
        "                 enc_hid_dim: int,\n",
        "                 dec_hid_dim: int,\n",
        "                 dropout: int,\n",
        "                 attention: nn.Module):\n",
        "        super().__init__()\n",
        "\n",
        "        self.emb_dim = emb_dim\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.dropout = dropout\n",
        "        self.attention = attention\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "\n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "\n",
        "        self.out = nn.Linear(self.attention.attn_in + emb_dim, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def _weighted_encoder_rep(self,\n",
        "                              decoder_hidden: Tensor,\n",
        "                              encoder_outputs: Tensor) -> Tensor:\n",
        "\n",
        "        a = self.attention(decoder_hidden, encoder_outputs)\n",
        "\n",
        "        a = a.unsqueeze(1)\n",
        "\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "\n",
        "        weighted_encoder_rep = torch.bmm(a, encoder_outputs)\n",
        "\n",
        "        weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2)\n",
        "\n",
        "        return weighted_encoder_rep\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "                input: Tensor,\n",
        "                decoder_hidden: Tensor,\n",
        "                encoder_outputs: Tensor) -> Tuple[Tensor]:\n",
        "\n",
        "        input = input.unsqueeze(0)\n",
        "\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "\n",
        "        weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden,\n",
        "                                                          encoder_outputs)\n",
        "\n",
        "        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim = 2)\n",
        "\n",
        "        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))\n",
        "\n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted_encoder_rep = weighted_encoder_rep.squeeze(0)\n",
        "\n",
        "        output = self.out(torch.cat((output,\n",
        "                                     weighted_encoder_rep,\n",
        "                                     embedded), dim = 1))\n",
        "\n",
        "        return output, decoder_hidden.squeeze(0)\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self,\n",
        "                 encoder: nn.Module,\n",
        "                 decoder: nn.Module,\n",
        "                 device: torch.device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                teacher_forcing_ratio: float = 0.5) -> Tensor:\n",
        "\n",
        "        batch_size = src.shape[1]\n",
        "        max_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "\n",
        "        # first input to the decoder is the <sos> token\n",
        "        output = trg[0,:]\n",
        "\n",
        "        for t in range(1, max_len):\n",
        "            output, hidden = self.decoder(output, hidden, encoder_outputs)\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.max(1)[1]\n",
        "            output = (trg[t] if teacher_force else top1)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "# ENC_EMB_DIM = 256\n",
        "# DEC_EMB_DIM = 256\n",
        "# ENC_HID_DIM = 512\n",
        "# DEC_HID_DIM = 512\n",
        "# ATTN_DIM = 64\n",
        "# ENC_DROPOUT = 0.5\n",
        "# DEC_DROPOUT = 0.5\n",
        "\n",
        "ENC_EMB_DIM = 32\n",
        "DEC_EMB_DIM = 32\n",
        "ENC_HID_DIM = 64\n",
        "DEC_HID_DIM = 64\n",
        "ATTN_DIM = 8\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n",
        "\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "\n",
        "def init_weights(m: nn.Module):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "\n",
        "\n",
        "model.apply(init_weights)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "\n",
        "def count_parameters(model: nn.Module):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 2,343,912 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxLJ3G1rLg5d",
        "colab_type": "text"
      },
      "source": [
        "## Transformer Model\n",
        "https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYHdYDJmSsSV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embedder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "    def forward(self, x):\n",
        "        return self.embed(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpYdmk1ISsK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len = 80):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        # create constant 'pe' matrix with values dependant on \n",
        "        # pos and i\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = \\\n",
        "                math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = \\\n",
        "                math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "                \n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        " \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # make embeddings relatively larger\n",
        "        x = x * math.sqrt(self.d_model)\n",
        "        #add constant to embedding\n",
        "        seq_len = x.size(1)\n",
        "        x = x + Variable(self.pe[:,:seq_len], \\\n",
        "        requires_grad=False).cuda()\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyIAgTg7OfL1",
        "colab_type": "text"
      },
      "source": [
        "Test training code, ignore/delete for production"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPa3wg-CSsE2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch = next(iter(train_iterator))\n",
        "input_seq = batch.src.transpose(0,1)\n",
        "input_pad = SRC.vocab.stoi['<pad>']\n",
        "# creates mask with 0s wherever there is padding in the input\n",
        "input_msk = (input_seq != input_pad).unsqueeze(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Em1YtoWNWVX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "24721189-3322-47f2-96ff-c080cbb534d6"
      },
      "source": [
        "# create mask as before\n",
        "target_seq = batch.trg.transpose(0,1)\n",
        "target_pad = TRG.vocab.stoi['<pad>']\n",
        "target_msk = (target_seq != target_pad).unsqueeze(1)\n",
        "size = target_seq.size(1) # get seq_len for matrix\n",
        "nopeak_mask = np.triu(np.ones(1, size, size),\n",
        "k=1).astype('uint8')\n",
        "nopeak_mask = Variable(torch.from_numpy(nopeak_mask) == 0)\n",
        "target_msk = target_msk & nopeak_mask"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-1a6c72f3c6b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtarget_msk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget_seq\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget_pad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# get seq_len for matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m nopeak_mask = np.triu(np.ones(1, size, size),\n\u001b[0m\u001b[1;32m      7\u001b[0m k=1)\n\u001b[1;32m      8\u001b[0m \u001b[0mnopeak_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnopeak_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mones\u001b[0;34m(shape, dtype, order)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \"\"\"\n\u001b[0;32m--> 207\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m     \u001b[0mmultiarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'unsafe'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: data type not understood"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUN80MNINWqW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, heads, d_model, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_model // heads\n",
        "        self.h = heads\n",
        "        \n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "    \n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        \n",
        "        bs = q.size(0)\n",
        "        \n",
        "        # perform linear operation and split into h heads\n",
        "        \n",
        "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
        "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
        "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
        "        \n",
        "        # transpose to get dimensions bs * h * sl * d_model\n",
        "       \n",
        "        k = k.transpose(1,2)\n",
        "        q = q.transpose(1,2)\n",
        "        v = v.transpose(1,2)\n",
        "# calculate attention using function we will define next\n",
        "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
        "        \n",
        "        # concatenate heads and put through final linear layer\n",
        "        concat = scores.transpose(1,2).contiguous()\\\n",
        "        .view(bs, -1, self.d_model)\n",
        "        \n",
        "        output = self.out(concat)\n",
        "    \n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH2N7sOWOs68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
        "    \n",
        "    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
        "    \n",
        "    if mask is not None:\n",
        "        mask = mask.unsqueeze(1)\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    \n",
        "    scores = F.softmax(scores, dim=-1)\n",
        "    \n",
        "    if dropout is not None:\n",
        "        scores = dropout(scores)\n",
        "        \n",
        "    output = torch.matmul(scores, v)\n",
        "    return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RotMoyBJOtNq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
        "        super().__init__() \n",
        "        # We set d_ff as a default to 2048\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(F.relu(self.linear_1(x)))\n",
        "        x = self.linear_2(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqYzxch1OtfP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Norm(nn.Module):\n",
        "    def __init__(self, d_model, eps = 1e-6):\n",
        "        super().__init__()\n",
        "    \n",
        "        self.size = d_model\n",
        "        # create two learnable parameters to calibrate normalisation\n",
        "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
        "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
        "        self.eps = eps\n",
        "    def forward(self, x):\n",
        "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
        "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
        "        return norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8xnVznAO7uY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build an encoder layer with one multi-head attention layer and one # feed-forward layer\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.attn = MultiHeadAttention(heads, d_model)\n",
        "        self.ff = FeedForward(d_model)\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        x2 = self.norm_1(x)\n",
        "        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
        "        x2 = self.norm_2(x)\n",
        "        x = x + self.dropout_2(self.ff(x2))\n",
        "        return x\n",
        "    \n",
        "# build a decoder layer with two multi-head attention layers and\n",
        "# one feed-forward layer\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.norm_3 = Norm(d_model)\n",
        "        \n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        self.dropout_3 = nn.Dropout(dropout)\n",
        "        \n",
        "        self.attn_1 = MultiHeadAttention(heads, d_model)\n",
        "        self.attn_2 = MultiHeadAttention(heads, d_model)\n",
        "        self.ff = FeedForward(d_model).cuda()\n",
        "def forward(self, x, e_outputs, src_mask, trg_mask):\n",
        "        x2 = self.norm_1(x)\n",
        "        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
        "        x2 = self.norm_2(x)\n",
        "        x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs,\n",
        "        src_mask))\n",
        "        x2 = self.norm_3(x)\n",
        "        x = x + self.dropout_3(self.ff(x2))\n",
        "        return x\n",
        "        \n",
        "# We can then build a convenient cloning function that can generate multiple layers:\n",
        "def get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPVQTcQqO8mg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, heads):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model)\n",
        "        self.layers = get_clones(EncoderLayer(d_model, heads), N)\n",
        "        self.norm = Norm(d_model)\n",
        "    def forward(self, src, mask):\n",
        "        x = self.embed(src)\n",
        "        x = self.pe(x)\n",
        "        for i in range(N):\n",
        "            x = self.layers[i](x, mask)\n",
        "        return self.norm(x)\n",
        "    \n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, heads):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model)\n",
        "        self.layers = get_clones(DecoderLayer(d_model, heads), N)\n",
        "        self.norm = Norm(d_model)\n",
        "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
        "        x = self.embed(trg)\n",
        "        x = self.pe(x)\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab, d_model, N, heads)\n",
        "        self.decoder = Decoder(trg_vocab, d_model, N, heads)\n",
        "        self.out = nn.Linear(d_model, trg_vocab)\n",
        "    def forward(self, src, trg, src_mask, trg_mask):\n",
        "        e_outputs = self.encoder(src, src_mask)\n",
        "        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
        "        output = self.out(d_output)\n",
        "        return output\n",
        "# we don't perform softmax on the output as this will be handled \n",
        "# automatically by our loss function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvjnAmH0Sk6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_masks(src, trg, opt):\n",
        "    \n",
        "    src_mask = (src != opt.src_pad).unsqueeze(-2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWrxRUafPH7C",
        "colab_type": "text"
      },
      "source": [
        "####model training prep code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60_nlAv6O9AM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d_model = 512\n",
        "heads = 8\n",
        "N = 6\n",
        "src_vocab = len(SRC.vocab)\n",
        "trg_vocab = len(TRG.vocab)\n",
        "model = Transformer(src_vocab, trg_vocab, d_model, N, heads)\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "# this code is very important! It initialises the parameters with a\n",
        "# range of values that stops the signal fading or getting too big.\n",
        "# See this blog for a mathematical explanation.\n",
        "optim = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXREuT2XnJPQ",
        "colab_type": "text"
      },
      "source": [
        "## Model Training Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5b6OtVkRcCm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(epochs, print_every=100):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    start = time.time()\n",
        "    temp = start\n",
        "    \n",
        "    total_loss = 0\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "       \n",
        "        for i, batch in enumerate(train_iterator):\n",
        "            src = batch.src.transpose\n",
        "            trg = batch.trg.transpose(0,1)\n",
        "            # the French sentence we input has all words except\n",
        "            # the last, as it is using each word to predict the next\n",
        "            \n",
        "            trg_input = trg[:, :-1]\n",
        "            \n",
        "            # the words we are trying to predict\n",
        "            \n",
        "            targets = trg[:, 1:].contiguous().view(-1)\n",
        "            \n",
        "            # create function to make masks using mask code above\n",
        "            \n",
        "            src_mask, trg_mask = create_masks(src, trg_input)\n",
        "            \n",
        "            preds = model(src, trg_input, src_mask, trg_mask)\n",
        "            \n",
        "            optim.zero_grad()\n",
        "            \n",
        "            loss = F.cross_entropy(preds.view(-1, preds.size(-1)),\n",
        "            results, ignore_index=target_pad)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            \n",
        "            total_loss += loss.data[0]\n",
        "            if (i + 1) % print_every == 0:\n",
        "                loss_avg = total_loss / print_every\n",
        "                print(\"time = %dm, epoch %d, iter = %d, loss = %.3f, %ds per %d iters\" % ((time.time() - start) // 60, epoch + 1, i + 1, loss_avg, time.time() - temp, print_every))\n",
        "                total_loss = 0\n",
        "                temp = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1q2KEeRR8h1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "ef32b12d-6811-44ce-ff27-2f9f78ef78cc"
      },
      "source": [
        "train_model(10)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-b605afb000fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-73-f5d1fc235016>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(epochs, print_every)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m# create function to make masks using mask code above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: create_masks() missing 1 required positional argument: 'opt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH0BE6gBS5ja",
        "colab_type": "code",
        "outputId": "265d97f5-09d1-4d37-f1f0-6e88ff14547c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "source": [
        "N_EPOCHS = 50\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-ed7dd127d25d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-972e3fb06593>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: forward() missing 2 required positional arguments: 'src_mask' and 'trg_mask'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLDwBrtuE-XQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}